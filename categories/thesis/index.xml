<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Thesis - Category - Utopizza</title>
        <link>https://utopizza.github.io/categories/thesis/</link>
        <description>Thesis - Category - Utopizza</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>648847079@qq.com (yusheng)</managingEditor>
            <webMaster>648847079@qq.com (yusheng)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Sun, 26 Jul 2020 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://utopizza.github.io/categories/thesis/" rel="self" type="application/rss+xml" /><item>
    <title>Raft</title>
    <link>https://utopizza.github.io/2020-07-26-%E8%AE%BA%E6%96%87-raft/</link>
    <pubDate>Sun, 26 Jul 2020 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2020-07-26-%E8%AE%BA%E6%96%87-raft/</guid>
    <description><![CDATA[一、背景及问题
共识算法（consensus algorithm）在实现大型分布式系统的可靠性方面扮演了十分关键的角色，因为它允许集群在部分机器故障的情况下保持任务正常运行。在过去十年中，主流的共识算法叫Paxos，大部分系统都使用了这个算法，并且该算法一度成为标准的教材。然而，该算法有一个缺陷就是过于复杂，想要完全理解或者完美实现都十分困难。因此，斯坦福的Diego Ongaro和John Ousterhout便提出了一个相对容易理解、容易实现的共识算法，命名为Raft。
二、相关概念
Replicated state machine：复制状态机，是共识算法的起源之处。像GFS、HDFS等的大规模分布式系统，其leader节点都只有一个，存在单点故障问题。要保证集群即使出现该故障的情况下自动恢复正常运行，如集群配置信息、leader选举心跳信息等数据必须克服单点故障的问题，这就要求这些重要的数据有可靠的方式来进行备份。一种可靠的方式就是使用复制状态机，如下图所示，复制状态机一般是通过复制日志（replicated log）来实现。每个机器上都会维护一份复制日志，日志的内容是一系列的command，机器会按日志的顺序来执行这些command，来修改自己的状态。只要保证每个机器上的复制日志是完全一致，那么就可以保证每个机器的操作结果是一致的。其中，共识算法的主要作用就是通过一些通信手段，来保持集群中每个机器的复制日志完全一致。目前比较有名的复制状态机的实现是Zookeeper和Chubby。
三、Raft算法
总的来说，Raft首先会从集群中选举一个leader，整个集群的复制日志完全由leader来维护：leader节点接收client端发送过来的日志实体（log entry），然后把日志实体分发给集群中其他节点，并告诉其他节点什么时候可以安全地执行这些日志，从而修改他们的状态。当leader宕机故障时，集群会自动发起选举，选出一个新的leader继续保持集群运行。为了便于理解和实践，Raft把共识问题拆分成三个彼此独立的子问题，从而逐一解决：（1）leader选举（2）日志复制（3）安全
1、Raft基础
Raft集群（一般规模为5台机器）中的节点在任何时刻（可以运行的情况下）必为以下三种状态之一：（1）leader（2）follower（3）candidate。在正常情况下，集群中只有一个leader，其余均为follower。follower不响应任何client端请求，它只会把这些请求重定向给leader，并且只响应leader的请求。
Raft把时间分割成任意长度的周期（term），这些周期用连续的整数来唯一地标识，可以理解为周期id。每个周期都由leader选举开始，在这个周期内如果一个节点当选为leader，它会一直作为leader直到这个周期结束。集群中每个节点自身都会维护一个周期id作为自己看到的当前周期。当节点之间进行通信时会带上自己的当前周期，如果节点发现对方周期id比自己大，则更新自己的当前周期；如果节点发现自己的当前周期已经过期，则马上把自己转为follower状态；如果节点发现对方的周期已经过期，则直接拒绝对方的请求。
2、leader选举
Raft使用心跳机制（heartbeat）来触发leader选举。当Raft集群刚启动时，所有节点都是follower状态。follower节点一段时间内发现没有收到来自leader的心跳，认为leader节点已经失效（election timeout），则马上把自己转变为candidate状态，并将自己的当前周期增大。它会先为自己投票，然后向集群中其他节点发起拉票请求（RequestVote RPC）。candidate节点会一直持续这个状态直到（1）它获得多数票从而赢得选举成为新leader（2）其他节点胜出成为新leader（3）选举超时，没有选出新leader。
Raft投票规则：（1）每个节点每次选举周期内最多投出一票（2）先到先得原则，给先请求的节点投票（2）candidate只给自己投一票，然后并发地向其他节点拉票（4）特殊情况下，可能会进入僵持状态——例如所有节点同时进入candidate状态，只给自己投票，然后等等其他人给自己投票，此时的集群状态称为“split”。该论文解决这个问题使用了一种简单的方法，就是随机话选举超时时间，避免多个节点同时开启选举周期，使得先开启选举的节点有先发优势成为新leader，减少集群不可用时间。
当一个candidate成为新leader后，马上开始向其他节点发送心跳宣告自己的leader身份。其他节点确认该新leader的当前周期是有效周期后，认可新leader的身份，把自己转变为follower；否则无视该leader，继续保持选举。
3、日志复制
当节点成为leader后，开始负责响应client的请求。client的请求会包含一个需要整个集群都执行的command。leader首先会把该command追加到自己的日志中，然后通过RPC（AppendEntries RPCs）把日志分发到其他节点。若某个follower宕机或者网络丢包没有复制成功，leader会无限地重新尝试，直到复制成功。分发完成后，leader执行该command，修改自己的状态，然后把执行结果返回给client。具体关于日志的组织形式、压缩方法，详见原论文。
4、安全性
这部分较为繁琐，详见原论文。]]></description>
</item><item>
    <title>Spark</title>
    <link>https://utopizza.github.io/2019-05-09-%E8%AE%BA%E6%96%87-spark/</link>
    <pubDate>Thu, 09 May 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-05-09-%E8%AE%BA%E6%96%87-spark/</guid>
    <description><![CDATA[一、背景及问题
在谷歌提出了 MapReduce 编程模型之后，UC Berkeley 在此基础上提出了一个更加高效的模型，称为 Spark。其核心思想是将 MapReduce 的中间结果缓存到内存中，使得 Workers 可以快速读取数据而无需启动延迟极大的磁盘读取操作。这种编程模型针对一些迭代次数高、需要反复使用或者修改数据的计算任务尤其有效。根据论文的实验，Spark 在一些高迭代次数的算法实验中处理速度是 MapReduce 的 10 倍以上。
然而因为没有了将中间结果写磁盘的操作来保证容灾和恢复，因此 Spark 设计了一套别的方案来达到该目的，那就是 RDD + Lineage。RDD(Resilient Distribute Datasets)称为弹性数据集，它是一种对被操作数据的抽象，而 Lineage 称为“血统”，顾名思义它记录了每一个 RDD 演变过程中的上下文信息。当故障出现时，可以根据丢失的 RDD 的 Lineage 来追寻它的祖先 RDD ，然后重新执行演化即可恢复出丢失的 RDD。
二、Spark 编程模型
使用 Spark 的用户通过编写一个称为 Driver Program 的程序来实现自己的计算任务。Spark 为用户的并行化编程提供了三个组件：resilient distribution dataset、 parallel operations、 parallel operations。
(1) Resilient Distribution Datasets(RDDs)：一个 RDD 是一个可恢复的只读对象集合，它无须存储在磁盘中。每次对一个 RDD 的操作都会被记录下来，从而每个 RDD 可以沿着它的演化过程一直追寻它的祖先 RDD，甚至可以追寻到最开始的第一次从磁盘读取初始数据。因此这个机制保证了任何一个 RDD 都可以被恢复。具体在 Spark 系统中，RDD 由 Scala 对象来表达。论文中规定了 RDD 仅可以从四种方式构造：]]></description>
</item><item>
    <title>MapReduce</title>
    <link>https://utopizza.github.io/2019-05-08-%E8%AE%BA%E6%96%87-mapreduce/</link>
    <pubDate>Wed, 08 May 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-05-08-%E8%AE%BA%E6%96%87-mapreduce/</guid>
    <description><![CDATA[一、背景及问题
这两天总算把谷歌三大论文之一的 MapReduce 看了。这篇论文的影响力就不多说了，谷歌AI首席科学家 Jeffrey Dean 的代表作之一。该论文主要提出了一个称为 MapReduce 的编程模型，主要是为了解决谷歌公司自身需要处理海量数据的问题。众所周知，谷歌的主要业务是搜索引擎，每天需要处理爬取到的海量文件和网页，以及对这些数据进行索引计算、处理查询请求等等。显然单台机器难以完成这样巨大的计算任务，必须使用机器集群。但如果使用专业的服务器，集群的成本就太高了。为了降低成本，谷歌采用了普通的计算机来搭建这个集群。为了实现这个目的，必须设计一套可以自动并行化计算、自我管理调度、以及拥有良好容错性的集群计算方案，因此 Jeffrey Dean 提出了这套编程模型，只要按照这个编程模型来调用接口实现计算任务，这些计算就可以自动地在一个由普通机器搭建的分布式集群上被并行地执行。
二、解决方案
具体来说，MapReduce 这个编程模型可以简单地分为 Map 部分以及 Reduce 部分。
Map 函数：由用户实现该接口，其功能是使算法输入的初始 key-value 对转化成用户定义的新 key-value 对。接下来系统会自动把它们按新 key 分组，即一个新 key 对应一组 values（key-values），并传递给 Reduce 函数。这一步类似 SQL 中的 Select GroupBy 操作。
$$key_{1}/value_{1} \implies key_{2}/value_{2} \implies key_{2}/value_{2}(s)$$
Reduce 函数：由用户实现该接口，其功能是使从 Map 函数接收过来的 key-values 组按用户定义的计算方式进行 Merge，然后输出每个 key 组的最终结果。这一步类似定义 SQL 中对被 GroupBy 的字段的聚合函数。
$$key_{2}/value_{2}(s) \implies key_{2}/value_{3} $$
论文中给出了一个具体例子，从一个巨大的文档集中统计每个单词的出现次数。每个单词可以看作 key，它的出现次数就是最终想得到的 value。按照 MapReduce 编程范式，可以分为两步，在 Map 阶段输入文档集合，为每个文档拆解出每个单词，并为每个单词赋值它的 value 为 1，表示这个单词在此出现了一次；在 Reduce 阶段，把每个单词对应的分组进行 value 的求和，即可得到每个单词的总出现次数。]]></description>
</item><item>
    <title>MCI</title>
    <link>https://utopizza.github.io/2019-01-27-%E8%AE%BA%E6%96%87-mci/</link>
    <pubDate>Sun, 27 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-27-%E8%AE%BA%E6%96%87-mci/</guid>
    <description><![CDATA[该文章依然是针对 APT 攻击而提出的一种基于模型推断的攻击因果分析。该方法比以往方法的优势在于不需要在系统中做任何修改，只需要启动日志，对系统事件进行记录即可。该方法的核心技术是使用一种称为 LDX 的动态分析方案，可以在系统调用之间进行因果推断。]]></description>
</item><item>
    <title>Oblix</title>
    <link>https://utopizza.github.io/2019-01-19-%E8%AE%BA%E6%96%87-oblix/</link>
    <pubDate>Sat, 19 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-19-%E8%AE%BA%E6%96%87-oblix/</guid>
    <description><![CDATA[一、背景及问题
索引（Index）是很多系统和应用的基础构件。最近，大量的研究专注于如何保护索引这样的敏感数据，例如，如何在对索引进行加密的同时允许客户端在索引上进行查询。这些研究提出的方案一般是这样的：当用户通过客户端发起查询请求时，客户端会先为用户输入的关键词（keyword）生成查询令牌(search token)，然后将令牌发送给服务端而不是直接发送用户的关键词，从而向服务端屏蔽用户输入的关键词（假设攻击者控制了服务端的内存，但不能控制服务端的处理器）。然后，服务器通过令牌去在已经加密的索引上执行检索，最后把匹配成功的加密数据返回给客户端。客户端对数据进行解密，显示给用户。
虽然加密索引的研究取得了重大进步，但是很多方案都有一个很严重的漏洞，就是泄漏了存取模式（access patterns）：关键词与数据的匹配过程。虽然关键词和索引都是已经加密的，但是它们的匹配过程是在服务端的内存进行的，而服务端的内存是被攻击者掌控的，因此攻击者可以对匹配的过程进行分析，从而从加密的索引中恢复敏感信息。除了存取模式的遗漏，还有如果攻击者可以获得匹配成功的文件数（result size），也可以恢复出敏感信息。
一种很直接的隐藏存取模式的方法就是使用 ORAM（Oblivious RAM），然而这个方案的成本十分昂贵，因此很少方案采用这个方法。针对这个问题，文章提出了一个称为 Oblix (OBLivious IndeX) 的索引方案，这个方案既不会泄漏任何的存取模式，也不会泄漏匹配文件数。进一步，Oblix 允许索引进行插入和删除、支持多用户。
二、解决方案
文章提出的 Oblix 主要针对以下四种问题进行解决：
1、高复杂度：对于 ORAM 系统，客户端维护一个位置映射数据结构（position map），该数据结构记录了索引与数据库中某个数据的位置的对应关系。由于索引的大小与数据库的数据量成线性关系，因此客户端不能直接存储整个 map。一种标准的方案是使用树形结构将一个数据库分解成多级数据库（类似于多级索引），以减小客户端需要的存储空间。但是如此一来就会需要重复多次查询来确定最终的位置，显然树搜索的时间复杂度是 O(logN)，这样的复杂度会带来查询延迟。文章使用的解决方案是使用飞地技术（enclave），把整个 ORAM 的客户端放进飞地中，这样让客户端和服务端就在同一台机器上进行，不需要通过网络，从而减小网络延迟。
2、飞地可能会被攻击者利用：有研究发现飞地即使物理上隔离了外界的控制，但是也可能被攻击者通过分析它的物理页级的存取模式（page-level access pattern）来恢复被加密的数据。因为飞地自身的内存空间很小，当飞地处理的数据量较大时，它就会使用二级存储，使用计算机中的内存。而计算机中的内存是会被攻击者控制的，因此攻击者可以通过观察飞地如果使用计算机主存来进行攻击。文章的解决方案：首先，提出一个称为 ODS (oblivious data structure) 的数据结构来取代 position map，这个数据结构保证不可被攻击者利用。其次，提出一个称为 doubly oblivious 的方案，该方案同时保证对服务端的存取、对客户端自己的内存空间的存取模式都是不可被攻击者利用的（oblivious）。
3、查询结果数的隐藏：前面提到，查询结果数可能也会被攻击者用，因此需要对其进行隐藏。一种最简单的做法是“worst-case upper bound”，但是这种做法代价太昂贵。文章提出的解决方案：对查询结果进行打分排序，只返回得分最高的前 r 条数据。
4、请求列表、查询效率：客户端可以一次向服务端发送多个请求，也就请求列表。特别地，其中包含插入和删除得操作时，执行效率就尤其重要。文章解决方案：提出一个称为 DOSM (doubly-oblivious sorted multimap) 的数据结构，支持高效的范围查询、插入、删除等操作。它实际上是一个树形的数据结构，插入和删除的复杂度是 O(logN) 而不是 O(n)。另外再结合特定的查询算法来保证范围查询的高效性。
三、总结
文章主要针对存取模式的漏洞来进行修复。攻击虽然无法直接攻击飞地，无法直接攻击加密数据，但是可以通过应用对主存的存取模式进行分析从而恢复加密数据。这种存取模式在文章中是指例如索引这样的负责映射数据的数据结构。]]></description>
</item><item>
    <title>Winnower</title>
    <link>https://utopizza.github.io/2019-01-12-%E8%AE%BA%E6%96%87-winnower/</link>
    <pubDate>Sat, 12 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-12-%E8%AE%BA%E6%96%87-winnower/</guid>
    <description><![CDATA[一、背景及问题
这篇文章针对的问题和前面看过的 《Dependence-Preserving Data Compaction for Scalable Forensic Analysis》和 这篇文章类似，目标都是减小日志文件的体积，以减少存储空间并提高攻击侦查的速度。
这篇文章主要针对的是分布式服务器集群下的 APT 攻击。在这样的情景中，服务器集群有一个中心监控节点（Monitor node），以及大量的工作节点（Worker nodes）。监控节点与所有的工作节点保持通信。工作节点在生产环境中不断地产生日志记录。当发起监控节点攻击侦查分析时，它需要从多个工作节点上拷贝日志文件到本地，再进行攻击分析。问题就出现在这里，大量的工作节点每天都产生大量的日志记录，监控节点几乎不可能从所有工作节点上拷贝全部的日志记录，因为这样的拷贝任务不但需要消耗大量的网络资源，并且需要消耗大量的存储空间。
二、解决方案
这篇文章和上述的那篇文章有点类似，其核心思想都是通过对资源图（Provenance Graph）进行剪枝、压缩、优化，从而减小对应的日志记录体积。不同的只是他们使用的剪枝算法不同而已，这篇文章使用的算法是一个称为 Deterministic Finite Automata (DFA) 的算法，这个算法可以对一个复杂的图中的每一个节点、边（对应系统日志记录中的事件以及信息流向）进行推断，判断出它是不是冗余的，如果是冗余的就可以去掉，从而极大提精简了日志记录。
文章提出的模型称为 Winnower，具体剪枝算法较复杂，主要分为图抽象（Provenance Graph Abstraction）和图推断（Provenance Graph Induction）两部分，其中涉及一些图语义学习（Graph Grammar Learning）的知识，以前没怎么接触过，感觉不太懂，以后有时间再补一下这方面的学习。]]></description>
</item><item>
    <title>PriorTracker</title>
    <link>https://utopizza.github.io/2019-01-05-%E8%AE%BA%E6%96%87-priortracker/</link>
    <pubDate>Sat, 05 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-05-%E8%AE%BA%E6%96%87-priortracker/</guid>
    <description><![CDATA[一、背景及问题
APT（Advanced Persistent Threat）攻击是一种长期、多步骤的攻击，它往往会涉及多个系统事件和文件。因此一旦发现系统收到这种攻击时，必须执行攻击因果分析（Attack Causality Analysis ），即从一个受到攻击的事件或者文件作为入口，向前追踪其攻击源头，并且向后分析所有被攻击牵涉中的其他文件或数据。
一般来说，攻击因果分析是基于系统资源的依赖图进行的。依赖图记录了系统中各种事件的先后次序和信息流，因此依赖图往往十分巨大，特别对于拥有大量机器的大型企业来说。以往的研究都是关注于如何对这个巨大的依赖图进行剪枝，以减少需要分析的数据量。但是这些研究都没有考虑执行分析时的时间限制，只对所有数据一视同仁地依次执行分析。文章提出，时效性对与攻击因果分析来说是非常重要的，因为在发现受到攻击后，越快地分析出攻击源以及受到污染的数据，就能越快地采取隔离措施或者修复措施，减少企业损失。据统计，90% 的 APT 攻击都是在数分钟内完成的。因此目前的问题就是，如何在保证攻击不会被遗漏和 CPU 运算能力一定的前提下，如何尽可能快地执行攻击因果分析？
二、解决方案
文章提出一个称为 PriorTracker 的模型， 其基本思路是，以往的研究没有办法加速攻击因果分析的的原因是，他们没有办法区分出异常的系统事件和正常的系统事件，因此他们只能对依赖图中的所有路径一视同仁地去执行分析，包括一些复杂但实际上与攻击无关的正常的系统调用事件，这就导致了分析的时候花费了大量的时间和算力在无关的数据上。如果我们能够有办法区分出异常的系统事件和正常的系统事件，那么就可以优先对异常事件进行分析，最快地命中攻击。
那么如果甄别出系统中的异常事件？文章提出的一个方案是，用算法对每个系统事件（System Event）计算一个优先权（Priority Score），优先权高的事件先执行分析。优先权的计算由如下公式给出
$$PriorityScore(e)=\alpha \cdot RarenessScore(e) + \beta \cdot FanoutScore(e)$$
其中 $e$ 表示资源依赖图中的一个系统事件；$RarenessScore$ 表示事件的稀有得分，因为异常事件必定是正常系统的过往历史中很少出现的事件；$FanoutScore$ 表示扇出得分，如果一个事件的扇出（也即在资源依赖图中的一个节点的出度）越多，那么执行这个事件的分析所花费的时间越多。另外从统计经验知道，一般来说一些正常的系统事件例如更新进程数据等等的扇出度是非常巨大的，但是这类数据对攻击分析是没什么贡献的，只会浪费 CPU 时间。因此一个自然的想法是优先计算扇出较小的事件节点，别免浪费时间。当然这也会导致分析覆盖面的减小，但是这样的 tradeoff 总的来说是需要的。而 $\alpha$ 和 $\beta$ 是两者的平滑系数，文章中使用机器学习方法来训练这两个参数。两者的计算公式如下：
 $$RarenessScore(e)= \begin{cases} 1, & \text{if e has not been observed by reference model} \\ \frac{1}{ref(e)}, & \text{otherwise} \end{cases}$$   $$FanoutScore(e)= \begin{cases} 0,& \text{if e reaches a read-only file in backtracking} \\ \delta,& \text{if e reaches a write-only file in forward tracking} \\ \frac{1}{fanout(e)}, & \text{otherwise} \end{cases} $$  其中 $ref(e)$ 由文章定义的一个称为 $Reference Model$ 的模型给出，其主要思想是统计一个系统事件在一个时间段（一周或者一个月，这个可以根据企业的生产情况自行调节）内的出现总次数。而 $fanout(e)$ 文章没有给出计算公式，估计就是节点出度？]]></description>
</item><item>
    <title>PII</title>
    <link>https://utopizza.github.io/2018-12-29-%E8%AE%BA%E6%96%87-pii/</link>
    <pubDate>Sat, 29 Dec 2018 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2018-12-29-%E8%AE%BA%E6%96%87-pii/</guid>
    <description><![CDATA[一、背景及问题
最近一些广告平台（Data Broker）例如 Facebook、Google 等推出了新的广告投放策略：广告投放商（Advertiser）可以在平台中上传目标用户的个人身份信息（Personally Identifying Information，PII）如用户的姓名、邮箱地址、电话号码、住址、生日等，以在该平台中直接定位目标用户并针对性地投放广告。与传统的数据经销商直接把数据访问权限卖给给广告投放商的方式不同，这些数据平台只允许广告投放商上传目标用户的 PII，然后平台在内部匹配这些用户，返回一个匹配成功并进行舍入（Rounding）后的广告投放目标用户（Audience）总数给广告投放商。
然而这样的策略会威胁到用户的隐私安全。虽然广告投放商不能直接访问用户的数据，但是依然可以利用这个策略的漏洞来推测用户的其他隐私信息，例如恶意的广告投放商可以利用它们手中的一些用户邮箱地址，通过平台以及某些手段来推出这些用户的姓名、电话号码、住址等等其他隐私信息。
这个策略的漏洞就是上述平台返回的匹配成功的用户总数。虽然 Facebook 等平台对该数进行了舍入（Rounding），返回的用户总数是模糊的，但是只要攻击者通过不断提交差别很小的请求，依然可以把取整策略的阈值测试出来，然后就可以反推出在阈值边界上的目标用户（Threshold audiences）的信息。例如当攻击者在第二次提交请求时，只比第一次请求多添加一个用户，而返回的成功匹配用户数从原来的 80 变成了 90，那么很显然这个用户就是在阈值边界上并且符合攻击者的匹配目标的用户。整篇文章所提的攻击都是通过此漏洞来进行的，论文原话：“We use threshold audiences throughout the paper to enable our attacks”
二、解决方案
具体地说，文章介绍的的攻击手段主要有以下三种：
1、逆匿名网页浏览用户（De-anonymizing web visitors）：广告投放商可以在一些外部网页上安装一个 Facebook 的 JavaScript 插件，Facebook 可以通过该插件追踪浏览过此网页的用户，然后将这些用户记录下来，作为广告目标用户集合（注意，这些用户对广告投放商来说是透明的，只有 Facebook 知道这些用户）。现在，攻击者（恶意的广告投放商）可以利用上述的漏洞来反推出某个用户 $V$ 是否访问了这个网页。攻击方法是： (1) 先确定 $V$ 是否是 Facebook 平台中的用户：提交一系列不含攻击目标 $V$ 的请求 ${ L_1,L_2,\cdots,L_n }$，每次请求只比上一次请求多一个用户，Facebook 返回的匹配用户总数分别为 ${A_1,A_2,\cdots,A_n }$。先测试出一个舍入阈值下界，例如，假设 $A_1$、$A_2$ 是 810，而 $A_3$ 是 820，那么 $A_2=810$ 是一个舍入下界。然后我们再在 $L_2$ 这个集合中加入攻击目标 $V$ 即 ${L_2 \bigcup V }$ 并提交给 Facebook，如果返回的是 810，那么 $V$ 不是 Facebook 平台中的用户，如果返回的是 820，那么说明 $V$ 是 Facebook 中的一个用户。 (2) 接下来，确定 $V$ 是否在 Facebook 追踪的网页浏览者名单中：同样地，提交一系列包含 $V$ 的请求，每个请求只比前一个请求多一个用户，然后测试出舍入上界，例如 ${L_1 \bigcup V}$、${L_2 \bigcup V}$ 是 930，而 ${L_3 \bigcup V}$ 是 940，那么 $A_3 = 940$ 就是攻击者要找的上界。此时，再提交不含 $V$ 的 ${L_4}$，看返回的结果是否下降到 930，如果下降了，说明 $V$ 就在浏览者名单中，即可以确定攻击目标 $V$ 浏览过了攻击者的网页。]]></description>
</item><item>
    <title>SVIM</title>
    <link>https://utopizza.github.io/2018-12-22-%E8%AE%BA%E6%96%87-svim/</link>
    <pubDate>Sat, 22 Dec 2018 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2018-12-22-%E8%AE%BA%E6%96%87-svim/</guid>
    <description><![CDATA[差分隐私（Differential Privacy）模型是微软研究院的 Cynthia Dwork 等人于 2006 年提出的一个严谨的数学模型，目标在于提出用以修改隐私数据的技术，使得修改后的数据可以安全发布(以供第三方进行研究)，而不会遭受去匿名化等隐私攻击。同时，修改后的数据要在保护隐私的前提下最大限度地保留原数据的整体信息，否则被发布的数据将毫无研究价值。
该论文主要提出了一个称为 Set-Value Item Mining (SVIM) 的协议，用于在隐私保护约束条件下提高本地差分隐私数据的精确度。由于差分隐私主要是纯理论的模型，没有具体的实现和应用，限于篇幅此文不展开论述。此处主要了解差分隐私的一些背景即可。
相关资料：
 差分隐私学习总结 苹果的 Differential Privacy 差分隐私技术是什么原理？ 大数据下的信息安全-差分隐私保护技术  ]]></description>
</item><item>
    <title>EnclaveDB</title>
    <link>https://utopizza.github.io/2018-12-15-%E8%AE%BA%E6%96%87-enclavedb/</link>
    <pubDate>Sat, 15 Dec 2018 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2018-12-15-%E8%AE%BA%E6%96%87-enclavedb/</guid>
    <description><![CDATA[一、背景及问题
目前一些云端的数据处理服务经常受到可能来自数据库管理员、服务器管理员、利用操作系统漏洞的黑客等的攻击。虽然一般意义上的数据加密可以为静态数据和数据传输提供强力高效的保护，但是却无法为数据处理系统提供足够的保护，因为数据处理系统在处理查询的时会在内存解密敏感数据。而内存是容易被攻击者入侵并掌控的区域，因此这会导致数据的不安全。
一种实现安全查询（secure query）的方法是，一些系统如 CryptDB，Monomi 以及 Seabed 等通过使用 property-preserving encryption 技术来进行在已加密数据上的安全查询。然而这种方法只能处理十分有限的查询请求，并且有信息泄漏的风险。
另一种实现安全查询的方法是在受信任的执行环境（trusted execution environment）或者内飞地（enclaves） 中执行查询。系统的内飞地如 Intel Software Guard Extensions (SGX) 可以保护敏感数据和代码，即使攻击者控制了整个操作系统或者主机。虽然如此，使用内飞地存在几个问题：
 为了实现安全保护，需要仔细地对应用程序进行重构，分解出可信任部分和不可信任部分 为了实现高层次的安全特性如机密性（confidentiality）、完整性（integrity）、时新性（freshness），需要增加额外的逻辑和机制来确保机密信息在进入和离开内飞地的时候不会被泄漏  这些问题对于一些简单的应用程序来说并不困难，但是对于一些大型复杂系统如数据库系统来说则非常不容易。先前的一些研究例如 CipherBase、TrustedDB 等采取的方式是将查询引擎（query engine）的一小部分放置于受信任的硬件之上，这种方式的问题是无法提供上述的机密性、完整性、时新性。另一种选择是将整个数据库放置于内飞地之中，但这显然会导致大量的 trusted computing base (TCB) 以及服务性能的下降，并且无法阻止来自数据库管理员的攻击。
 【根据维基百科，飞地指在某个地理区划境内有一块隶属于他地的区域。根据地区与国家之间的相对关系，飞地又可以分为“外飞地”（Exclave）与“内飞地”（Enclave）两种概念，其关系如下：内飞地（enclave）：意指某个国家境内有块土地，其主权属于另外一个国家，则该地区称为此国家的内飞地。外飞地（exclave）：某国家拥有一块与本国分离开来的领土，该领土被其他国家包围，则该领土称为某国的外飞地。】
 二、解决方案
文章提出了一个称为 EnclaveDB 的数据库，它可以为查询和数据提供机密性、完整性、时新性。它与传统关系型数据库一样提供了数据处理、SQL查询、存储过程等基本功能。与传统数据库不同的是：
 EnclaveDB 通过把所有敏感数据（tables, indexes, queries and other intermediate state）维护在内飞地的内存中来保护数据库状态 EnclaveDB 使用可信任的 EnclaveDB 客户端来预编译查询代码，并进行加密 预编译代码后，受信任的 EnclaveDB 客户端与不受信任的服务器上的内飞地直接建立一个安全的加密通道，然后将已加密的查询和参数发送过去 不受信任的服务器上的内飞地收到查询请求后，对请求进行验证、解密、执行查询、对查询结果进行加密、返回给客户端 EnclaveDB 重新设计了数据库的日志与恢复机制，以确保服务器无法对数据库的日志进行攻击 EnclaveDB 采取了一些优化手段，减少线程上下文切换，减少服务器开销  三、总结
文章提出了一个通过使用内飞地来实现不信任服务器上的安全数据库。通过可信任客户端的预编译、加密安全通道的通信、内飞地的隔离保护、重新设计数据库的日志与恢复机制等一系列措施，实现了机密性、完整性、时新性这三个重要的安全特性。]]></description>
</item></channel>
</rss>
