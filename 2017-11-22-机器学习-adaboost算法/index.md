# Adaboost算法


 
一、提升方法（boosting）

提升方法主要用于分类问题，它的基本思想是，通过改变训练样本的权重，学习多个不同的分类器，最后把这些基本分类器（也称弱分类器）通过线性组合，得到最终的强分类器。

这里所谓的提升，通俗地说其实就是将一个分类问题交给多个分类器来处理，“对于一个复杂任务来说，将多个专家的判断进行适当的综合，得出的最终判断，要比任何一个专家单独的判断要好”。虽然每一个弱分类器都是只是一个窄领域的专家，但是把一系列这样的弱分类组合到一起，得到的分类能力并不比单个强分类器差。而且直接学习一个强分类器远比学习一个弱分类器难。

目前大多数提升方法是通过不断改变训练数据的概率分布（样本权重分布）来不断学习出一系列弱分类器。那么这样需要确定两个问题：

 1. 在每一轮如何改变训练样本的权值或者概率分布
 2. 如何将这些弱分类器组合成一个强分类器

提升方法中的代表性算法有 Adaboost 算法，它的做法是：

 1. 在每一轮学习后，提高前一轮学习中被错误分类的样本的权值，降低前一轮学习中被正确分类的样本的权值。这样，后面学习的分类器将会专注于前面的分类器不能很好处理的那些样本，弥补了前面的分类器的不足（窄领域）。举个例子，小明和小红在学习分类一堆水果，小明先学习，他在学习分类的时候由于精力和时间有限，在一轮学习后，小明只能很好地对体积大的水果进行分类，对那些体积小的水果经常分错类。然后到小红学习的时候，为了保证两个人最终能够把这堆水果都正确分类，那么聪明的小红应该知道，自己应该专注于对那些小明不擅长的体积小的水果进行学习，因为如果小明能够很好地分类体积大的水果，而自己能够很好地分类体积小的水果，那么两个人组合互补起来得到的分类能力，远比两个人都强行学习对所有水果分类的效果好得多。
 2. 对于弱分类器的组合，Adaboost 采取加权多数表决的方法，即加大分类误差小的弱分类器的权值，使其在最终表决的时候起较大作用，减小分类误差较大的弱分类器的权值，使其在最终表决中起较小作用。这个很容易理解，谁分类的误差小、准确度高，当然谁在最终的表决里就有更大的话语权了。

二、Adaboost 算法

算法目标：给定一个二分类的训练数据集 $T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$，其中 $x_i \in R^n$ 是样本实例，$y_i \in \{-1,+1\}$ 是样本类别标记。本算法目标是从该训练数据集中，学习出 $M$ 个弱分类器，最后将这些弱分类器通过**线性组合**，得到最终的一个强分类器。

输入：训练数据集 $T$，弱学习算法
输出：最终分类器 $G(x)$

1、初始化训练集 $N$ 个样本的权重分布

$$D_1=(w_{1,1},\cdots,w_{1,i},\cdots,w_{1,N})$$

$$w_{1,i}=\frac{1}{N}，i=1,2,\cdots,N$$
 
2、对 $M$ 个分类器，开展对应的 $m=1,2,\cdots,M$ 轮学习。对第 $m$ 轮学习：

(1)、用训练数据集 $T$、第 $m$ 轮的样本权重 $D_m$、和弱分类器学习算法，学习得到第 $m$ 个弱分类器 

$$G_m(x):X \to \{-1,+1\}$$

(2)、计算 $G_m(x)$ 在训练集上的分类误差率

$$e_m=P(G_m(x_i) \neq y_i)=\sum_{i=1}^{N}w_{m,i}I(G_m(x_i) \neq y_i)$$

(3)、计算 $G_m(x)$ 的权重

$$\alpha_m=\frac{1}{2}\ln(\frac{1}{e_m}-1)$$

(4)、计算下一轮学习的样本权重分布

$$D_{m+1}=(w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N})$$

$$
w_{m+1,i}=\frac
{w_{m,i} \cdot e^{-\alpha_m y_i G_m(x_i)}}
{\sum_{i=1}^{N} w_{m,i} \cdot e^{-\alpha_m y_i G_m(x_i)}}
，i=1,2,\cdots,N
$$

3、执行完 $M$ 轮学习后，得到 $M$ 个弱分类器 $G_1(x),G_2(x),\cdots,G_M(x)$，通过线性组合得到最终表决

$$f(x)=\sum_{m=1}^{M} \alpha_m \cdot G_m(x)$$

加上符号函数 $sign()$，从而得到最终分类器

$$G(x)=sign(f(x))=sign \left( \sum_{m=1}^{M} \alpha_m \cdot G_m(x) \right)$$

三、Adaboost 的解释

Adaboost 算法是模型为**加法模型**、损失函数为**指数函数**、学习算法为**前向分步算法**时的**二分类**学习方法。

1、加法模型（additive model）：

$$f(x)=\sum_{m=1}^{M} \beta_m \cdot b(x;\gamma_m)$$

其中 $b(x;\gamma_m)$ 为基函数，$\gamma_m$ 为基函数的参数，$\beta_m$ 为基函数的系数。

2、前向分步算法

具体参见《统计学习方法》，这里直接给出结论：由前向分步算法可以推导出 Adaboost 算法，Adaboost 算法是前向分步算法的一个特例。以后有时间再回来补这部分推导。

3、Adaboost 的训练误差

Adaboost 的训练误差可以证明是指数下降的。

