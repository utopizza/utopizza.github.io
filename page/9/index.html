<!DOCTYPE html>
<html lang="en">
    <head>
	<meta name="generator" content="Hugo 0.69.2" />
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="robots" content="noodp" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge, chrome=1">
        <title>Utopizza</title>
        <meta name="Description" content="About LoveIt Theme"><meta property="og:title" content="Utopizza" />
<meta property="og:description" content="About LoveIt Theme" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://utopizza.github.io/" />
<meta property="og:image" content="https://utopizza.github.io/logo.png"/>
<meta property="og:updated_time" content="2019-09-16T00:00:00+00:00" />
<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://utopizza.github.io/logo.png"/>

<meta name="twitter:title" content="Utopizza"/>
<meta name="twitter:description" content="About LoveIt Theme"/>
<meta name="application-name" content="Utopizza">
<meta name="apple-mobile-web-app-title" content="Utopizza"><meta name="theme-color" content="#ffffff"><meta name="msapplication-TileColor" content="#da532c"><link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" />
        <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
        <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"><link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png"><link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5"><link rel="manifest" href="/site.webmanifest"><link rel="canonical" href="https://utopizza.github.io/" /><link rel="alternate" href="/index.xml" type="application/rss+xml" title="Utopizza">
    <link rel="feed" href="/index.xml" type="application/rss+xml" title="Utopizza"><link rel="stylesheet" href="/lib/normalize/normalize.min.css"><link rel="stylesheet" href="/css/style.min.css"><link rel="stylesheet" href="/lib/fontawesome-free/all.min.css"><link rel="stylesheet" href="/lib/animate/animate.min.css"><script type="application/ld+json">
    {
        "@context": "http://schema.org",
        "@type": "WebSite",
        "url": "https:\/\/utopizza.github.io\/","inLanguage": "en","author": {
                "@type": "Person",
                "name": "yusheng"
            },"description": "About LoveIt Theme","image": "https:\/\/utopizza.github.io\/cover.png","thumbnailUrl": "https:\/\/utopizza.github.io\/logo.png","license": "This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.","name": "Utopizza"
    }
    </script></head>
    <body><script type="text/javascript">(window.localStorage && localStorage.getItem('theme') ? localStorage.getItem('theme') === 'dark' : ('auto' === 'auto' ? window.matchMedia('(prefers-color-scheme: dark)').matches : 'auto' === 'dark')) && document.body.setAttribute('theme', 'dark');</script>

        <div id="mask"></div><div class="wrapper"><header class="desktop" id="header-desktop">
    <div class="header-wrapper">
        <div class="header-title">
            <a href="/" title="Utopizza"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span>Utopizza</a>
        </div>
        <div class="menu">
            <div class="menu-inner"><a class="menu-item" href="/posts/"> Posts </a><a class="menu-item" href="/categories/"> Categories </a><a class="menu-item" href="https://github.com/utopizza" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i>  </a><span class="menu-item delimiter"></span><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                    <i class="fas fa-adjust fa-fw"></i>
                </a>
            </div>
        </div>
    </div>
</header><header class="mobile" id="header-mobile">
    <div class="header-container">
        <div class="header-wrapper">
            <div class="header-title">
                <a href="/" title="Utopizza"><span class="header-title-pre"><i class='far fa-kiss-wink-heart fa-fw'></i></span>Utopizza</a>
            </div>
            <div class="menu-toggle" id="menu-toggle-mobile">
                <span></span><span></span><span></span>
            </div>
        </div>
        <div class="menu" id="menu-mobile"><a class="menu-item" href="/posts/" title="">Posts</a><a class="menu-item" href="/categories/" title="">Categories</a><a class="menu-item" href="https://github.com/utopizza" title="GitHub" rel="noopener noreffer" target="_blank"><i class='fab fa-github fa-fw'></i></a><a href="javascript:void(0);" class="menu-item theme-switch" title="Switch Theme">
                <i class="fas fa-adjust fa-fw"></i>
            </a></div>
    </div>
</header>
<div class="search-dropdown desktop">
    <div id="search-dropdown-desktop"></div>
</div>
<div class="search-dropdown mobile">
    <div id="search-dropdown-mobile"></div>
</div>
<main class="main">
                <div class="container"><div class="page home"><div class="home-profile"><div class="home-avatar"><a href="/posts/" title="Posts"><img
        class="lazyload"
        src="/svg/loading/small.min.svg"
        data-src="/avatar.jpg"
        data-srcset="/avatar.jpg, /avatar.jpg 1.5x, /avatar.jpg 2x"
        data-sizes="auto"
        alt="Posts"
        title="Posts" /></a></div><h1 class="home-title">CODE FOR FOOD :)</h1><h2 class="home-subtitle"><div id="id-1" class="typeit"></div></h2><div class="social-links"><a href="https://github.com/utopizza" title="GitHub" target="_blank" rel="noopener noreffer me"><i class="fab fa-github-alt fa-fw"></i></a><a href="mailto:648847079@qq.com" title="Email" rel=" me"><i class="far fa-envelope fa-fw"></i></a></div></div>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/2017-12-23-%E6%80%BB%E7%BB%93-%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%932/">阶段总结(2)</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>yusheng</a></span>&nbsp;<span class="post-publish">
            published on&nbsp;<time datetime=2017-12-23>2017-12-23</time>
        </span>&nbsp;
            <span class="post-category">included in<a href="/categories/summary/">
                        <i class="far fa-folder fa-fw"></i>Summary
                    </a></span></div><div class="content">上一次总结是8月17日，到现在刚好过去了4个月。2018元旦快到了，研二上学期也接近尾声，稍微总结一下这四个月的工作。
9月份先花了了半个月左右的时候整理了本科期间的大部分技术文档和比赛经验，补了些博客。后半个月开始看《算法导论》，主要精力集中于动态规划那一章，自己用 Java 实现了书上的几个算法并按照自己的理解写了几篇博客。之后感觉做题还是太少，理解和熟练程度都不够，便开始在网上搜一些动态规划的经典题目进行补充。这个月基本以补博客为主
10月份中秋国庆假回了趟家，到10月9日左右才回学校。这个月过的比较混乱，同时在尝试做论文和算法，写博客。但因为论文的数据问题进度被卡住了，所以主要精力貌似集中于刷 leetcode 的算法题了，那段时间刷了大概有100多道，大部分是 Easy 难度。19号左右，同届的专硕同学和上一届的学硕学长找完工作回来了，请教了不少经验，要了个书单，不过帮助不大，他们找的都是前后台研发岗，不是机器学习和算法岗。这个月可以说主要以刷 leetcode 为主
11月份，大神舍友带我打天池比赛。11月8日左右弄到了服务器，选好题，选好了 baseline 模型 LightGBM，我就完全停止了论文和算法的学习，全部精力放在了比赛上面。第一次正式使用 Python 的 pandas，numpy，sklearn 这些工具库来实现机器学习，花了不少精力，学习到了很多非常实用的知识。因为 lgb 的本质是梯度提升树，所以我回头去看书，复习了一遍决策树，提升算法，集成学习，提升树与回归树，推导了一些细节，写了一系列博客。
12月份，当然是继续以比赛为主要工作内容。不断尝试新的想法，代码更新并整理了一遍又一遍，成绩排名还行，一直在前20。另外由于题目比较坑，官方的数据有问题，重新发布了两次数据集，导致我们之前的大量调试工作都白费了。同时也逐渐感到遇到提高的瓶颈了，需要继续深入学习新的知识，我们又尝试了模型融合，但是效果并不是很理想。这个月主要精力在调试参数和补决策树的理论知识为主。另外向舍友大神要了个书单和coursera的视频单，但是感觉难度较大，也没太多时间去看了，主要看了吴恩达的《Machine Learning》，并完成作业。
总的来说，这四个月来完成的主要工作：
 整理出了本科时的部分技术博客 读《算法导论》，研究并练习动态规划算法 刷了100多道 leetcode 题，从 Easy 难度开始入手 参加天池大数据比赛，初步学习正规使用 Python 看吴恩达《Machine Learning》视频，做课后作业 看了一些 Kaggle 的资料，初步了解 Kaggle   接下来计划：
 继续弄好天池，争取拿个拿得出手的成绩 继续完成吴恩达《Machine Learning》的后半部分课程 开始刷 leetcode 的 medium 难度的题，适时总结 论文希望能有时间推进进度吧   推荐资料：
 《程序员的数学》（日）结成浩等 《概率论与数理统计》 陈希孺（中科大版） 《具体数学：计算机学科基础》 Knuth等 《Python for Data Analysis》 《Hands-On Machine Learning with Scikit-Learn and TensorFlow》 Coursera:《Machine Learning Specialization》 Coursera:《Neural Networks and Deep Learning》 Coursera:《Neural Networks for Machine Learning》  </div><div class="post-footer">
        <a href="/2017-12-23-%E6%80%BB%E7%BB%93-%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%932/">Read More</a></div>
</article>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/2017-12-12-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9C%A8%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E4%B8%AD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC/">逻辑回归在梯度提升树中损失函数的梯度推导</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>yusheng</a></span>&nbsp;<span class="post-publish">
            published on&nbsp;<time datetime=2017-12-12>2017-12-12</time>
        </span>&nbsp;
            <span class="post-category">included in<a href="/categories/machinelearning/">
                        <i class="far fa-folder fa-fw"></i>MachineLearning
                    </a></span></div><div class="content">一、梯度提升树
因为做比赛最近用到了 LightGBM 来实现逻辑回归，并尝试修改 LightGBM 的目标函数，于是顺便温习一下梯度提升树和逻辑回归，并简单推导了一下逻辑回归在梯度提升树中的损失函数的梯度计算。
前面的博文介绍过，当损失函数是平方损失和指数损失时，前向加法模型的提升树（回归提升树）可以很方便地进行目标函数的优化并构造决策树。但对于一般的损失函数，前向加法提升树就比较难实现优化。这时就需要使用梯度提升模型的提升树，它能适应一般的损失函数。类似最速下降法，梯度提升树利用损失函数的负梯度在当前模型的值
$$- \left[ \frac{\partial L(y,f(x_i))}{\partial f(x_i)} \right]$$
作为回归提升树算法中的残差的近似值，来拟合一个回归树。
二、逻辑回归
逻辑回归是比较简单而且应用非常广泛的机器学习算法。它主要用于二分类任务，但是与一般的分类器不同，逻辑回归并不直接给出未知样本的预测类别，而是给出属于某个类别的概率。
对一个二分类任务，每个训练样本都属于且只属于两种类别之中的一种，即要么是正样本，要么是负样本。习惯上，一般用 $0$ 表示负样本，用 $1$ 表示正样本。
设训练集一共有 $m$ 个样本，每个样本有 $n$ 个属性，第 $i$ 个训练样本表示为 $x_i=(x_{i}^{(1)},x_{i}^{(2)},\cdots,x_{i}^{(n)})$，$1 \leq i \leq m$。对应地，每个样本有一个类别，设为 $Y$，则对第 $i$ 个样本，要么 $Y_i=1$，要么 $Y_i=0$。
现在的任务是，给定 $m$ 个已知对应类别的样本 ${(x_1,Y_1),(x_2,Y_2),\cdots,(x_m,Y_m)}$，用来作为训练集，学习一个分类器模型 $f(x,Y)$，然后用这个模型来对未知类别的样本如 $(x_{m+1},？)$ 进行预测，预测它的类别 $Y_{m+1}$ 是等于 $1$ 还是等于 $0$。
逻辑回归使用 $sigmod$ 函数作为预测模型，它不直接判定某个样本是正样本还是负样本，而是给出一个条件概率，即在已知样本的 $n$ 个属性的情况下，该样本属于正样本或负样本的概率。数学描述为
$$P(Y=1 \mid x)=\frac{1}{1+e^{-w \cdot x}}$$
$$P(Y=0 \mid x)=1-\frac{1}{1+e^{-w \cdot x}}$$
其中 $w$ 是 $n$ 维的向量，每一维对应样本 $x$ 的 $n$ 个特征，可以理解为：$w^{(i)}$ 表示训练样本 $x$ 第 $i$ 个特征 $x^{(i)}$ 的权重。$w \cdot x$ 为两个向量的内积，即</div><div class="post-footer">
        <a href="/2017-12-12-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9C%A8%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E4%B8%AD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC/">Read More</a></div>
</article>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/2017-12-05-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-randomforest%E4%B8%8Egbdt/">RandomForest与GBDT</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>yusheng</a></span>&nbsp;<span class="post-publish">
            published on&nbsp;<time datetime=2017-12-05>2017-12-05</time>
        </span>&nbsp;
            <span class="post-category">included in<a href="/categories/machinelearning/">
                        <i class="far fa-folder fa-fw"></i>MachineLearning
                    </a></span></div><div class="content">一、随机森林（RandomForest）
前面学习过了 bagging 方法，我们知道该方法是通过有放回随机抽样来构建 $S$ 个和原数据集大小相等的训练集，然后分别进行训练，得到 $S$ 个分类器，再把这些分类器通过多数表决的方式组合得到最终的分类器。
随机森林算法正是 bagging 方法的一种扩展。随机森林算法不仅对训练集的样本（行）进行抽样，而且对训练集的特征（列）也进行抽样。具体来说，随机森林分为三个步骤：
1、随机采样
对于行采样，使用有放回随机采样，和常规 bagging 方法一致，前面已经介绍过，此处不再赘述。
对于列采样，假设样本有 $M$ 个特征（即训练集有 $M$ 列），那么该算法在每次决策树的节点要分裂的时候，随机选择其中 $m$ 个特征作为考察的特征子集（即只在这 $m$ 个特征之中选出最优特征作为分裂决策，而不是像传统决策树那样考察全部特征）。满足 $m &laquo; M$ ，即每次随机选择的子特征集的大小应远小于总特征集的大小，一般情况下推荐取 $m=log_2(M)$。
行采样的目的是 “用有限数据模拟无限数据”，而列采样的目的是使每个决策树专注于一小部分特征的学习，使其成为各自的 “窄领域专家”，当最终把这些 “擅长于不同领域的专家” 组合到一起时，就可以大大减少 “所有专家犯同样错误” 的可能，也即过拟合的可能。
2、完全分裂
因为有了上一步采样的过程，最终分类器的过拟合现象基本不可能发生，因此在学习各个决策树的时候就按照完全分裂的方式来构造，无须剪枝。如在执行分类任务时，分裂的决策依据就可以选择常规决策树的生成算法的决策依据，如 ID3 算法的信息增益等。
3、执行决策
当学习完成，得到 $S$ 个彼此独立的决策树后，就可以把这些决策树组合在一起，作为最终的分类器。组合的方式是常规 bagging 方式，即在对预测输出进行结合时，让各个分类器分别执行预测，得到 $S$ 个预测结果，如果预测任务是分类任务则使用投票法选择票数最多的那个类别返回，如果是回归任务则使用均值法取这些预测结果的均值返回。
二、梯度提升树（Gradient boosting decision tree，GBDT）
梯度提升树是 boosting 提升方法中的一种。它的提出是为了解决 “回归提升树在使用一般损失函数的时候，求解目标函数时每一步的优化比较困难” 这一问题。之前学习的回归提升树在使用前向分步算法求解目标函数时，使用的损失函数是指数函数，每一步的优化很简单。但如果要扩展到一般的损失函数，就不那么容易了。因此 Freidman 提出了梯度提升（gradient boosting）算法，利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值作为回归提升树中的残差的近似值，来拟合一个回归树。
梯度提升算法：
输入：训练数据集 $T$，输入空间 $X$，输出空间 $Y$，损失函数 $L(y,f(x))$ 输出：回归提升树 $f_M(x)$
1、初始化
$$f_0(x)=argmin\sum_{i=1}^{N}L(y_i,c)$$</div><div class="post-footer">
        <a href="/2017-12-05-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-randomforest%E4%B8%8Egbdt/">Read More</a></div>
</article>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/2017-11-30-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-boosting%E4%B8%8Ebagging/">boosting与bagging</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>yusheng</a></span>&nbsp;<span class="post-publish">
            published on&nbsp;<time datetime=2017-11-30>2017-11-30</time>
        </span>&nbsp;
            <span class="post-category">included in<a href="/categories/machinelearning/">
                        <i class="far fa-folder fa-fw"></i>MachineLearning
                    </a></span></div><div class="content">一、boosting
前面已经学习过，boosting 是一种提升方法，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行组合，提高分类的性能。
boosting 方法每一轮学习一个分类器，并根据本次学习的误差，改变整个样本集合中每个样本的权重，被误分类的那些样本的权重将增大。在下一轮学习新的分类器时，这些被误分类的样本将会被赋予更大的关注。
因此可以看出，boosting 方法是通过串行训练而获得的，下一轮的学习是基于上一轮的误差进行的。另外，每一轮的训练集都是整个原数据集，数据集中的样本不变，变的是各个样本的权重。
最后，在学习基本分类器时，会同时计算得到每个分类器的权重。boosting 的最终组合分类器是这一系列基本分类器的加权组合。
目前比较典型的两种 boosting 算法是 Adaboosting （Adaptive Boosting，自适应boosting）算法，和 GBDT（gradient boosting decision tree，梯度提升决策树）算法。
二、bagging
自举汇聚法（bootstrap aggregating），也称 bagging 方法。该方法通过从原数据集中随机放回抽样，得到 $S$ 个和原数据集大小相等的数据集，来作为 $S$ 次训练的训练集，从而训练得到 $S$ 个分类器。
因为是放回抽样，所以新的数据集中可以有重复的样本，原数据集也可以有部分样本不在新数据集中出现。
得到了 $S$ 个分类器后，就将这些分类器组合成最终的分类器。执行预测时，让这 $S$ 个分类器分别对新数据进行预测，得到 $S$ 个预测结果。如果是分类任务，则采用多数表决的方式，选择这 $S$ 个结果中票数最多的那个类别返回。如果是回归任务，则取均值作为最终结果返回。
目前比较典型的 bagging 类的方法有随机森林（Random Forest）。
三、区别
1、学习方式
boosting 的每一轮学习是基于前一轮的误差，因此它的一系列基本分类器的训练是串行的。bagging 是随机不放回抽样得到若干个新的训练集进行各自的训练，彼此间没有联系，可并行。
2、训练样本
boosting 不改变样本，而是在每一轮根据误差改变每个样本的权重。bagging 不改变权重，而是从原数据集抽选其中一部分样本（可重复）来构成不同的训练集。
3、分类器组合方式
boosting 的最终分类器是基本分类器的加权之和，每个分类器有各自的权重，最终的预测结果是每个分类器的预测结果的加权之和。bagging 的分类器没有权重的概念，每个基本分类器都有相等权重的一票，最终的预测结果是票数最多的那一个类（标签）。</div><div class="post-footer">
        <a href="/2017-11-30-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-boosting%E4%B8%8Ebagging/">Read More</a></div>
</article>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/2017-11-26-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E6%8F%90%E5%8D%87%E6%A0%91%E4%B8%8E%E5%9B%9E%E5%BD%92%E6%8F%90%E5%8D%87%E6%A0%91/">分类提升树与回归提升树</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>yusheng</a></span>&nbsp;<span class="post-publish">
            published on&nbsp;<time datetime=2017-11-26>2017-11-26</time>
        </span>&nbsp;
            <span class="post-category">included in<a href="/categories/machinelearning/">
                        <i class="far fa-folder fa-fw"></i>MachineLearning
                    </a></span></div><div class="content">一、提升树（boosting tree）
提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。提升方法采用加法模型（即基函数的线性组合）与前向分步算法。对分类问题，决策树是二叉分类树；对回归问题，决策树是二叉回归树。
提升树模型可以表示为决策树的加法模型：
$$f_M(x)=\sum_{m=1}^{M}T(x;\theta_m)$$
其中，$T(x;\theta_m)$ 表示决策树，$\theta_m$ 为决策树的参数，$M$ 为决策树的个数。
二、二分类问题的提升树
对于二分类问题，提升树算法只需将 Adaboost 算法中的基本分类器限制为二分类树即可。
目前比较流行的方案是采用一种简单决策树作为基本分类器：单层决策树（decision stump，也称决策树桩）。它仅仅基于单个特征来执行决策，因此对应地只有一个树结点：根节点。它只能执行一次二分类，如 $x &lt; v$ 或 $x &gt; v$。
三、回归问题的提升树
回归树的形式化描述：设训练数据集 $T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$，$x_i \in X \subseteq R^n$，$y_i \in Y \subseteq R$，$X$ 是输入空间，$Y$ 是输出空间。如果将输入空间 $X$ 划分为 $J$ 个不相交的区域 $R_1,R_2,\cdots,R_J$，并且在每个区域上有确定的输出常量 $c_j$，那么回归树可以表示为：
$$T(x;\theta)=\sum_{j=1}^{J}c_j \cdot I(x \in R_j)$$
其中，参数 $\theta={(R_1,c_1),(R_2,c_2),\cdots,(R_J,c_J)}$ 表示回归树的区域划分和各区域上的输出常量（即输出的回归值），$J$ 是回归树的复杂度（即叶结点个数）。
回归问题的提升树算法：
输入：如上，训练数据集 $T$，输入空间 $X$，输出空间 $Y$ 输出：回归提升树 $f_M(x)$
1、初始化 $f_0(x)=0$
2、对 $M$ 个分类器，进行对应的第 $m=1,2,\cdots,M$ 轮学习。对第 $m$ 轮学习：
(1)、计算以前一轮为止，目前学习到的回归提升树 $f_{m-1}(x)$ 的残差
$$r_{mi}=y_i-f_{m-1}(x_i)，i=1,2,\cdots,N$$</div><div class="post-footer">
        <a href="/2017-11-26-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E6%8F%90%E5%8D%87%E6%A0%91%E4%B8%8E%E5%9B%9E%E5%BD%92%E6%8F%90%E5%8D%87%E6%A0%91/">Read More</a></div>
</article>
<article class="single summary" itemscope itemtype="http://schema.org/Article"><h1 class="single-title" itemprop="name headline">
        <a href="/2017-11-22-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-adaboost%E7%AE%97%E6%B3%95/">Adaboost算法</a>
    </h1><div class="post-meta"><span class="post-author"><a href="/" title="Author" rel=" author" class="author"><i class="fas fa-user-circle fa-fw"></i>yusheng</a></span>&nbsp;<span class="post-publish">
            published on&nbsp;<time datetime=2017-11-22>2017-11-22</time>
        </span>&nbsp;
            <span class="post-category">included in<a href="/categories/machinelearning/">
                        <i class="far fa-folder fa-fw"></i>MachineLearning
                    </a></span></div><div class="content">一、提升方法（boosting）
提升方法主要用于分类问题，它的基本思想是，通过改变训练样本的权重，学习多个不同的分类器，最后把这些基本分类器（也称弱分类器）通过线性组合，得到最终的强分类器。
这里所谓的提升，通俗地说其实就是将一个分类问题交给多个分类器来处理，“对于一个复杂任务来说，将多个专家的判断进行适当的综合，得出的最终判断，要比任何一个专家单独的判断要好”。虽然每一个弱分类器都是只是一个窄领域的专家，但是把一系列这样的弱分类组合到一起，得到的分类能力并不比单个强分类器差。而且直接学习一个强分类器远比学习一个弱分类器难。
目前大多数提升方法是通过不断改变训练数据的概率分布（样本权重分布）来不断学习出一系列弱分类器。那么这样需要确定两个问题：
 在每一轮如何改变训练样本的权值或者概率分布 如何将这些弱分类器组合成一个强分类器  提升方法中的代表性算法有 Adaboost 算法，它的做法是：
 在每一轮学习后，提高前一轮学习中被错误分类的样本的权值，降低前一轮学习中被正确分类的样本的权值。这样，后面学习的分类器将会专注于前面的分类器不能很好处理的那些样本，弥补了前面的分类器的不足（窄领域）。举个例子，小明和小红在学习分类一堆水果，小明先学习，他在学习分类的时候由于精力和时间有限，在一轮学习后，小明只能很好地对体积大的水果进行分类，对那些体积小的水果经常分错类。然后到小红学习的时候，为了保证两个人最终能够把这堆水果都正确分类，那么聪明的小红应该知道，自己应该专注于对那些小明不擅长的体积小的水果进行学习，因为如果小明能够很好地分类体积大的水果，而自己能够很好地分类体积小的水果，那么两个人组合互补起来得到的分类能力，远比两个人都强行学习对所有水果分类的效果好得多。 对于弱分类器的组合，Adaboost 采取加权多数表决的方法，即加大分类误差小的弱分类器的权值，使其在最终表决的时候起较大作用，减小分类误差较大的弱分类器的权值，使其在最终表决中起较小作用。这个很容易理解，谁分类的误差小、准确度高，当然谁在最终的表决里就有更大的话语权了。  二、Adaboost 算法
算法目标：给定一个二分类的训练数据集 $T={(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)}$，其中 $x_i \in R^n$ 是样本实例，$y_i \in {-1,+1}$ 是样本类别标记。本算法目标是从该训练数据集中，学习出 $M$ 个弱分类器，最后将这些弱分类器通过线性组合，得到最终的一个强分类器。
输入：训练数据集 $T$，弱学习算法 输出：最终分类器 $G(x)$
1、初始化训练集 $N$ 个样本的权重分布
$$D_1=(w_{1,1},\cdots,w_{1,i},\cdots,w_{1,N})$$
$$w_{1,i}=\frac{1}{N}，i=1,2,\cdots,N$$
2、对 $M$ 个分类器，开展对应的 $m=1,2,\cdots,M$ 轮学习。对第 $m$ 轮学习：
(1)、用训练数据集 $T$、第 $m$ 轮的样本权重 $D_m$、和弱分类器学习算法，学习得到第 $m$ 个弱分类器
$$G_m(x):X \to {-1,+1}$$
(2)、计算 $G_m(x)$ 在训练集上的分类误差率
$$e_m=P(G_m(x_i) \neq y_i)=\sum_{i=1}^{N}w_{m,i}I(G_m(x_i) \neq y_i)$$
(3)、计算 $G_m(x)$ 的权重
$$\alpha_m=\frac{1}{2}\ln(\frac{1}{e_m}-1)$$
(4)、计算下一轮学习的样本权重分布
$$D_{m+1}=(w_{m+1,1},\cdots,w_{m+1,i},\cdots,w_{m+1,N})$$
$$ w_{m+1,i}=\frac {w_{m,i} \cdot e^{-\alpha_m y_i G_m(x_i)}} {\sum_{i=1}^{N} w_{m,i} \cdot e^{-\alpha_m y_i G_m(x_i)}} ，i=1,2,\cdots,N $$</div><div class="post-footer">
        <a href="/2017-11-22-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-adaboost%E7%AE%97%E6%B3%95/">Read More</a></div>
</article>
<ul class="pagination"><li class="page-item ">
                    <span class="page-link">
                        <a href="/">1</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/7/">7</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/8/">8</a>
                    </span>
                </li><li class="page-item active">
                    <span class="page-link">
                        <a href="/page/9/">9</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/10/">10</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/11/">11</a>
                    </span>
                </li><li class="page-item ">
                    <span class="page-link" aria-hidden="true">&hellip;</span>
                </li><li class="page-item ">
                    <span class="page-link">
                        <a href="/page/17/">17</a>
                    </span>
                </li></ul></div></div>
            </main><footer class="footer">
        <div class="footer-container"><div class="footer-line">Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener noreffer" title="Hugo 0.69.2">Hugo</a> | Theme - <a href="https://github.com/dillonzq/LoveIt" target="_blank" rel="noopener noreffer" title="LoveIt 0.2.6"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a>
                </div><div class="footer-line"><i class="far fa-copyright fa-fw"></i><span itemprop="copyrightYear">2019 - 2020</span><span class="author" itemprop="copyrightHolder">&nbsp;<a href="/" target="_blank">yusheng</a></span>&nbsp;|&nbsp;<span class="license"><a rel="license external nofollow noopener noreffer" href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank">CC BY-NC 4.0</a></span></div>
        </div>
    </footer></div>

        <div id="fixed-buttons"><a href="#" id="back-to-top" class="fixed-button" title="Back to Top">
                <i class="fas fa-arrow-up fa-fw"></i>
            </a><a href="#" id="view-comments" class="fixed-button" title="View Comments">
                <i class="fas fa-comment fa-fw"></i>
            </a>
        </div><link rel="stylesheet" href="/lib/katex/katex.min.css"><link rel="stylesheet" href="/lib/katex/copy-tex.min.css"><script type="text/javascript">
    window.config = {"code":{"copyTitle":"Copy to clipboard","maxShownLines":100},"data":{"id-1":"Stay hungry, stay foolish."},"headerMode":{"desktop":"fixed","mobile":"auto"},"math":{"delimiters":[{"display":true,"left":"$$","right":"$$"},{"display":true,"left":"\\[","right":"\\]"},{"display":false,"left":"$","right":"$"},{"display":false,"left":"\\(","right":"\\)"}],"strict":false},"typeit":{"cursorChar":"|","cursorSpeed":1000,"data":{"id-1":["id-1"]},"duration":-1,"speed":100}};
</script><script type="text/javascript" src="https://polyfill.io/v3/polyfill.min.js?features=html5shiv%2CElement.prototype.closest%2CrequestAnimationFrame%2CCustomEvent%2CPromise%2CObject.entries%2CObject.assign%2CObject.values%2Cfetch%2CElement.prototype.after%2CArray.prototype.fill%2CIntersectionObserver%2CArray.from%2CArray.prototype.find%2CMath.sign"></script><script type="text/javascript" src="/lib/smooth-scroll/smooth-scroll.min.js"></script><script type="text/javascript" src="/lib/lazysizes/lazysizes.min.js"></script><script type="text/javascript" src="/lib/object-fit-images/ofi.min.js"></script><script type="text/javascript" src="/lib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="/lib/sharer/sharer.min.js"></script><script type="text/javascript" src="/lib/typeit/typeit.min.js"></script><script type="text/javascript" src="/lib/katex/katex.min.js"></script><script type="text/javascript" src="/lib/katex/auto-render.min.js"></script><script type="text/javascript" src="/lib/katex/copy-tex.min.js"></script><script type="text/javascript" src="/lib/katex/mhchem.min.js"></script><script type="text/javascript" src="/js/theme.min.js"></script></body>
</html>
