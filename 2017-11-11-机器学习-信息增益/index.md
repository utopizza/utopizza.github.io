# 信息增益




一、熵（entropy）

熵是表示随机变量不确定性的度量。熵越大，随机变量的不确定性越大。设 $X$ 是一个取值为有限个的离散型随机变量，其概率分布为：

$$P(X=x_i)=p_i \text{ , } i=1,2,\cdots,n$$

则随机变量 $X$ 的熵定义为

$$H(X)=-\sum_{i=1}^{n}p_i\log(p_i)$$

当式中对数以 $2$ 为底时，熵的单位为比特（bit）；以自然对数 $e$ 为底时，熵的单位为纳特（nat）。


二、条件熵（conditional entropy）

条件熵 $H(Y \mid X)$ 表示已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。

设有随机变量 $(X,Y)$，联合分布概率为

$$P(X=x_i,Y=y_i)=p_{ij} \text{ , } i=1,2,\cdots,n \text{ ; }j=1,2,\cdots,m$$

则随机变量 $X$ 给定条件下，随机变量 $Y$ 的条件熵为

$$H(Y \mid X)=\sum_{i=1}^{n} p_i H(Y \mid X=x_i)$$

这里 $p_i=P(X=x_i) \text{ , } i=1,2,\cdots,n$。

当熵和条件熵的概率有数据估计得到时，所对应的熵与条件熵称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。

三、信息增益（information gain）

信息增益表示得知特征 $X$ 的信息而使得类 $Y$ 的信息的不确定性减少的程度。

特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$ 定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D \mid A)$ 之差

$$g(D,A)=H(D)-H(D \mid A)$$

进一步，如果 $A$ 的取值把 $D$ 划分成 $D_1,D_2,\cdots,D_n$ 这 $n$ 个子集，那么

$$H(D \mid A)=\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)$$

因此

$$g(D,A)=H(D)-\sum_{i=1}^{n}\frac{|D_i|}{|D|}H(D_i)$$

上述公式可以理解为，选择特征 $A$ 带来的信息增益为：原数据集的熵（混乱程度，也可以理解为不确定性），减去被特征 $A$ 划分成 $n$ 个子集后这 $n$ 个子集的熵的期望，而得到的差值。

我们在构建决策树时的目标是：使每次被划分数据集的不确定性尽可能小。由于原数据集的熵已经确定，那么，如果选择某个特征划分数据集后，整个数据集的不确定性减小最多（即信息增益最大），那么该特征就是当前最优的分类特征。

找到最优特征后，取该特征下的每一个值构建一个子结点，把数据集划分成 $n$ 个子集合。例如，假设当前最优特征为“年龄”，该特征下一共有“青年”、“中年”、“老年”三种取值，那么在决策树的当前结点下面新建对应的三个孩子结点，把数据集按这三种取值划分为三个子集，传递到对应的三个孩子结点。

四、信息增益比（information gain ratio）

以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。用信息增益比可以对这一问题校正。

特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即

$$g_R(D,A)=\frac{g(D,A)}{H_A(D)}$$

其中

$$H_A(D)=-\sum_{i=1}^{n}\frac{|D_i|}{|D|}\log_2\frac{|D_i|}{|D|}$$

$n$ 是特征 $A$ 取值的个数。


