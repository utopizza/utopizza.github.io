<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Utopizza</title>
        <link>https://utopizza.github.io/</link>
        <description>About LoveIt Theme</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>648847079@qq.com (yusheng)</managingEditor>
            <webMaster>648847079@qq.com (yusheng)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 04 Aug 2020 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://utopizza.github.io/" rel="self" type="application/rss+xml" /><item>
    <title>Kafka</title>
    <link>https://utopizza.github.io/2020-08-04-%E8%AE%BA%E6%96%87-kafka/</link>
    <pubDate>Tue, 04 Aug 2020 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2020-08-04-%E8%AE%BA%E6%96%87-kafka/</guid>
    <description><![CDATA[一、背景问题
在互联网公司中往往会产生大量的日志数据（log data），例如（1）用户事件如用户的登录、网页访问、点击、收藏、分享、评论、搜索；（2）内部系统产生的事件例如服务调用、错误日志、网络事件、系统调用等等。这些日志数据十分重要，可用于分析用户喜好、系统使用情况等等。以往这些日志数据一般用于离线的数据分析，然而随着互联网业务蓬勃的发展，越来越需要更加实时地利用这些日志数据来进行在线分析，例如（1）根据用户搜索历史进行实时更新的个性化推荐、相关推荐、广告投放（2）系统实时识别垃圾邮件、非法数据并自动进行过滤等等。因此，一些早期的系统，例如Facebook的Scribe，Yahoo的Data Highway，Cloudera的Flume等等，都是被设计成把日志数据进行收集并写入Hadoop这样的数据仓库然后进行离线的分析，不能满足进行实时大规模日志在线分析的需求。业界需要一个能支持延迟秒级的实时（real-time）日志收集系统。
二、Kafka
1、基本概念&amp;架构
 一个message流被称为一个topic，每个topic下有多个partition分区（分区数量十分重要，因为它决定了可以并行消费的并行度，见下文） 存储message的kafka节点称为broker。Kafka是一个分布式系统，有多个broker节点，每个broker节点存储一个或者多个partition分区 每个producer可以推送message到一个topic 每个consumer可以订阅一个或者多个topic，从对应的broker中拉取message  2、Efficiency
（1）简单的存储结构
每个topic的每个partition分区对应一个逻辑日志，该日志物理上其实是由一系列的文件片（segment file）构成，每个文件片大约1GB。每当producer推送一个message到某个partition时，broker只是简单地把该message追加到最后一个片文件的末尾。考虑到写磁盘的性能问题，kafka只有在指定数量的message到达、或者指定时间周期结束才执行一次flush把文件片写入磁盘。
至于message的数据结构，与传统message系统不同，kafka中的message并没有所谓的“messageId”，每个message只根据它在日志中的offset来定位：下一条message在日志中的位置等于当前message的位置+message的长度。
consumer从一个partition消费message时，它只能从某个offset开始连续地消费，并且该offset之前的message已经完成消费。consumer在向broker提交每个的pull请求中都会带上所请求的message的offset以及请求的字节数。broker根据offset找到对应的文件片（会在内存中维护一个“文件片首部offset”与“文件片”的映射关系），定位message，然后发送被请求的数据返回给consumer。consumer收到数据后，计算出下一个message的offset然后再继续发送请求。
（2）高效的传输方式
producer可以在单个send请求中一次性提交多个message；同样，consumer也可以在单个pull请求中一次性拉取多个message。
另外，kafka为了防止数据在机器上被缓冲两次（double buffering），它不在内存中缓存任何数据，所有数据只利用了操作系统的页缓冲（page cache，应该就是操作系统底层为写磁盘提供的内核缓冲区）。这样做的好处，文中解析是基本不用考虑内存的垃圾回收，并且在kafka进程重启的时候可以很方便的从页缓冲中重新加载数据（如果是维护在内存中，进程吃消失后分配的内存会被回收释放）
更进一步，kafka对网络传输也做了优化。传统的程序把文件数据发送到网络中一般需要以下几个步骤：
 从磁盘或者其他存储媒体把文件数据读到操作系统内核的缓冲区（page cache） 从内核缓冲区把数据拷贝到应用程序中的buffer 把数据从应用程序中的buffer又重新拷贝到内核中的另一个缓冲区（可能是为网卡分配的缓冲） 把内核缓冲区中的数据发送到socket套接字层，进行网络传输  可见上述过程经历了4次拷贝：文件-》内核缓冲-》应用程序缓冲-》内核缓冲-》socket。然而在Linux或者Unix类的操作系统中，其实有一个名为“sendfile”的API可以直接把文件读到一个内核缓冲后直接发送到socket，kafka利用了该API进行高效的文件数据发送：文件-》内核缓冲-》socket。
（3）无状态broker
在kafka中，每个consumer消费到哪个offset位置并不会被kafka记录，而是由consumer自己进行记录。这样设计的好处是极大了简化了kafka系统的复杂度。但是这样会引入另一个问题：kafka无法知道一个message是否已经被所有的consumer消费过，是否可以进行删除。kafka使用了一个简单解决方案——每个message保留7天，超过该时间的message会被删除。这样设计的好处一是简洁，二是有一定的容灾能力，例如consumer崩溃或者出现一些故障后，可以减小offset从头消费，重新获取消费错误或丢失的数据。
3、分布式协作
在kafka中，可以定义一个consumer组（consumer group），每个这样的组消费一个指定的topic，该topic下的每个partition只能被组中的一个consumer消费，该consumer可以是一个线程或者进程。这样设计的好处是足够简单，因为如果有多个consumer同时消费一个partition，势必需要进行同步来保证message不会被一个组重复消费，可能需要引入锁，这样会使得消费效率大打折扣。因为规定了组内一个consumer只能消费一个partition，因此组内的consumer同步只会出现在进行rebalance的时候，而这是一个出现频率很低的场景。
对于kafka节点结构，它并没有设计成master-slave结构，而是每个broker均为同等地位的数据节点，但是会把一些重要的集群信息存放到Zookeeper中。Zookeeper在kafka中负责的任务主要有：
 监测broker、consumer的加入和离开 触发一次rebalance，当上一点发生的时候 记录consumer与partition对应关系，追踪每个partition的offset  4、交付保证
kafka只保证“至少一次”的数据交付（at-least-once）。如果外部应用程序不能引入重复数据，则它需要自行实现去重逻辑（deduplicated logic）。kafka只保证单个partition内的message会被按顺序交付给consumer，并不会保证不同partition之间的message的交付顺序。]]></description>
</item><item>
    <title>Raft</title>
    <link>https://utopizza.github.io/2020-07-26-%E8%AE%BA%E6%96%87-raft/</link>
    <pubDate>Sun, 26 Jul 2020 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2020-07-26-%E8%AE%BA%E6%96%87-raft/</guid>
    <description><![CDATA[一、背景及问题
共识算法（consensus algorithm）在实现大型分布式系统的可靠性方面扮演了十分关键的角色，因为它允许集群在部分机器故障的情况下保持任务正常运行。在过去十年中，主流的共识算法叫Paxos，大部分系统都使用了这个算法，并且该算法一度成为标准的教材。然而，该算法有一个缺陷就是过于复杂，想要完全理解或者完美实现都十分困难。因此，斯坦福的Diego Ongaro和John Ousterhout便提出了一个相对容易理解、容易实现的共识算法，命名为Raft。
二、相关概念
Replicated state machine：复制状态机，是共识算法的起源之处。像GFS、HDFS等的大规模分布式系统，其leader节点都只有一个，存在单点故障问题。要保证集群即使出现该故障的情况下自动恢复正常运行，如集群配置信息、leader选举心跳信息等数据必须克服单点故障的问题，这就要求这些重要的数据有可靠的方式来进行备份。一种可靠的方式就是使用复制状态机，如下图所示，复制状态机一般是通过复制日志（replicated log）来实现。每个机器上都会维护一份复制日志，日志的内容是一系列的command，机器会按日志的顺序来执行这些command，来修改自己的状态。只要保证每个机器上的复制日志是完全一致，那么就可以保证每个机器的操作结果是一致的。其中，共识算法的主要作用就是通过一些通信手段，来保持集群中每个机器的复制日志完全一致。目前比较有名的复制状态机的实现是Zookeeper和Chubby。
三、Raft算法
总的来说，Raft首先会从集群中选举一个leader，整个集群的复制日志完全由leader来维护：leader节点接收client端发送过来的日志实体（log entry），然后把日志实体分发给集群中其他节点，并告诉其他节点什么时候可以安全地执行这些日志，从而修改他们的状态。当leader宕机故障时，集群会自动发起选举，选出一个新的leader继续保持集群运行。为了便于理解和实践，Raft把共识问题拆分成三个彼此独立的子问题，从而逐一解决：（1）leader选举（2）日志复制（3）安全
1、Raft基础
Raft集群（一般规模为5台机器）中的节点在任何时刻（可以运行的情况下）必为以下三种状态之一：（1）leader（2）follower（3）candidate。在正常情况下，集群中只有一个leader，其余均为follower。follower不响应任何client端请求，它只会把这些请求重定向给leader，并且只响应leader的请求。
Raft把时间分割成任意长度的周期（term），这些周期用连续的整数来唯一地标识，可以理解为周期id。每个周期都由leader选举开始，在这个周期内如果一个节点当选为leader，它会一直作为leader直到这个周期结束。集群中每个节点自身都会维护一个周期id作为自己看到的当前周期。当节点之间进行通信时会带上自己的当前周期，如果节点发现对方周期id比自己大，则更新自己的当前周期；如果节点发现自己的当前周期已经过期，则马上把自己转为follower状态；如果节点发现对方的周期已经过期，则直接拒绝对方的请求。
2、leader选举
Raft使用心跳机制（heartbeat）来触发leader选举。当Raft集群刚启动时，所有节点都是follower状态。follower节点一段时间内发现没有收到来自leader的心跳，认为leader节点已经失效（election timeout），则马上把自己转变为candidate状态，并将自己的当前周期增大。它会先为自己投票，然后向集群中其他节点发起拉票请求（RequestVote RPC）。candidate节点会一直持续这个状态直到（1）它获得多数票从而赢得选举成为新leader（2）其他节点胜出成为新leader（3）选举超时，没有选出新leader。
Raft投票规则：（1）每个节点每次选举周期内最多投出一票（2）先到先得原则，给先请求的节点投票（2）candidate只给自己投一票，然后并发地向其他节点拉票（4）特殊情况下，可能会进入僵持状态——例如所有节点同时进入candidate状态，只给自己投票，然后等等其他人给自己投票，此时的集群状态称为“split”。该论文解决这个问题使用了一种简单的方法，就是随机话选举超时时间，避免多个节点同时开启选举周期，使得先开启选举的节点有先发优势成为新leader，减少集群不可用时间。
当一个candidate成为新leader后，马上开始向其他节点发送心跳宣告自己的leader身份。其他节点确认该新leader的当前周期是有效周期后，认可新leader的身份，把自己转变为follower；否则无视该leader，继续保持选举。
3、日志复制
当节点成为leader后，开始负责响应client的请求。client的请求会包含一个需要整个集群都执行的command。leader首先会把该command追加到自己的日志中，然后通过RPC（AppendEntries RPCs）把日志分发到其他节点。若某个follower宕机或者网络丢包没有复制成功，leader会无限地重新尝试，直到复制成功。分发完成后，leader执行该command，修改自己的状态，然后把执行结果返回给client。具体关于日志的组织形式、压缩方法，详见原论文。
4、安全性
这部分较为繁琐，详见原论文。]]></description>
</item><item>
    <title>阶段总结(9)</title>
    <link>https://utopizza.github.io/2020-01-20-%E6%80%BB%E7%BB%93-%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%939/</link>
    <pubDate>Mon, 20 Jan 2020 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2020-01-20-%E6%80%BB%E7%BB%93-%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%939/</guid>
    <description><![CDATA[上一次写阶段总结是2019年6月，6月底毕业搬回家后，在家呆了两周不到，7月10日就来深圳入职了。从7月入职到现在2020年1月，已过去了6个月的试用期并且顺利转正。现对这半年试用期做个简要的总结。
2019年7月
系统里定的7月10号报到入职，因此在家里只呆了不到两周就上深圳了。刚入职就遇到团建——东西冲穿越，不知道是暴晒的原因还是过度运动的原因，回来竟然发烧、鼻炎复发。先后看了两次医生，做了鼻镜检查，浪费了不少时间，不过还好没事。并进行了安全域串讲，没过。这次串讲最大的问题是没有了解清楚串讲的内容需求，以为只要讲清楚策略审计模块就行，结果被说没有完整介绍安全域。还要讲清楚数据从页面、采集端、计算、存储、再到页面展示这一整个过程，感觉这两周时间全部弄清楚还是有点难度。总结了几点问题：
 对安全域认识的广度和深度都不够，首先需要弄懂产品定位，从一个俯瞰的角度来看整个产品，有一个全局的认识。例如说，为什么需要安全域，因为公司拥有大量的高价值数据，需要保护这些核心数据不被泄露或盗窃，那么就行需要为数据划定安全的边界，这就是『域』的意义由来。安全域是在公司有了BILS和BAAS这两套基础系统之后为了更进一步保护公司核心数据，然后在其基础上搭建的。还有一个问题就是今天只讲了安全域的感知和审计功能，漏掉了阻断功能，后面需要补充 应该注重细节。要弄清楚数据从被采集到的那一刻开始，从到被传输、被流式处理、落地到数据库、被分析计算、最后到呈现在Web页面上，每一步是怎么走的。反过来从用户提交一个数据或者在Web的一个操作，到数据是怎么被系统接受，下发给各种agent，对系统产生什么影响，也要讲清楚。 在阅读代码的时候应该多思考，为什么这样设计，分别有什么好处和不足之处，为什么要这样tradeoff，等等。同时应该善于总结，抽象出一个个的pattern，为以后面对类似的设计问题时都会非常有帮助 要多考虑工程上的特殊情况，例如服务器宕机了对系统有什么影响，丢失的数据能重现吗，定时任务执行太久导致新来的任务和旧的任务都来不及执行怎么办，什么时候使用反射什么时候使用工厂等等 阅读代码需要理解到可以动手改代码，对代码进行优化或者重构的程度，才能说是真正理解了代码  2019年8月
8月分主要是在弄入职后第一个研发issue，账单分摊系统GianoBills。第一次进行了详细设计评审，结果不理想。评审后开始研发，到月底都基本就是在进行编码实现，其中被日志模块坑了很久。一直没有弄清楚日志模块，导致错误的设计、错误的编码、卡住了很多进度。总结以下几点：
 进行详细设计讲解、技术分享等等之前，最好先自己演练几次，把握一下时间，时间应该多准备几个版本，例如5分钟版本、15分钟版本、30分钟版本等 应该注意抓整体，先讲整体的思路、流程、架构，让大家有一个整体的认识，再根据时间是否允许来一步步展开细节，不要过早陷入太多detail 应该突出主要的一些设计亮点、要点，如果只有平铺直叙是无法吸引大家的注意和兴趣的 在实现一个功能之前，应该先看看目前业界是怎么实现的，有哪些优秀的设计方案，成熟的设计是怎么做的 一定要看官方文档，按照正确的方式来使用API，不要想当然，例如生成多个logger对象这样愚蠢的想法  2019年9月
9月份主要是在进行docker的虚拟化实现，因为没有接触过docker，学习了好一段时间。把账单分摊系统打包进docker的时候遇到了很多配置环境的坑，拖了很久。9月份还做了把crate接入安全域的工作，以及试写Giano SDK接入、安全域运营后台、flume维护等，都是比较杂的小任务。
2019年10月
10月份主要在虚拟化galaxy storm集群，迁移到公司的paas化平台Opera。先是要充分了解storm，所以花了比较多时间看storm的官方文档，在自己笔记本搭建虚拟机集群然后部署storm集群。然后是弄清楚galaxy storm上运行的topology，花了很多时间。加上国庆放假，回来后开始迁移工作。迁移过程实在太多坑太艰辛了，18号周五还莫名其妙弄崩了storm集群，要周围很多同事一起来重新提交所有job，感觉十分愧疚。回来在自己笔记本上想复现那个故障，但是却运行得好好的，完全没有问题，真是郁闷到爆炸。后来查了很多资料发现，故障原因是搭建线下的storm测试集群时，使用了和线上集群同一个zookeeper，但是因为这个storm版本太老还不支持多个nimbus同时运行，然后两个集群都直接崩。就这样躺了版本过旧的坑（这个坑在高版本的storm中被修复了）。月底重新串讲了安全域，因为准备了好久，得到了组内的一致高度肯定和赞扬。
2019年11月
11月份主要是在做BILS故障查询系统GianoCheck，用golang来实现一个http server，于是抽了一个周末的时间学习golang，然后一周内基本就完成了这个任务，这个速度得到了大家的好评。产品体验也得到相关用户的高度评价。第一次在入职后体验到了一些小成就感。另外还做了黄金眼的串讲、一些线上服务的bug定位，安全域运营后台的优化等等。
2019年12月
12月份主要在做一些述职的准备工作，其余时间是日常的运维。由于10月paas化spe-storm后，有很多遗留问题，因此经过详细的讨论后，决定对spe进行优化，用原生storm重写spe，聚合相同的审计任务，把130个审计任务压缩成4个通用任务，优化后单域任务启动耗时从1个小时缩减到3秒以内，机器资源需求数量从30台缩减到5台，优化效果相当明显。这次对原有复杂系统的重构优化，确实从中学到很多东西，比如storm上的分布式任务编程、多节点选主、缓存实时刷新、读写冲突、流式数据批量写库提高吞吐等等。
2020年1月
1月10日顺利转正，总算熬过了试用期。1月份应该也是主要在研发SPE优化项目，以及一些日常运维。
二、后续计划
最近想了下，工作之余应该有自己的学习计划和安排，否则会浪费很多时间。因此制作个最近的一些学习小目标吧：
 代码规范：这是工程师的最基本素养（python已看，java未看） 多看代码，总结设计模式：同为最基本素养之一 学习大数据组件：hadoop，spark，storm，kafka，zookeeper，flume，thrift，protobuf 学习虚拟化组件：docker，k8s 学会使用上述组件之后，进行源码学习，尝试在git中进行contribute，争取被accept leetcode要重新开始刷，每天至少一道到两道 学有余力的话，学习新语言：mapreduce，storm topology，golang 英语单词落下一段时间了，要重启百词斩和听力练习了 关注下hk的实验室和团队，还有research方向 坚持锻炼身体  ]]></description>
</item><item>
    <title>阶段总结(8)</title>
    <link>https://utopizza.github.io/2019-06-30-%E6%80%BB%E7%BB%93-%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%938/</link>
    <pubDate>Sun, 30 Jun 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-06-30-%E6%80%BB%E7%BB%93-%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%938/</guid>
    <description><![CDATA[上一次写阶段总结是2019年4月底，现在是6月底，这几个月主要都是在学校忙毕业论文和毕业答辩的事情。
2019年5月 毕业答辩日期安排在5月26日，因此这个月基本全部精力时间都花在论文上。论文主体在4月份基本已经写完了，5月份基本都是在补一些报告书如开题报告、中期检查报告等等。月初和舍友去木兰天池玩了一天，山上挺凉快的。中间抽时间在宿舍看了一部电视剧《我们离恶有多远》，相当震撼，因为剧中探讨的问题十分有深度。剧情比较虐心，让人看得都郁闷了很久。就这样刷刷剧、写写文档，就浑浑噩噩到了答辩日期。答辩地点就在实验室旁边的一个大会议室，但是评委老师一个都没见过，刚开始还是有点紧张的。由于准备还算较充分，流畅地讲完了PPT，评委老师也没问什么很深入的问题，就问了我在汉口那边呆了多久，什么时候回来的，我说3月回来的，老师还表扬了一波我说这么短时间能弄出这么多公式理论，还不错。。大概就是就这样水过了硕士答辩。硕士阶段的最后一块大石终于落地。答辩完了基本就知道通过了。回去的路上，感觉整个世界都不一样，虽然是下午四点多，但是感觉阳光特别特别的明媚。
2019年6月 答辩完了之后，回去就是整理各种报告、文档，然后去打印店打印封装论文。然后就是各种浪了，白天蹲在宿舍天天看《bigbang》，下午就约几个小伙伴一起去游泳、密室逃脱、攀岩、射箭、桌游、吃吃喝喝。月初端午节百度寄来了一箱很粽子，16号的时候学校还举办了毕业晚餐会。18号晚上老爸老妈到汉，带他们吃了西餐，然后安顿好酒店。19号带他们逛了武大、省博、楚河汉街。20号上午是毕业典礼，下午弄离校手续，旁晚去拍了毕业照。21号主要是收拾宿舍东西，寄东西，下午带爸妈去逛光谷、拍照。晚上和舍友聚餐。22号早上等爸妈过来吃了早饭，就跟舍友告别，滴滴去高铁站了。
如此，研究生生涯正式告一段落，顺利毕业。]]></description>
</item><item>
    <title>PageRank在Spark的分布式实现</title>
    <link>https://utopizza.github.io/2019-05-19-%E7%AE%97%E6%B3%95-pagerank%E5%9C%A8spark%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0/</link>
    <pubDate>Sun, 19 May 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-05-19-%E7%AE%97%E6%B3%95-pagerank%E5%9C%A8spark%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0/</guid>
    <description><![CDATA[最近在研究 MapReduce 和 Spark 的相关资料，顺便补个关于如何在虚拟机中搭建的 Spark 上分布式运行 PageRank 算法的博客。犹记得这个小任务是研一开学时，导师布置的第一个小任务，而现在马上就要硕士毕业答辩了，不禁思绪万千。
一、PageRank 算法
PageRank 算法是谷歌的起家算法，凭借该算法谷歌击败了当时所有的其他门户网站以及搜索引擎。该算法的目的是对数以亿计的网页进行排序，重要的网页将被排在前列，作为搜索结果返回给用户。想起了昆丁的电影《低俗小说》中的对白：“如果你要把一具尸体藏起来，你知道世界上哪里最安全吗？那就是谷歌搜索结果的第二页”。言外之意，谷歌搜索的前几条解决方案总能满足用户，用户永远不需要翻到第二页寻找答案。由此可见谷歌的搜索算法及 PageRank 网页排序算法之强大。
PageRank 算法的详细介绍见 维基百科。总而言之，该算法的主要思路是：如果一个网页被很多重要的网页指向，那么它也是一个重要的网页。具体地，互联网中的每个网页被抽象成一个节点；如果网页 A 包含网页 B 的链接，那么有一条有向边从节点 A 指向节点 B。如此，互联网中的网页及其链接被抽象成一个由节点及有向边组成的巨大拓扑图。
拓扑图建立好后，初始化系统，令每个节点的重要性分数均为1。然后开始迭代系统，在每一轮迭代中，对于每个节点，做如下两件事：
 如果出边是加权的，将该节点的分数按权重比例进行拆解并传送到对应的节点；如果出边不加权，那么将该节点的分数平均拆解并传送 搜集从其他节点传送过来的分数并求和，替换该节点原来的分数  该过程从数学上来说就是一个 马尔可夫过程，可以从数学上证明其收敛性。也就是说，该系统经过若干次迭代，必定可以演化到一个平衡态。在这个状态下，每个节点的每一次分数收入约等于其分数支出。此时，每个节点上的分数就是稳定的分数，PageRank 算法按照该分数从大到小对网页进行排序并（分页地）返回给用户。
二、PageRank 算法的 Spark 分布式实现
输入数据是一个文件，如下所示。第一行只有一个数字，表明了该数据集一共有 114529 个网页节点。从第二行开始，每一行表示一个节点的出边以及对应的权重，以 [key-value] 形式表示 ：[指向的节点id:权重]。在使用该数据集时，需要把第一行的数字删去，刚好剩下一共 114529 行，每一行的行号表示其节点id。例如，删掉第一行后，第 2 行为空行，说明节点 2 出度为 0，不指向任何其他节点；第 3 行的数据表示节点 3 指向了节点 8107、节点 22950 和 节点 108053，边的权重分别为 3320、4 和 1。
要分布式实现 PageRank，就需要按 MapReduce 编程范式来编写代码。MapReduce 接受的输入是 key-value 对，在 Map 过程中映射成新的 key-value 对，在 Reduce 过程中对相同 key 的 values 进行聚合，输入最终结果。为方便起见，使用函数式编程语言 Scala 编写。伪码如下：]]></description>
</item><item>
    <title>Spark</title>
    <link>https://utopizza.github.io/2019-05-09-%E8%AE%BA%E6%96%87-spark/</link>
    <pubDate>Thu, 09 May 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-05-09-%E8%AE%BA%E6%96%87-spark/</guid>
    <description><![CDATA[一、背景及问题
在谷歌提出了 MapReduce 编程模型之后，UC Berkeley 在此基础上提出了一个更加高效的模型，称为 Spark。其核心思想是将 MapReduce 的中间结果缓存到内存中，使得 Workers 可以快速读取数据而无需启动延迟极大的磁盘读取操作。这种编程模型针对一些迭代次数高、需要反复使用或者修改数据的计算任务尤其有效。根据论文的实验，Spark 在一些高迭代次数的算法实验中处理速度是 MapReduce 的 10 倍以上。
然而因为没有了将中间结果写磁盘的操作来保证容灾和恢复，因此 Spark 设计了一套别的方案来达到该目的，那就是 RDD + Lineage。RDD(Resilient Distribute Datasets)称为弹性数据集，它是一种对被操作数据的抽象，而 Lineage 称为“血统”，顾名思义它记录了每一个 RDD 演变过程中的上下文信息。当故障出现时，可以根据丢失的 RDD 的 Lineage 来追寻它的祖先 RDD ，然后重新执行演化即可恢复出丢失的 RDD。
二、Spark 编程模型
使用 Spark 的用户通过编写一个称为 Driver Program 的程序来实现自己的计算任务。Spark 为用户的并行化编程提供了三个组件：resilient distribution dataset、 parallel operations、 parallel operations。
(1) Resilient Distribution Datasets(RDDs)：一个 RDD 是一个可恢复的只读对象集合，它无须存储在磁盘中。每次对一个 RDD 的操作都会被记录下来，从而每个 RDD 可以沿着它的演化过程一直追寻它的祖先 RDD，甚至可以追寻到最开始的第一次从磁盘读取初始数据。因此这个机制保证了任何一个 RDD 都可以被恢复。具体在 Spark 系统中，RDD 由 Scala 对象来表达。论文中规定了 RDD 仅可以从四种方式构造：]]></description>
</item><item>
    <title>MapReduce</title>
    <link>https://utopizza.github.io/2019-05-08-%E8%AE%BA%E6%96%87-mapreduce/</link>
    <pubDate>Wed, 08 May 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-05-08-%E8%AE%BA%E6%96%87-mapreduce/</guid>
    <description><![CDATA[一、背景及问题
这两天总算把谷歌三大论文之一的 MapReduce 看了。这篇论文的影响力就不多说了，谷歌AI首席科学家 Jeffrey Dean 的代表作之一。该论文主要提出了一个称为 MapReduce 的编程模型，主要是为了解决谷歌公司自身需要处理海量数据的问题。众所周知，谷歌的主要业务是搜索引擎，每天需要处理爬取到的海量文件和网页，以及对这些数据进行索引计算、处理查询请求等等。显然单台机器难以完成这样巨大的计算任务，必须使用机器集群。但如果使用专业的服务器，集群的成本就太高了。为了降低成本，谷歌采用了普通的计算机来搭建这个集群。为了实现这个目的，必须设计一套可以自动并行化计算、自我管理调度、以及拥有良好容错性的集群计算方案，因此 Jeffrey Dean 提出了这套编程模型，只要按照这个编程模型来调用接口实现计算任务，这些计算就可以自动地在一个由普通机器搭建的分布式集群上被并行地执行。
二、解决方案
具体来说，MapReduce 这个编程模型可以简单地分为 Map 部分以及 Reduce 部分。
Map 函数：由用户实现该接口，其功能是使算法输入的初始 key-value 对转化成用户定义的新 key-value 对。接下来系统会自动把它们按新 key 分组，即一个新 key 对应一组 values（key-values），并传递给 Reduce 函数。这一步类似 SQL 中的 Select GroupBy 操作。
$$key_{1}/value_{1} \implies key_{2}/value_{2} \implies key_{2}/value_{2}(s)$$
Reduce 函数：由用户实现该接口，其功能是使从 Map 函数接收过来的 key-values 组按用户定义的计算方式进行 Merge，然后输出每个 key 组的最终结果。这一步类似定义 SQL 中对被 GroupBy 的字段的聚合函数。
$$key_{2}/value_{2}(s) \implies key_{2}/value_{3} $$
论文中给出了一个具体例子，从一个巨大的文档集中统计每个单词的出现次数。每个单词可以看作 key，它的出现次数就是最终想得到的 value。按照 MapReduce 编程范式，可以分为两步，在 Map 阶段输入文档集合，为每个文档拆解出每个单词，并为每个单词赋值它的 value 为 1，表示这个单词在此出现了一次；在 Reduce 阶段，把每个单词对应的分组进行 value 的求和，即可得到每个单词的总出现次数。]]></description>
</item><item>
    <title>阶段总结(7)</title>
    <link>https://utopizza.github.io/2019-04-30-%E6%80%BB%E7%BB%93-%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%937/</link>
    <pubDate>Tue, 30 Apr 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-04-30-%E6%80%BB%E7%BB%93-%E9%98%B6%E6%AE%B5%E6%80%BB%E7%BB%937/</guid>
    <description><![CDATA[上一次写阶段总结是2018年10月，转眼过了半年多。自从10月初选定了offer后，就转去忙论文、数学、以及英语。
一、论文
因为之前看过十来篇兴趣点推荐相关的顶会论文，考虑了下就还是继续做这个方向。从10月份开始，断断续续开始重新看了一遍之前看过的论文，以及做过的笔记和思路。因为之前专注于找工作，从4月份到9月份一直都是在看CS基础以及研发技术相关的书，学术工作放下了半年多，很多细节都忘记了。幸好之前认真做了笔记，复习起来不是太吃力。花了大约一星期的上午完成了复习，感觉还是没啥新灵感，继续用dblp去找了很多新的论文来看。印象中大概一直到快要放寒假的时候才想出了一个基本的雏形，用英文写了个小论文版本。寒假放了13天（2月1日到13日），在家基本都是休息，也没继续弄论文。回来后在公司陆续完成了论文的后半部分。思路基本是定下来了，给这边的老师看了下，确定思路没啥问题后，就差不多搬回学校了（3月3日）。
搬回学校之后就开始做实验，幸亏舍友给力，提供了他们实验室的服务器，不然我这小破笔记本完全不可能做的出实验。实验基本代码很快就写出来了，大概三天左右，接下来就是无尽的噩梦般的炼丹（调参）了。不知道是代码写错还是啥原因，我发现很有的参数即使是很小的改变，竟然对实验结果的影响很大（大雾）。到了三月中下旬，实验结果调得差不多之后，就感觉开始写大论文（毕业论文）。因为你之前已经写好了英文版的小论文，所以大论文写起来比较轻松，一共只用了6天左右。其中花的时间比较多的是第一章绪论，扯了很多废话，找了一些中文的参考文献。后面的相关理论、自己提出的模型、实验设计等等几章直接从英文小论文翻译下扩充下，雏形就基本出来了。四月份基本都在学校修修补补论文以及继续调参，还有就是考虑优化实验里的SGD。四月底，定了稿，发给了导师，以及回公司找导师签字一些报告。
总而言之，从2018年10月到2019年4月主要在弄论文，时间线大概为
 10月份主要复习以前看过的论文 11、12月看新论文 1月份开始构思自己的模型，以及写出一个基本的英语小论文雏形 2月份中旬寒假回来后继续完善英文小论文，2月底跟导师确认思路没有问题 3月初搬回学校，开始做实验。3月中下旬用一周写出了大论文 4月份继续修补大论文以及实验调参。4月底定稿。  二、数学
11月、12月、1月这三个月因为比较闲，回头复习了一波微积分、线性代数、概率统计。其中，微积分主要通过复习本科时候学的《高等数学》，线性代数主要通过网上的教学视频以及研究生学的《矩阵论》，概率统计看的是中科大的《概率论与数理统计》。一般是晚上下班回去看书和吃饭的时候看视频，算是一个小复习。下面贴出资料链接。这波数学复习主要以晚上回宿舍看书为主，最后整理了大概9篇 博客。
 《高等数学》 《概率论与数理统计》 《矩阵论》 MIT公开课：线性代数 MIT公开课：微积分重点 3b1b：线性代数的本质 3b1b：微积分的本质  三、英语
10月、11月、12月三个月基本上都是在背四六级、雅思词汇。因为定了个考雅思的目标。因为习惯了这个新东方系列的排版，索性就又买了这个系列的雅思词汇。背单词的同时也注意听力和写作。针对听力，主要就是直接听真题听力，把MP3下载到音乐播放软件，整天整夜没完没了地放。至于写作，买了本写作的书和一本笔记本，每天抄一篇范文，然后读几遍。从3月份搬回学校之后，因为每天去图书馆，就很少读英语范文了，改成用百词斩背单词。总的来说，这几个月基本把四六级背得比较熟了，雅思那本过了两三遍，词汇量在上面的网上测是9000个左右，百词斩测是一万个左右。搬回学校前主要以背单词书、抄写范文为主。搬回学校后以刷百词斩复习、在网上做雅思题为主。因为回学校后做论文实验花了很多时间，分配给英语的时间不是很多，练的比较少，目前效果不太理想。现在论文定了稿，后面重新把英语提上日程
 《新东方雅思词汇》 《顾家北手把手教你雅思写作》 雅思在线 词汇量测试  ]]></description>
</item><item>
    <title>MCI</title>
    <link>https://utopizza.github.io/2019-01-27-%E8%AE%BA%E6%96%87-mci/</link>
    <pubDate>Sun, 27 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-27-%E8%AE%BA%E6%96%87-mci/</guid>
    <description><![CDATA[该文章依然是针对 APT 攻击而提出的一种基于模型推断的攻击因果分析。该方法比以往方法的优势在于不需要在系统中做任何修改，只需要启动日志，对系统事件进行记录即可。该方法的核心技术是使用一种称为 LDX 的动态分析方案，可以在系统调用之间进行因果推断。]]></description>
</item><item>
    <title>Oblix</title>
    <link>https://utopizza.github.io/2019-01-19-%E8%AE%BA%E6%96%87-oblix/</link>
    <pubDate>Sat, 19 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-19-%E8%AE%BA%E6%96%87-oblix/</guid>
    <description><![CDATA[一、背景及问题
索引（Index）是很多系统和应用的基础构件。最近，大量的研究专注于如何保护索引这样的敏感数据，例如，如何在对索引进行加密的同时允许客户端在索引上进行查询。这些研究提出的方案一般是这样的：当用户通过客户端发起查询请求时，客户端会先为用户输入的关键词（keyword）生成查询令牌(search token)，然后将令牌发送给服务端而不是直接发送用户的关键词，从而向服务端屏蔽用户输入的关键词（假设攻击者控制了服务端的内存，但不能控制服务端的处理器）。然后，服务器通过令牌去在已经加密的索引上执行检索，最后把匹配成功的加密数据返回给客户端。客户端对数据进行解密，显示给用户。
虽然加密索引的研究取得了重大进步，但是很多方案都有一个很严重的漏洞，就是泄漏了存取模式（access patterns）：关键词与数据的匹配过程。虽然关键词和索引都是已经加密的，但是它们的匹配过程是在服务端的内存进行的，而服务端的内存是被攻击者掌控的，因此攻击者可以对匹配的过程进行分析，从而从加密的索引中恢复敏感信息。除了存取模式的遗漏，还有如果攻击者可以获得匹配成功的文件数（result size），也可以恢复出敏感信息。
一种很直接的隐藏存取模式的方法就是使用 ORAM（Oblivious RAM），然而这个方案的成本十分昂贵，因此很少方案采用这个方法。针对这个问题，文章提出了一个称为 Oblix (OBLivious IndeX) 的索引方案，这个方案既不会泄漏任何的存取模式，也不会泄漏匹配文件数。进一步，Oblix 允许索引进行插入和删除、支持多用户。
二、解决方案
文章提出的 Oblix 主要针对以下四种问题进行解决：
1、高复杂度：对于 ORAM 系统，客户端维护一个位置映射数据结构（position map），该数据结构记录了索引与数据库中某个数据的位置的对应关系。由于索引的大小与数据库的数据量成线性关系，因此客户端不能直接存储整个 map。一种标准的方案是使用树形结构将一个数据库分解成多级数据库（类似于多级索引），以减小客户端需要的存储空间。但是如此一来就会需要重复多次查询来确定最终的位置，显然树搜索的时间复杂度是 O(logN)，这样的复杂度会带来查询延迟。文章使用的解决方案是使用飞地技术（enclave），把整个 ORAM 的客户端放进飞地中，这样让客户端和服务端就在同一台机器上进行，不需要通过网络，从而减小网络延迟。
2、飞地可能会被攻击者利用：有研究发现飞地即使物理上隔离了外界的控制，但是也可能被攻击者通过分析它的物理页级的存取模式（page-level access pattern）来恢复被加密的数据。因为飞地自身的内存空间很小，当飞地处理的数据量较大时，它就会使用二级存储，使用计算机中的内存。而计算机中的内存是会被攻击者控制的，因此攻击者可以通过观察飞地如果使用计算机主存来进行攻击。文章的解决方案：首先，提出一个称为 ODS (oblivious data structure) 的数据结构来取代 position map，这个数据结构保证不可被攻击者利用。其次，提出一个称为 doubly oblivious 的方案，该方案同时保证对服务端的存取、对客户端自己的内存空间的存取模式都是不可被攻击者利用的（oblivious）。
3、查询结果数的隐藏：前面提到，查询结果数可能也会被攻击者用，因此需要对其进行隐藏。一种最简单的做法是“worst-case upper bound”，但是这种做法代价太昂贵。文章提出的解决方案：对查询结果进行打分排序，只返回得分最高的前 r 条数据。
4、请求列表、查询效率：客户端可以一次向服务端发送多个请求，也就请求列表。特别地，其中包含插入和删除得操作时，执行效率就尤其重要。文章解决方案：提出一个称为 DOSM (doubly-oblivious sorted multimap) 的数据结构，支持高效的范围查询、插入、删除等操作。它实际上是一个树形的数据结构，插入和删除的复杂度是 O(logN) 而不是 O(n)。另外再结合特定的查询算法来保证范围查询的高效性。
三、总结
文章主要针对存取模式的漏洞来进行修复。攻击虽然无法直接攻击飞地，无法直接攻击加密数据，但是可以通过应用对主存的存取模式进行分析从而恢复加密数据。这种存取模式在文章中是指例如索引这样的负责映射数据的数据结构。]]></description>
</item></channel>
</rss>
