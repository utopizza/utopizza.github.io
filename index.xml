<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Utopizza</title>
        <link>https://utopizza.github.io/</link>
        <description>About LoveIt Theme</description>
        <generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>648847079@qq.com (yusheng)</managingEditor>
            <webMaster>648847079@qq.com (yusheng)</webMaster><copyright>This work is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License.</copyright><lastBuildDate>Tue, 04 Aug 2020 00:00:00 &#43;0000</lastBuildDate><atom:link href="https://utopizza.github.io/" rel="self" type="application/rss+xml" /><item>
    <title>Kafka</title>
    <link>https://utopizza.github.io/2020-08-04-%E8%AE%BA%E6%96%87-kafka/</link>
    <pubDate>Tue, 04 Aug 2020 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2020-08-04-%E8%AE%BA%E6%96%87-kafka/</guid>
    <description><![CDATA[一、背景问题
在互联网公司中往往会产生大量的日志数据（log data），例如（1）用户事件如用户的登录、网页访问、点击、收藏、分享、评论、搜索；（2）内部系统产生的事件例如服务调用、错误日志、网络事件、系统调用等等。这些日志数据十分重要，可用于分析用户喜好、系统使用情况等等。以往这些日志数据一般用于离线的数据分析，然而随着互联网业务蓬勃的发展，越来越需要更加实时地利用这些日志数据来进行在线分析，例如（1）根据用户搜索历史进行实时更新的个性化推荐、相关推荐、广告投放（2）系统实时识别垃圾邮件、非法数据并自动进行过滤等等。因此，一些早期的系统，例如Facebook的Scribe，Yahoo的Data Highway，Cloudera的Flume等等，都是被设计成把日志数据进行收集并写入Hadoop这样的数据仓库然后进行离线的分析，不能满足进行实时大规模日志在线分析的需求。业界需要一个能支持延迟秒级的实时（real-time）日志收集系统。
二、Kafka
1、基本概念&amp;架构
 一个message流被称为一个topic，每个topic下有多个partition分区（分区数量十分重要，因为它决定了可以并行消费的并行度，见下文） 存储message的kafka节点称为broker。Kafka是一个分布式系统，有多个broker节点，每个broker节点存储一个或者多个partition分区 每个producer可以推送message到一个topic 每个consumer可以订阅一个或者多个topic，从对应的broker中拉取message  2、Efficiency
（1）简单的存储结构
每个topic的每个partition分区对应一个逻辑日志，该日志物理上其实是由一系列的文件片（segment file）构成，每个文件片大约1GB。每当producer推送一个message到某个partition时，broker只是简单地把该message追加到最后一个片文件的末尾。考虑到写磁盘的性能问题，kafka只有在指定数量的message到达、或者指定时间周期结束才执行一次flush把文件片写入磁盘。
至于message的数据结构，与传统message系统不同，kafka中的message并没有所谓的“messageId”，每个message只根据它在日志中的offset来定位：下一条message在日志中的位置等于当前message的位置+message的长度。
consumer从一个partition消费message时，它只能从某个offset开始连续地消费，并且该offset之前的message已经完成消费。consumer在向broker提交每个的pull请求中都会带上所请求的message的offset以及请求的字节数。broker根据offset找到对应的文件片（会在内存中维护一个“文件片首部offset”与“文件片”的映射关系），定位message，然后发送被请求的数据返回给consumer。consumer收到数据后，计算出下一个message的offset然后再继续发送请求。
（2）高效的传输方式
producer可以在单个send请求中一次性提交多个message；同样，consumer也可以在单个pull请求中一次性拉取多个message。
另外，kafka为了防止数据在机器上被缓冲两次（double buffering），它不在内存中缓存任何数据，所有数据只利用了操作系统的页缓冲（page cache，应该就是操作系统底层为写磁盘提供的内核缓冲区）。这样做的好处，文中解析是基本不用考虑内存的垃圾回收，并且在kafka进程重启的时候可以很方便的从页缓冲中重新加载数据（如果是维护在内存中，进程吃消失后分配的内存会被回收释放）
更进一步，kafka对网络传输也做了优化。传统的程序把文件数据发送到网络中一般需要以下几个步骤：
 从磁盘或者其他存储媒体把文件数据读到操作系统内核的缓冲区（page cache） 从内核缓冲区把数据拷贝到应用程序中的buffer 把数据从应用程序中的buffer又重新拷贝到内核中的另一个缓冲区（可能是为网卡分配的缓冲） 把内核缓冲区中的数据发送到socket套接字层，进行网络传输  可见上述过程经历了4次拷贝：文件-》内核缓冲-》应用程序缓冲-》内核缓冲-》socket。然而在Linux或者Unix类的操作系统中，其实有一个名为“sendfile”的API可以直接把文件读到一个内核缓冲后直接发送到socket，kafka利用了该API进行高效的文件数据发送：文件-》内核缓冲-》socket。
（3）无状态broker
在kafka中，每个consumer消费到哪个offset位置并不会被kafka记录，而是由consumer自己进行记录。这样设计的好处是极大了简化了kafka系统的复杂度。但是这样会引入另一个问题：kafka无法知道一个message是否已经被所有的consumer消费过，是否可以进行删除。kafka使用了一个简单解决方案——每个message保留7天，超过该时间的message会被删除。这样设计的好处一是简洁，二是有一定的容灾能力，例如consumer崩溃或者出现一些故障后，可以减小offset从头消费，重新获取消费错误或丢失的数据。
3、分布式协作
在kafka中，可以定义一个consumer组（consumer group），每个这样的组消费一个指定的topic，该topic下的每个partition只能被组中的一个consumer消费，该consumer可以是一个线程或者进程。这样设计的好处是足够简单，因为如果有多个consumer同时消费一个partition，势必需要进行同步来保证message不会被一个组重复消费，可能需要引入锁，这样会使得消费效率大打折扣。因为规定了组内一个consumer只能消费一个partition，因此组内的consumer同步只会出现在进行rebalance的时候，而这是一个出现频率很低的场景。
对于kafka节点结构，它并没有设计成master-slave结构，而是每个broker均为同等地位的数据节点，但是会把一些重要的集群信息存放到Zookeeper中。Zookeeper在kafka中负责的任务主要有：
 监测broker、consumer的加入和离开 触发一次rebalance，当上一点发生的时候 记录consumer与partition对应关系，追踪每个partition的offset  4、交付保证
kafka只保证“至少一次”的数据交付（at-least-once）。如果外部应用程序不能引入重复数据，则它需要自行实现去重逻辑（deduplicated logic）。kafka只保证单个partition内的message会被按顺序交付给consumer，并不会保证不同partition之间的message的交付顺序。]]></description>
</item><item>
    <title>Raft</title>
    <link>https://utopizza.github.io/2020-07-26-%E8%AE%BA%E6%96%87-raft/</link>
    <pubDate>Sun, 26 Jul 2020 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2020-07-26-%E8%AE%BA%E6%96%87-raft/</guid>
    <description><![CDATA[一、背景及问题
共识算法（consensus algorithm）在实现大型分布式系统的可靠性方面扮演了十分关键的角色，因为它允许集群在部分机器故障的情况下保持任务正常运行。在过去十年中，主流的共识算法叫Paxos，大部分系统都使用了这个算法，并且该算法一度成为标准的教材。然而，该算法有一个缺陷就是过于复杂，想要完全理解或者完美实现都十分困难。因此，斯坦福的Diego Ongaro和John Ousterhout便提出了一个相对容易理解、容易实现的共识算法，命名为Raft。
二、相关概念
Replicated state machine：复制状态机，是共识算法的起源之处。像GFS、HDFS等的大规模分布式系统，其leader节点都只有一个，存在单点故障问题。要保证集群即使出现该故障的情况下自动恢复正常运行，如集群配置信息、leader选举心跳信息等数据必须克服单点故障的问题，这就要求这些重要的数据有可靠的方式来进行备份。一种可靠的方式就是使用复制状态机，如下图所示，复制状态机一般是通过复制日志（replicated log）来实现。每个机器上都会维护一份复制日志，日志的内容是一系列的command，机器会按日志的顺序来执行这些command，来修改自己的状态。只要保证每个机器上的复制日志是完全一致，那么就可以保证每个机器的操作结果是一致的。其中，共识算法的主要作用就是通过一些通信手段，来保持集群中每个机器的复制日志完全一致。目前比较有名的复制状态机的实现是Zookeeper和Chubby。
三、Raft算法
总的来说，Raft首先会从集群中选举一个leader，整个集群的复制日志完全由leader来维护：leader节点接收client端发送过来的日志实体（log entry），然后把日志实体分发给集群中其他节点，并告诉其他节点什么时候可以安全地执行这些日志，从而修改他们的状态。当leader宕机故障时，集群会自动发起选举，选出一个新的leader继续保持集群运行。为了便于理解和实践，Raft把共识问题拆分成三个彼此独立的子问题，从而逐一解决：（1）leader选举（2）日志复制（3）安全
1、Raft基础
Raft集群（一般规模为5台机器）中的节点在任何时刻（可以运行的情况下）必为以下三种状态之一：（1）leader（2）follower（3）candidate。在正常情况下，集群中只有一个leader，其余均为follower。follower不响应任何client端请求，它只会把这些请求重定向给leader，并且只响应leader的请求。
Raft把时间分割成任意长度的周期（term），这些周期用连续的整数来唯一地标识，可以理解为周期id。每个周期都由leader选举开始，在这个周期内如果一个节点当选为leader，它会一直作为leader直到这个周期结束。集群中每个节点自身都会维护一个周期id作为自己看到的当前周期。当节点之间进行通信时会带上自己的当前周期，如果节点发现对方周期id比自己大，则更新自己的当前周期；如果节点发现自己的当前周期已经过期，则马上把自己转为follower状态；如果节点发现对方的周期已经过期，则直接拒绝对方的请求。
2、leader选举
Raft使用心跳机制（heartbeat）来触发leader选举。当Raft集群刚启动时，所有节点都是follower状态。follower节点一段时间内发现没有收到来自leader的心跳，认为leader节点已经失效（election timeout），则马上把自己转变为candidate状态，并将自己的当前周期增大。它会先为自己投票，然后向集群中其他节点发起拉票请求（RequestVote RPC）。candidate节点会一直持续这个状态直到（1）它获得多数票从而赢得选举成为新leader（2）其他节点胜出成为新leader（3）选举超时，没有选出新leader。
Raft投票规则：（1）每个节点每次选举周期内最多投出一票（2）先到先得原则，给先请求的节点投票（2）candidate只给自己投一票，然后并发地向其他节点拉票（4）特殊情况下，可能会进入僵持状态——例如所有节点同时进入candidate状态，只给自己投票，然后等等其他人给自己投票，此时的集群状态称为“split”。该论文解决这个问题使用了一种简单的方法，就是随机话选举超时时间，避免多个节点同时开启选举周期，使得先开启选举的节点有先发优势成为新leader，减少集群不可用时间。
当一个candidate成为新leader后，马上开始向其他节点发送心跳宣告自己的leader身份。其他节点确认该新leader的当前周期是有效周期后，认可新leader的身份，把自己转变为follower；否则无视该leader，继续保持选举。
3、日志复制
当节点成为leader后，开始负责响应client的请求。client的请求会包含一个需要整个集群都执行的command。leader首先会把该command追加到自己的日志中，然后通过RPC（AppendEntries RPCs）把日志分发到其他节点。若某个follower宕机或者网络丢包没有复制成功，leader会无限地重新尝试，直到复制成功。分发完成后，leader执行该command，修改自己的状态，然后把执行结果返回给client。具体关于日志的组织形式、压缩方法，详见原论文。
4、安全性
这部分较为繁琐，详见原论文。]]></description>
</item><item>
    <title>PageRank在Spark的分布式实现</title>
    <link>https://utopizza.github.io/2019-05-19-%E7%AE%97%E6%B3%95-pagerank%E5%9C%A8spark%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0/</link>
    <pubDate>Sun, 19 May 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-05-19-%E7%AE%97%E6%B3%95-pagerank%E5%9C%A8spark%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0/</guid>
    <description><![CDATA[最近在研究 MapReduce 和 Spark 的相关资料，顺便补个关于如何在虚拟机中搭建的 Spark 上分布式运行 PageRank 算法的博客。犹记得这个小任务是研一开学时，导师布置的第一个小任务，而现在马上就要硕士毕业答辩了，不禁思绪万千。
一、PageRank 算法
PageRank 算法是谷歌的起家算法，凭借该算法谷歌击败了当时所有的其他门户网站以及搜索引擎。该算法的目的是对数以亿计的网页进行排序，重要的网页将被排在前列，作为搜索结果返回给用户。想起了昆丁的电影《低俗小说》中的对白：“如果你要把一具尸体藏起来，你知道世界上哪里最安全吗？那就是谷歌搜索结果的第二页”。言外之意，谷歌搜索的前几条解决方案总能满足用户，用户永远不需要翻到第二页寻找答案。由此可见谷歌的搜索算法及 PageRank 网页排序算法之强大。
PageRank 算法的详细介绍见 维基百科。总而言之，该算法的主要思路是：如果一个网页被很多重要的网页指向，那么它也是一个重要的网页。具体地，互联网中的每个网页被抽象成一个节点；如果网页 A 包含网页 B 的链接，那么有一条有向边从节点 A 指向节点 B。如此，互联网中的网页及其链接被抽象成一个由节点及有向边组成的巨大拓扑图。
拓扑图建立好后，初始化系统，令每个节点的重要性分数均为1。然后开始迭代系统，在每一轮迭代中，对于每个节点，做如下两件事：
 如果出边是加权的，将该节点的分数按权重比例进行拆解并传送到对应的节点；如果出边不加权，那么将该节点的分数平均拆解并传送 搜集从其他节点传送过来的分数并求和，替换该节点原来的分数  该过程从数学上来说就是一个 马尔可夫过程，可以从数学上证明其收敛性。也就是说，该系统经过若干次迭代，必定可以演化到一个平衡态。在这个状态下，每个节点的每一次分数收入约等于其分数支出。此时，每个节点上的分数就是稳定的分数，PageRank 算法按照该分数从大到小对网页进行排序并（分页地）返回给用户。
二、PageRank 算法的 Spark 分布式实现
输入数据是一个文件，如下所示。第一行只有一个数字，表明了该数据集一共有 114529 个网页节点。从第二行开始，每一行表示一个节点的出边以及对应的权重，以 [key-value] 形式表示 ：[指向的节点id:权重]。在使用该数据集时，需要把第一行的数字删去，刚好剩下一共 114529 行，每一行的行号表示其节点id。例如，删掉第一行后，第 2 行为空行，说明节点 2 出度为 0，不指向任何其他节点；第 3 行的数据表示节点 3 指向了节点 8107、节点 22950 和 节点 108053，边的权重分别为 3320、4 和 1。
要分布式实现 PageRank，就需要按 MapReduce 编程范式来编写代码。MapReduce 接受的输入是 key-value 对，在 Map 过程中映射成新的 key-value 对，在 Reduce 过程中对相同 key 的 values 进行聚合，输入最终结果。为方便起见，使用函数式编程语言 Scala 编写。伪码如下：]]></description>
</item><item>
    <title>Spark</title>
    <link>https://utopizza.github.io/2019-05-09-%E8%AE%BA%E6%96%87-spark/</link>
    <pubDate>Thu, 09 May 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-05-09-%E8%AE%BA%E6%96%87-spark/</guid>
    <description><![CDATA[一、背景及问题
在谷歌提出了 MapReduce 编程模型之后，UC Berkeley 在此基础上提出了一个更加高效的模型，称为 Spark。其核心思想是将 MapReduce 的中间结果缓存到内存中，使得 Workers 可以快速读取数据而无需启动延迟极大的磁盘读取操作。这种编程模型针对一些迭代次数高、需要反复使用或者修改数据的计算任务尤其有效。根据论文的实验，Spark 在一些高迭代次数的算法实验中处理速度是 MapReduce 的 10 倍以上。
然而因为没有了将中间结果写磁盘的操作来保证容灾和恢复，因此 Spark 设计了一套别的方案来达到该目的，那就是 RDD + Lineage。RDD(Resilient Distribute Datasets)称为弹性数据集，它是一种对被操作数据的抽象，而 Lineage 称为“血统”，顾名思义它记录了每一个 RDD 演变过程中的上下文信息。当故障出现时，可以根据丢失的 RDD 的 Lineage 来追寻它的祖先 RDD ，然后重新执行演化即可恢复出丢失的 RDD。
二、Spark 编程模型
使用 Spark 的用户通过编写一个称为 Driver Program 的程序来实现自己的计算任务。Spark 为用户的并行化编程提供了三个组件：resilient distribution dataset、 parallel operations、 parallel operations。
(1) Resilient Distribution Datasets(RDDs)：一个 RDD 是一个可恢复的只读对象集合，它无须存储在磁盘中。每次对一个 RDD 的操作都会被记录下来，从而每个 RDD 可以沿着它的演化过程一直追寻它的祖先 RDD，甚至可以追寻到最开始的第一次从磁盘读取初始数据。因此这个机制保证了任何一个 RDD 都可以被恢复。具体在 Spark 系统中，RDD 由 Scala 对象来表达。论文中规定了 RDD 仅可以从四种方式构造：]]></description>
</item><item>
    <title>MapReduce</title>
    <link>https://utopizza.github.io/2019-05-08-%E8%AE%BA%E6%96%87-mapreduce/</link>
    <pubDate>Wed, 08 May 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-05-08-%E8%AE%BA%E6%96%87-mapreduce/</guid>
    <description><![CDATA[一、背景及问题
这两天总算把谷歌三大论文之一的 MapReduce 看了。这篇论文的影响力就不多说了，谷歌AI首席科学家 Jeffrey Dean 的代表作之一。该论文主要提出了一个称为 MapReduce 的编程模型，主要是为了解决谷歌公司自身需要处理海量数据的问题。众所周知，谷歌的主要业务是搜索引擎，每天需要处理爬取到的海量文件和网页，以及对这些数据进行索引计算、处理查询请求等等。显然单台机器难以完成这样巨大的计算任务，必须使用机器集群。但如果使用专业的服务器，集群的成本就太高了。为了降低成本，谷歌采用了普通的计算机来搭建这个集群。为了实现这个目的，必须设计一套可以自动并行化计算、自我管理调度、以及拥有良好容错性的集群计算方案，因此 Jeffrey Dean 提出了这套编程模型，只要按照这个编程模型来调用接口实现计算任务，这些计算就可以自动地在一个由普通机器搭建的分布式集群上被并行地执行。
二、解决方案
具体来说，MapReduce 这个编程模型可以简单地分为 Map 部分以及 Reduce 部分。
Map 函数：由用户实现该接口，其功能是使算法输入的初始 key-value 对转化成用户定义的新 key-value 对。接下来系统会自动把它们按新 key 分组，即一个新 key 对应一组 values（key-values），并传递给 Reduce 函数。这一步类似 SQL 中的 Select GroupBy 操作。
$$key_{1}/value_{1} \implies key_{2}/value_{2} \implies key_{2}/value_{2}(s)$$
Reduce 函数：由用户实现该接口，其功能是使从 Map 函数接收过来的 key-values 组按用户定义的计算方式进行 Merge，然后输出每个 key 组的最终结果。这一步类似定义 SQL 中对被 GroupBy 的字段的聚合函数。
$$key_{2}/value_{2}(s) \implies key_{2}/value_{3} $$
论文中给出了一个具体例子，从一个巨大的文档集中统计每个单词的出现次数。每个单词可以看作 key，它的出现次数就是最终想得到的 value。按照 MapReduce 编程范式，可以分为两步，在 Map 阶段输入文档集合，为每个文档拆解出每个单词，并为每个单词赋值它的 value 为 1，表示这个单词在此出现了一次；在 Reduce 阶段，把每个单词对应的分组进行 value 的求和，即可得到每个单词的总出现次数。]]></description>
</item><item>
    <title>MCI</title>
    <link>https://utopizza.github.io/2019-01-27-%E8%AE%BA%E6%96%87-mci/</link>
    <pubDate>Sun, 27 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-27-%E8%AE%BA%E6%96%87-mci/</guid>
    <description><![CDATA[该文章依然是针对 APT 攻击而提出的一种基于模型推断的攻击因果分析。该方法比以往方法的优势在于不需要在系统中做任何修改，只需要启动日志，对系统事件进行记录即可。该方法的核心技术是使用一种称为 LDX 的动态分析方案，可以在系统调用之间进行因果推断。]]></description>
</item><item>
    <title>Oblix</title>
    <link>https://utopizza.github.io/2019-01-19-%E8%AE%BA%E6%96%87-oblix/</link>
    <pubDate>Sat, 19 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-19-%E8%AE%BA%E6%96%87-oblix/</guid>
    <description><![CDATA[一、背景及问题
索引（Index）是很多系统和应用的基础构件。最近，大量的研究专注于如何保护索引这样的敏感数据，例如，如何在对索引进行加密的同时允许客户端在索引上进行查询。这些研究提出的方案一般是这样的：当用户通过客户端发起查询请求时，客户端会先为用户输入的关键词（keyword）生成查询令牌(search token)，然后将令牌发送给服务端而不是直接发送用户的关键词，从而向服务端屏蔽用户输入的关键词（假设攻击者控制了服务端的内存，但不能控制服务端的处理器）。然后，服务器通过令牌去在已经加密的索引上执行检索，最后把匹配成功的加密数据返回给客户端。客户端对数据进行解密，显示给用户。
虽然加密索引的研究取得了重大进步，但是很多方案都有一个很严重的漏洞，就是泄漏了存取模式（access patterns）：关键词与数据的匹配过程。虽然关键词和索引都是已经加密的，但是它们的匹配过程是在服务端的内存进行的，而服务端的内存是被攻击者掌控的，因此攻击者可以对匹配的过程进行分析，从而从加密的索引中恢复敏感信息。除了存取模式的遗漏，还有如果攻击者可以获得匹配成功的文件数（result size），也可以恢复出敏感信息。
一种很直接的隐藏存取模式的方法就是使用 ORAM（Oblivious RAM），然而这个方案的成本十分昂贵，因此很少方案采用这个方法。针对这个问题，文章提出了一个称为 Oblix (OBLivious IndeX) 的索引方案，这个方案既不会泄漏任何的存取模式，也不会泄漏匹配文件数。进一步，Oblix 允许索引进行插入和删除、支持多用户。
二、解决方案
文章提出的 Oblix 主要针对以下四种问题进行解决：
1、高复杂度：对于 ORAM 系统，客户端维护一个位置映射数据结构（position map），该数据结构记录了索引与数据库中某个数据的位置的对应关系。由于索引的大小与数据库的数据量成线性关系，因此客户端不能直接存储整个 map。一种标准的方案是使用树形结构将一个数据库分解成多级数据库（类似于多级索引），以减小客户端需要的存储空间。但是如此一来就会需要重复多次查询来确定最终的位置，显然树搜索的时间复杂度是 O(logN)，这样的复杂度会带来查询延迟。文章使用的解决方案是使用飞地技术（enclave），把整个 ORAM 的客户端放进飞地中，这样让客户端和服务端就在同一台机器上进行，不需要通过网络，从而减小网络延迟。
2、飞地可能会被攻击者利用：有研究发现飞地即使物理上隔离了外界的控制，但是也可能被攻击者通过分析它的物理页级的存取模式（page-level access pattern）来恢复被加密的数据。因为飞地自身的内存空间很小，当飞地处理的数据量较大时，它就会使用二级存储，使用计算机中的内存。而计算机中的内存是会被攻击者控制的，因此攻击者可以通过观察飞地如果使用计算机主存来进行攻击。文章的解决方案：首先，提出一个称为 ODS (oblivious data structure) 的数据结构来取代 position map，这个数据结构保证不可被攻击者利用。其次，提出一个称为 doubly oblivious 的方案，该方案同时保证对服务端的存取、对客户端自己的内存空间的存取模式都是不可被攻击者利用的（oblivious）。
3、查询结果数的隐藏：前面提到，查询结果数可能也会被攻击者用，因此需要对其进行隐藏。一种最简单的做法是“worst-case upper bound”，但是这种做法代价太昂贵。文章提出的解决方案：对查询结果进行打分排序，只返回得分最高的前 r 条数据。
4、请求列表、查询效率：客户端可以一次向服务端发送多个请求，也就请求列表。特别地，其中包含插入和删除得操作时，执行效率就尤其重要。文章解决方案：提出一个称为 DOSM (doubly-oblivious sorted multimap) 的数据结构，支持高效的范围查询、插入、删除等操作。它实际上是一个树形的数据结构，插入和删除的复杂度是 O(logN) 而不是 O(n)。另外再结合特定的查询算法来保证范围查询的高效性。
三、总结
文章主要针对存取模式的漏洞来进行修复。攻击虽然无法直接攻击飞地，无法直接攻击加密数据，但是可以通过应用对主存的存取模式进行分析从而恢复加密数据。这种存取模式在文章中是指例如索引这样的负责映射数据的数据结构。]]></description>
</item><item>
    <title>一维随机变量</title>
    <link>https://utopizza.github.io/2019-01-16-%E6%A6%82%E7%8E%87-%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/</link>
    <pubDate>Wed, 16 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-16-%E6%A6%82%E7%8E%87-%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/</guid>
    <description><![CDATA[一、随机变量 随机变量就是“其值随机会而定”的变量。随机变量主要分为两类，一类是离散型随机变量，它有若干个不同的可枚举的取值，这些取值以不同（或相同）的概率出现，例如扔一个骰子可能会出现的点数。另一类是连续型随机变量，它的全部可能取值不仅是无穷多的，并且还不能无遗漏地逐一排列，而是充满一个区间，例如称量一个物体重量的误差。
二、离散型随机变量 1、离散型随机变量的概率分布：设 $X$ 为离散型随机变量，其全部可能取值 ${a_1,a_2,\cdots}$。则
$$p_i=P(X=a_i) \quad (i=1,2,\cdots)$$
称为 $X$ 的概率分布。根据概率的定义，显然有
$$p_i \geq 0$$
$$p_1+p_2+\cdots=1$$
用分布表的形式给出为
 $$ \begin{array}{c|lcr} \text{可能值} & a_1 & a_2 & \cdots & a_i \cdots \\ \hline \text{概率} & p_1 & p_2 & \cdots &p_i \cdots \end{array} $$  2、离散型随机变量的分布函数：设 $X$ 为一离散型随机变量，则函数
$$F(x)=P(X \leq x)=\sum_{{i|a_i \leq x}}p_i \quad (-\infty &lt; x &lt; +\infty)$$
称为 $X$ 的分布函数。实质上就是随机变量 $X$ 的部分可能取值的概率之和。分布函数有下面的一般性质：
 当 $x_1 &lt; x_2$ 时，有 $F(x_1) \leq F(x_2)$ 当 $x \to +\infty$ 时，$F(x) \to 1$；当 $x \to -\infty$ 时，$F(x) \to 0$  3、常见的离散型随机变量]]></description>
</item><item>
    <title>Winnower</title>
    <link>https://utopizza.github.io/2019-01-12-%E8%AE%BA%E6%96%87-winnower/</link>
    <pubDate>Sat, 12 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-12-%E8%AE%BA%E6%96%87-winnower/</guid>
    <description><![CDATA[一、背景及问题
这篇文章针对的问题和前面看过的 《Dependence-Preserving Data Compaction for Scalable Forensic Analysis》和 这篇文章类似，目标都是减小日志文件的体积，以减少存储空间并提高攻击侦查的速度。
这篇文章主要针对的是分布式服务器集群下的 APT 攻击。在这样的情景中，服务器集群有一个中心监控节点（Monitor node），以及大量的工作节点（Worker nodes）。监控节点与所有的工作节点保持通信。工作节点在生产环境中不断地产生日志记录。当发起监控节点攻击侦查分析时，它需要从多个工作节点上拷贝日志文件到本地，再进行攻击分析。问题就出现在这里，大量的工作节点每天都产生大量的日志记录，监控节点几乎不可能从所有工作节点上拷贝全部的日志记录，因为这样的拷贝任务不但需要消耗大量的网络资源，并且需要消耗大量的存储空间。
二、解决方案
这篇文章和上述的那篇文章有点类似，其核心思想都是通过对资源图（Provenance Graph）进行剪枝、压缩、优化，从而减小对应的日志记录体积。不同的只是他们使用的剪枝算法不同而已，这篇文章使用的算法是一个称为 Deterministic Finite Automata (DFA) 的算法，这个算法可以对一个复杂的图中的每一个节点、边（对应系统日志记录中的事件以及信息流向）进行推断，判断出它是不是冗余的，如果是冗余的就可以去掉，从而极大提精简了日志记录。
文章提出的模型称为 Winnower，具体剪枝算法较复杂，主要分为图抽象（Provenance Graph Abstraction）和图推断（Provenance Graph Induction）两部分，其中涉及一些图语义学习（Graph Grammar Learning）的知识，以前没怎么接触过，感觉不太懂，以后有时间再补一下这方面的学习。]]></description>
</item><item>
    <title>线性变换的矩阵</title>
    <link>https://utopizza.github.io/2019-01-12-%E6%95%B0%E5%AD%A6-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E7%9A%84%E7%9F%A9%E9%98%B5/</link>
    <pubDate>Sat, 12 Jan 2019 00:00:00 &#43;0000</pubDate>
    <author>Author</author>
    <guid>https://utopizza.github.io/2019-01-12-%E6%95%B0%E5%AD%A6-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E7%9A%84%E7%9F%A9%E9%98%B5/</guid>
    <description><![CDATA[我们知道，线性空间中，在给定基的情况下，对任意一个向量 $\vec{v}$ 施予一个线性变换 $T$ 时，就让该向量的坐标向量左乘某个矩阵 $A$ 即可。例如，设在线性空间 $V_n(F)$ 中，取一组基为 ${\vec{\alpha_1},\vec{\alpha_2},\cdots,\vec{\alpha_n}}$，向量 $\vec{w}$ 在该基下的坐标为 $\vec{Y}$，向量 $\vec{v}$ 在该基下的坐标为 $\vec{X}$，即：
$$\vec{w}=(\vec{\alpha_1},\vec{\alpha_2},\cdots,\vec{\alpha_n}) \vec{Y}$$
$$\vec{v}=(\vec{\alpha_1},\vec{\alpha_2},\cdots,\vec{\alpha_n}) \vec{X}$$
如果向量 $\vec{w}$ 是由向量 $\vec{v}$ 经过线性变换 $T$ 得到的，即
$$\vec{w}=T(\vec{v})$$
那么它们的坐标关系为：
$$\vec{Y}=A \vec{X}$$
并且，线性空间中任一个线性变换 $T$ 可以由某个矩阵 $A$ 唯一对应：
$$T \iff A$$
为什么？这个矩阵 $A$ 如何得到？接下来复习一下 $A$ 的推导。需要的预备知识：
 线性空间定义 线性空间的基与坐标 线性变换定义   记 $T$ 为线性空间 $V_n(F)$ 上的线性变换。记 ${\vec{\alpha_1},\vec{\alpha_2},\cdots,\vec{\alpha_n}}$ 为 $V_n(F)$ 的一组基，则对任意向量 $\forall \pmb\beta \in V_n(F)$，由基的定义可得
 $$\vec{v}=(\vec{\alpha_1},\vec{\alpha_2},\cdots,\vec{\alpha_n}) \vec{X}=(\vec{\alpha_1},\vec{\alpha_2},\cdots,\vec{\alpha_n}) \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_n \end{pmatrix} =x_1\vec{\alpha_1}+x_2\vec{\alpha_2}+\cdots+x_n\vec{\alpha_n}$$  对 $\vec{v}$ 施予线性变换 $T$：]]></description>
</item></channel>
</rss>
