[{"categories":["Thesis"],"content":"一、背景问题 在互联网公司中往往会产生大量的日志数据（log data），例如（1）用户事件如用户的登录、网页访问、点击、收藏、分享、评论、搜索；（2）内部系统产生的事件例如服务调用、错误日志、网络事件、系统调用等等。这些日志数据十分重要，可用于分析用户喜好、系统使用情况等等。以往这些日志数据一般用于离线的数据分析，然而随着互联网业务蓬勃的发展，越来越需要更加实时地利用这些日志数据来进行在线分析，例如（1）根据用户搜索历史进行实时更新的个性化推荐、相关推荐、广告投放（2）系统实时识别垃圾邮件、非法数据并自动进行过滤等等。因此，一些早期的系统，例如Facebook的Scribe，Yahoo的Data Highway，Cloudera的Flume等等，都是被设计成把日志数据进行收集并写入Hadoop这样的数据仓库然后进行离线的分析，不能满足进行实时大规模日志在线分析的需求。业界需要一个能支持延迟秒级的实时（real-time）日志收集系统。 二、Kafka 1、基本概念\u0026架构 一个message流被称为一个topic，每个topic下有多个partition分区（分区数量十分重要，因为它决定了可以并行消费的并行度，见下文） 存储message的kafka节点称为broker。Kafka是一个分布式系统，有多个broker节点，每个broker节点存储一个或者多个partition分区 每个producer可以推送message到一个topic 每个consumer可以订阅一个或者多个topic，从对应的broker中拉取message 2、Efficiency （1）简单的存储结构 每个topic的每个partition分区对应一个逻辑日志，该日志物理上其实是由一系列的文件片（segment file）构成，每个文件片大约1GB。每当producer推送一个message到某个partition时，broker只是简单地把该message追加到最后一个片文件的末尾。考虑到写磁盘的性能问题，kafka只有在指定数量的message到达、或者指定时间周期结束才执行一次flush把文件片写入磁盘。 至于message的数据结构，与传统message系统不同，kafka中的message并没有所谓的“messageId”，每个message只根据它在日志中的offset来定位：下一条message在日志中的位置等于当前message的位置+message的长度。 consumer从一个partition消费message时，它只能从某个offset开始连续地消费，并且该offset之前的message已经完成消费。consumer在向broker提交每个的pull请求中都会带上所请求的message的offset以及请求的字节数。broker根据offset找到对应的文件片（会在内存中维护一个“文件片首部offset”与“文件片”的映射关系），定位message，然后发送被请求的数据返回给consumer。consumer收到数据后，计算出下一个message的offset然后再继续发送请求。 （2）高效的传输方式 producer可以在单个send请求中一次性提交多个message；同样，consumer也可以在单个pull请求中一次性拉取多个message。 另外，kafka为了防止数据在机器上被缓冲两次（double buffering），它不在内存中缓存任何数据，所有数据只利用了操作系统的页缓冲（page cache，应该就是操作系统底层为写磁盘提供的内核缓冲区）。这样做的好处，文中解析是基本不用考虑内存的垃圾回收，并且在kafka进程重启的时候可以很方便的从页缓冲中重新加载数据（如果是维护在内存中，进程吃消失后分配的内存会被回收释放） 更进一步，kafka对网络传输也做了优化。传统的程序把文件数据发送到网络中一般需要以下几个步骤： 从磁盘或者其他存储媒体把文件数据读到操作系统内核的缓冲区（page cache） 从内核缓冲区把数据拷贝到应用程序中的buffer 把数据从应用程序中的buffer又重新拷贝到内核中的另一个缓冲区（可能是为网卡分配的缓冲） 把内核缓冲区中的数据发送到socket套接字层，进行网络传输 可见上述过程经历了4次拷贝：文件-》内核缓冲-》应用程序缓冲-》内核缓冲-》socket。然而在Linux或者Unix类的操作系统中，其实有一个名为“sendfile”的API可以直接把文件读到一个内核缓冲后直接发送到socket，kafka利用了该API进行高效的文件数据发送：文件-》内核缓冲-》socket。 （3）无状态broker 在kafka中，每个consumer消费到哪个offset位置并不会被kafka记录，而是由consumer自己进行记录。这样设计的好处是极大了简化了kafka系统的复杂度。但是这样会引入另一个问题：kafka无法知道一个message是否已经被所有的consumer消费过，是否可以进行删除。kafka使用了一个简单解决方案——每个message保留7天，超过该时间的message会被删除。这样设计的好处一是简洁，二是有一定的容灾能力，例如consumer崩溃或者出现一些故障后，可以减小offset从头消费，重新获取消费错误或丢失的数据。 3、分布式协作 在kafka中，可以定义一个consumer组（consumer group），每个这样的组消费一个指定的topic，该topic下的每个partition只能被组中的一个consumer消费，该consumer可以是一个线程或者进程。这样设计的好处是足够简单，因为如果有多个consumer同时消费一个partition，势必需要进行同步来保证message不会被一个组重复消费，可能需要引入锁，这样会使得消费效率大打折扣。因为规定了组内一个consumer只能消费一个partition，因此组内的consumer同步只会出现在进行rebalance的时候，而这是一个出现频率很低的场景。 对于kafka节点结构，它并没有设计成master-slave结构，而是每个broker均为同等地位的数据节点，但是会把一些重要的集群信息存放到Zookeeper中。Zookeeper在kafka中负责的任务主要有： 监测broker、consumer的加入和离开 触发一次rebalance，当上一点发生的时候 记录consumer与partition对应关系，追踪每个partition的offset 4、交付保证 kafka只保证“至少一次”的数据交付（at-least-once）。如果外部应用程序不能引入重复数据，则它需要自行实现去重逻辑（deduplicated logic）。kafka只保证单个partition内的message会被按顺序交付给consumer，并不会保证不同partition之间的message的交付顺序。 ","date":"2020-08-04","objectID":"/2020-08-04-%E8%AE%BA%E6%96%87-kafka/:0:0","tags":null,"title":"Kafka","uri":"/2020-08-04-%E8%AE%BA%E6%96%87-kafka/"},{"categories":["Thesis"],"content":"一、背景及问题 共识算法（consensus algorithm）在实现大型分布式系统的可靠性方面扮演了十分关键的角色，因为它允许集群在部分机器故障的情况下保持任务正常运行。在过去十年中，主流的共识算法叫Paxos，大部分系统都使用了这个算法，并且该算法一度成为标准的教材。然而，该算法有一个缺陷就是过于复杂，想要完全理解或者完美实现都十分困难。因此，斯坦福的Diego Ongaro和John Ousterhout便提出了一个相对容易理解、容易实现的共识算法，命名为Raft。 二、相关概念 Replicated state machine：复制状态机，是共识算法的起源之处。像GFS、HDFS等的大规模分布式系统，其leader节点都只有一个，存在单点故障问题。要保证集群即使出现该故障的情况下自动恢复正常运行，如集群配置信息、leader选举心跳信息等数据必须克服单点故障的问题，这就要求这些重要的数据有可靠的方式来进行备份。一种可靠的方式就是使用复制状态机，如下图所示，复制状态机一般是通过复制日志（replicated log）来实现。每个机器上都会维护一份复制日志，日志的内容是一系列的command，机器会按日志的顺序来执行这些command，来修改自己的状态。只要保证每个机器上的复制日志是完全一致，那么就可以保证每个机器的操作结果是一致的。其中，共识算法的主要作用就是通过一些通信手段，来保持集群中每个机器的复制日志完全一致。目前比较有名的复制状态机的实现是Zookeeper和Chubby。 三、Raft算法 总的来说，Raft首先会从集群中选举一个leader，整个集群的复制日志完全由leader来维护：leader节点接收client端发送过来的日志实体（log entry），然后把日志实体分发给集群中其他节点，并告诉其他节点什么时候可以安全地执行这些日志，从而修改他们的状态。当leader宕机故障时，集群会自动发起选举，选出一个新的leader继续保持集群运行。为了便于理解和实践，Raft把共识问题拆分成三个彼此独立的子问题，从而逐一解决：（1）leader选举（2）日志复制（3）安全 1、Raft基础 Raft集群（一般规模为5台机器）中的节点在任何时刻（可以运行的情况下）必为以下三种状态之一：（1）leader（2）follower（3）candidate。在正常情况下，集群中只有一个leader，其余均为follower。follower不响应任何client端请求，它只会把这些请求重定向给leader，并且只响应leader的请求。 Raft把时间分割成任意长度的周期（term），这些周期用连续的整数来唯一地标识，可以理解为周期id。每个周期都由leader选举开始，在这个周期内如果一个节点当选为leader，它会一直作为leader直到这个周期结束。集群中每个节点自身都会维护一个周期id作为自己看到的当前周期。当节点之间进行通信时会带上自己的当前周期，如果节点发现对方周期id比自己大，则更新自己的当前周期；如果节点发现自己的当前周期已经过期，则马上把自己转为follower状态；如果节点发现对方的周期已经过期，则直接拒绝对方的请求。 2、leader选举 Raft使用心跳机制（heartbeat）来触发leader选举。当Raft集群刚启动时，所有节点都是follower状态。follower节点一段时间内发现没有收到来自leader的心跳，认为leader节点已经失效（election timeout），则马上把自己转变为candidate状态，并将自己的当前周期增大。它会先为自己投票，然后向集群中其他节点发起拉票请求（RequestVote RPC）。candidate节点会一直持续这个状态直到（1）它获得多数票从而赢得选举成为新leader（2）其他节点胜出成为新leader（3）选举超时，没有选出新leader。 Raft投票规则：（1）每个节点每次选举周期内最多投出一票（2）先到先得原则，给先请求的节点投票（2）candidate只给自己投一票，然后并发地向其他节点拉票（4）特殊情况下，可能会进入僵持状态——例如所有节点同时进入candidate状态，只给自己投票，然后等等其他人给自己投票，此时的集群状态称为“split”。该论文解决这个问题使用了一种简单的方法，就是随机话选举超时时间，避免多个节点同时开启选举周期，使得先开启选举的节点有先发优势成为新leader，减少集群不可用时间。 当一个candidate成为新leader后，马上开始向其他节点发送心跳宣告自己的leader身份。其他节点确认该新leader的当前周期是有效周期后，认可新leader的身份，把自己转变为follower；否则无视该leader，继续保持选举。 3、日志复制 当节点成为leader后，开始负责响应client的请求。client的请求会包含一个需要整个集群都执行的command。leader首先会把该command追加到自己的日志中，然后通过RPC（AppendEntries RPCs）把日志分发到其他节点。若某个follower宕机或者网络丢包没有复制成功，leader会无限地重新尝试，直到复制成功。分发完成后，leader执行该command，修改自己的状态，然后把执行结果返回给client。具体关于日志的组织形式、压缩方法，详见原论文。 4、安全性 这部分较为繁琐，详见原论文。 ","date":"2020-07-26","objectID":"/2020-07-26-%E8%AE%BA%E6%96%87-raft/:0:0","tags":null,"title":"Raft","uri":"/2020-07-26-%E8%AE%BA%E6%96%87-raft/"},{"categories":["Algorithm"],"content":"最近在研究 MapReduce 和 Spark 的相关资料，顺便补个关于如何在虚拟机中搭建的 Spark 上分布式运行 PageRank 算法的博客。犹记得这个小任务是研一开学时，导师布置的第一个小任务，而现在马上就要硕士毕业答辩了，不禁思绪万千。 一、PageRank 算法 PageRank 算法是谷歌的起家算法，凭借该算法谷歌击败了当时所有的其他门户网站以及搜索引擎。该算法的目的是对数以亿计的网页进行排序，重要的网页将被排在前列，作为搜索结果返回给用户。想起了昆丁的电影《低俗小说》中的对白：“如果你要把一具尸体藏起来，你知道世界上哪里最安全吗？那就是谷歌搜索结果的第二页”。言外之意，谷歌搜索的前几条解决方案总能满足用户，用户永远不需要翻到第二页寻找答案。由此可见谷歌的搜索算法及 PageRank 网页排序算法之强大。 PageRank 算法的详细介绍见 维基百科。总而言之，该算法的主要思路是：如果一个网页被很多重要的网页指向，那么它也是一个重要的网页。具体地，互联网中的每个网页被抽象成一个节点；如果网页 A 包含网页 B 的链接，那么有一条有向边从节点 A 指向节点 B。如此，互联网中的网页及其链接被抽象成一个由节点及有向边组成的巨大拓扑图。 拓扑图建立好后，初始化系统，令每个节点的重要性分数均为1。然后开始迭代系统，在每一轮迭代中，对于每个节点，做如下两件事： 如果出边是加权的，将该节点的分数按权重比例进行拆解并传送到对应的节点；如果出边不加权，那么将该节点的分数平均拆解并传送 搜集从其他节点传送过来的分数并求和，替换该节点原来的分数 该过程从数学上来说就是一个 马尔可夫过程，可以从数学上证明其收敛性。也就是说，该系统经过若干次迭代，必定可以演化到一个平衡态。在这个状态下，每个节点的每一次分数收入约等于其分数支出。此时，每个节点上的分数就是稳定的分数，PageRank 算法按照该分数从大到小对网页进行排序并（分页地）返回给用户。 二、PageRank 算法的 Spark 分布式实现 输入数据是一个文件，如下所示。第一行只有一个数字，表明了该数据集一共有 114529 个网页节点。从第二行开始，每一行表示一个节点的出边以及对应的权重，以 [key-value] 形式表示 ：[指向的节点id:权重]。在使用该数据集时，需要把第一行的数字删去，刚好剩下一共 114529 行，每一行的行号表示其节点id。例如，删掉第一行后，第 2 行为空行，说明节点 2 出度为 0，不指向任何其他节点；第 3 行的数据表示节点 3 指向了节点 8107、节点 22950 和 节点 108053，边的权重分别为 3320、4 和 1。 要分布式实现 PageRank，就需要按 MapReduce 编程范式来编写代码。MapReduce 接受的输入是 key-value 对，在 Map 过程中映射成新的 key-value 对，在 Reduce 过程中对相同 key 的 values 进行聚合，输入最终结果。为方便起见，使用函数式编程语言 Scala 编写。伪码如下： 输入数据(每行) noteId:[outlinkId1:weight1, outlinkedId2:weight2, ...] 预处理(每行) noteId:[outlinkId1, outlinkedId2, ...] 统计出度 [noteId1:outlinkCount1, noteId2:outlinkCount2, ...] for 0 -\u003e 迭代次数： 拆分分数 [noteId1:contribution=score1/outlinkCount1, ...] 收集分数 [noteId1:sum(inlinkIds:contributions), ...] 归一化 打印显示每个节点的最终分数 具体实验代码及效果如下所示。 按分数排序，并结合另一个数据集里面的 url 名称进行显示，得到结果如下所示。可以由域名看到排在前列的都是一些英国政府的权威网页（这个数据集是英国的网页数据集），因此该算法的排序效果是很好的。学术上更加具体的排序效果评价指标是 nDCG，这里不再展开说明了。 ","date":"2019-05-19","objectID":"/2019-05-19-%E7%AE%97%E6%B3%95-pagerank%E5%9C%A8spark%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0/:0:0","tags":null,"title":"PageRank在Spark的分布式实现","uri":"/2019-05-19-%E7%AE%97%E6%B3%95-pagerank%E5%9C%A8spark%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E5%AE%9E%E7%8E%B0/"},{"categories":["Thesis"],"content":"一、背景及问题 在谷歌提出了 MapReduce 编程模型之后，UC Berkeley 在此基础上提出了一个更加高效的模型，称为 Spark。其核心思想是将 MapReduce 的中间结果缓存到内存中，使得 Workers 可以快速读取数据而无需启动延迟极大的磁盘读取操作。这种编程模型针对一些迭代次数高、需要反复使用或者修改数据的计算任务尤其有效。根据论文的实验，Spark 在一些高迭代次数的算法实验中处理速度是 MapReduce 的 10 倍以上。 然而因为没有了将中间结果写磁盘的操作来保证容灾和恢复，因此 Spark 设计了一套别的方案来达到该目的，那就是 RDD + Lineage。RDD(Resilient Distribute Datasets)称为弹性数据集，它是一种对被操作数据的抽象，而 Lineage 称为“血统”，顾名思义它记录了每一个 RDD 演变过程中的上下文信息。当故障出现时，可以根据丢失的 RDD 的 Lineage 来追寻它的祖先 RDD ，然后重新执行演化即可恢复出丢失的 RDD。 二、Spark 编程模型 使用 Spark 的用户通过编写一个称为 Driver Program 的程序来实现自己的计算任务。Spark 为用户的并行化编程提供了三个组件：resilient distribution dataset、 parallel operations、 parallel operations。 (1) Resilient Distribution Datasets(RDDs)：一个 RDD 是一个可恢复的只读对象集合，它无须存储在磁盘中。每次对一个 RDD 的操作都会被记录下来，从而每个 RDD 可以沿着它的演化过程一直追寻它的祖先 RDD，甚至可以追寻到最开始的第一次从磁盘读取初始数据。因此这个机制保证了任何一个 RDD 都可以被恢复。具体在 Spark 系统中，RDD 由 Scala 对象来表达。论文中规定了 RDD 仅可以从四种方式构造： 从文件系统中构造，例如从 HDFS 中读取数据 并行化处理 Scala 对象，例如将 Scala 的数组分片 从另一个 RDD 转化 改变一个 RDD 的持久化状态 (2) Parallel Operations：Spark 支持的并行化操作，大体上类似于 MapReduce 的 Map 和 Reduce。有以下三类： reduce collect foreach (3) Shared Variables：Spark 支持的共享变量，在机器集群中可以传递并共享这些变量。有两类共享变量： Broadcast variables：被定义为这种变量的数据会被复制到每一个 Worker，并且只复制一次。一般用于一些只读的并且需要被每个 Worker 使用的数据，例如 lookup table 等。 Accumulators：一般用于进行计数求和的数据。没看太懂论文中这段的解释。 三、编程示例 (1) Text Search：类似于 MapReduce 论文中给出的统计单词示例。这里是统计日志文件中出现 “ERROR” 的行数。 val file = spark.textFile(\"hdfs://...\") val errs = file.filter(_.contains(\"ERROR\")) val ones = errs.map(_ =\u003e 1) val count = ones.reduce(_+_) (2) Logistic Regression：逻辑回归学习的 Spark 并行化版本。 // Read points from a text file and cache them val points = spark.textFile(\"hdfs://...\").map(parsePoint).cache() // Initialize w to random D-dimensional vector var w = Vector.random(D) // Run multiple iterations to update w for (i \u003c- 1 to ITERATIONS) { val grad = spark.accumulator(new Vector(D)) for (p \u003c- points) { // Runs in parallel val s = (1/(1+exp(-p.y*(w dot p.x)))-1)*p.y grad += s * p.x } w -= grad.value } (3) Alternating Least Squares：ALS 算法的 Spark 并行版本 val Rb = spark.broadcast(R) for (i \u003c- 1 to ITERATIONS) { U = spark.parallelize(0 until u).map(j =\u003e updateUser(j, Rb, M)).collect() M = spark.parallelize(0 until m).map(j =\u003e updateUser(j, Rb, U)).collect() } 四、Spark 实现细节 这一节论文也没有讲得很详细，具体细节可以考虑参考工具书 Spark: The Definitive Guide: Big Data Processing Made Simple Learning Spark: Lightning-Fast Big Data Analysis ","date":"2019-05-09","objectID":"/2019-05-09-%E8%AE%BA%E6%96%87-spark/:0:0","tags":null,"title":"Spark","uri":"/2019-05-09-%E8%AE%BA%E6%96%87-spark/"},{"categories":["Thesis"],"content":"一、背景及问题 这两天总算把谷歌三大论文之一的 MapReduce 看了。这篇论文的影响力就不多说了，谷歌AI首席科学家 Jeffrey Dean 的代表作之一。该论文主要提出了一个称为 MapReduce 的编程模型，主要是为了解决谷歌公司自身需要处理海量数据的问题。众所周知，谷歌的主要业务是搜索引擎，每天需要处理爬取到的海量文件和网页，以及对这些数据进行索引计算、处理查询请求等等。显然单台机器难以完成这样巨大的计算任务，必须使用机器集群。但如果使用专业的服务器，集群的成本就太高了。为了降低成本，谷歌采用了普通的计算机来搭建这个集群。为了实现这个目的，必须设计一套可以自动并行化计算、自我管理调度、以及拥有良好容错性的集群计算方案，因此 Jeffrey Dean 提出了这套编程模型，只要按照这个编程模型来调用接口实现计算任务，这些计算就可以自动地在一个由普通机器搭建的分布式集群上被并行地执行。 二、解决方案 具体来说，MapReduce 这个编程模型可以简单地分为 Map 部分以及 Reduce 部分。 Map 函数：由用户实现该接口，其功能是使算法输入的初始 key-value 对转化成用户定义的新 key-value 对。接下来系统会自动把它们按新 key 分组，即一个新 key 对应一组 values（key-values），并传递给 Reduce 函数。这一步类似 SQL 中的 Select GroupBy 操作。 $$key_{1}/value_{1} \\implies key_{2}/value_{2} \\implies key_{2}/value_{2}(s)$$ Reduce 函数：由用户实现该接口，其功能是使从 Map 函数接收过来的 key-values 组按用户定义的计算方式进行 Merge，然后输出每个 key 组的最终结果。这一步类似定义 SQL 中对被 GroupBy 的字段的聚合函数。 $$key_{2}/value_{2}(s) \\implies key_{2}/value_{3} $$ 论文中给出了一个具体例子，从一个巨大的文档集中统计每个单词的出现次数。每个单词可以看作 key，它的出现次数就是最终想得到的 value。按照 MapReduce 编程范式，可以分为两步，在 Map 阶段输入文档集合，为每个文档拆解出每个单词，并为每个单词赋值它的 value 为 1，表示这个单词在此出现了一次；在 Reduce 阶段，把每个单词对应的分组进行 value 的求和，即可得到每个单词的总出现次数。 Map 函数伪代码如下： // key: document name // value: document content map(String key, String value): foreach word in value: EmitIntermediate(word,\"1\"); Reduce 函数伪代码如下： // key: a word // values: a lists of counts reduce(String key, Iterator values): int results=0; foreach value in values: result += ParseInt(value); Emit(AsString(result)); 论文中还提到另外一些使用了该编程模型的例子，例如 Distributed Grep Count of URL Access Frequency Reverse Web-Link Graph Term-Vector per Host Inverted Index Distributed Sort 论文接下来就是介绍 MapReduce 系统的实现。总的来说，其架构采用 Master-Slaves 结构，Master 节点负责分配计算任务与分发数据以及监视 Workers 节点的状态。Workers 节点负责执行被分配给自己的任务，为了保证系统的容错性以及减少网络带宽消耗，Workers 的中间计算结果写入本地存储，并且尽可能由本地的 Worker 来读取。工作流程及其架构如下图所示。 其他一些具体特性如系统容错性、数据备份方案、工作任务粒度等等具体参见论文。 三、总结 论文主要针对并行处理海量数据的问题，提出了称为 MapReduce 的编程模型，只要用户将按照该范式实现 Map 和 Reduce 两个接口，该计算任务就可以自动地被一个由普通机器搭建而成的集群正确地分布式执行，而用户无需关心底层的具体执行情况。 ","date":"2019-05-08","objectID":"/2019-05-08-%E8%AE%BA%E6%96%87-mapreduce/:0:0","tags":null,"title":"MapReduce","uri":"/2019-05-08-%E8%AE%BA%E6%96%87-mapreduce/"},{"categories":["Thesis"],"content":"该文章依然是针对 APT 攻击而提出的一种基于模型推断的攻击因果分析。该方法比以往方法的优势在于不需要在系统中做任何修改，只需要启动日志，对系统事件进行记录即可。该方法的核心技术是使用一种称为 LDX 的动态分析方案，可以在系统调用之间进行因果推断。 ","date":"2019-01-27","objectID":"/2019-01-27-%E8%AE%BA%E6%96%87-mci/:0:0","tags":null,"title":"MCI","uri":"/2019-01-27-%E8%AE%BA%E6%96%87-mci/"},{"categories":["Thesis"],"content":"一、背景及问题 索引（Index）是很多系统和应用的基础构件。最近，大量的研究专注于如何保护索引这样的敏感数据，例如，如何在对索引进行加密的同时允许客户端在索引上进行查询。这些研究提出的方案一般是这样的：当用户通过客户端发起查询请求时，客户端会先为用户输入的关键词（keyword）生成查询令牌(search token)，然后将令牌发送给服务端而不是直接发送用户的关键词，从而向服务端屏蔽用户输入的关键词（假设攻击者控制了服务端的内存，但不能控制服务端的处理器）。然后，服务器通过令牌去在已经加密的索引上执行检索，最后把匹配成功的加密数据返回给客户端。客户端对数据进行解密，显示给用户。 虽然加密索引的研究取得了重大进步，但是很多方案都有一个很严重的漏洞，就是泄漏了存取模式（access patterns）：关键词与数据的匹配过程。虽然关键词和索引都是已经加密的，但是它们的匹配过程是在服务端的内存进行的，而服务端的内存是被攻击者掌控的，因此攻击者可以对匹配的过程进行分析，从而从加密的索引中恢复敏感信息。除了存取模式的遗漏，还有如果攻击者可以获得匹配成功的文件数（result size），也可以恢复出敏感信息。 一种很直接的隐藏存取模式的方法就是使用 ORAM（Oblivious RAM），然而这个方案的成本十分昂贵，因此很少方案采用这个方法。针对这个问题，文章提出了一个称为 Oblix (OBLivious IndeX) 的索引方案，这个方案既不会泄漏任何的存取模式，也不会泄漏匹配文件数。进一步，Oblix 允许索引进行插入和删除、支持多用户。 二、解决方案 文章提出的 Oblix 主要针对以下四种问题进行解决： 1、高复杂度：对于 ORAM 系统，客户端维护一个位置映射数据结构（position map），该数据结构记录了索引与数据库中某个数据的位置的对应关系。由于索引的大小与数据库的数据量成线性关系，因此客户端不能直接存储整个 map。一种标准的方案是使用树形结构将一个数据库分解成多级数据库（类似于多级索引），以减小客户端需要的存储空间。但是如此一来就会需要重复多次查询来确定最终的位置，显然树搜索的时间复杂度是 O(logN)，这样的复杂度会带来查询延迟。文章使用的解决方案是使用飞地技术（enclave），把整个 ORAM 的客户端放进飞地中，这样让客户端和服务端就在同一台机器上进行，不需要通过网络，从而减小网络延迟。 2、飞地可能会被攻击者利用：有研究发现飞地即使物理上隔离了外界的控制，但是也可能被攻击者通过分析它的物理页级的存取模式（page-level access pattern）来恢复被加密的数据。因为飞地自身的内存空间很小，当飞地处理的数据量较大时，它就会使用二级存储，使用计算机中的内存。而计算机中的内存是会被攻击者控制的，因此攻击者可以通过观察飞地如果使用计算机主存来进行攻击。文章的解决方案：首先，提出一个称为 ODS (oblivious data structure) 的数据结构来取代 position map，这个数据结构保证不可被攻击者利用。其次，提出一个称为 doubly oblivious 的方案，该方案同时保证对服务端的存取、对客户端自己的内存空间的存取模式都是不可被攻击者利用的（oblivious）。 3、查询结果数的隐藏：前面提到，查询结果数可能也会被攻击者用，因此需要对其进行隐藏。一种最简单的做法是“worst-case upper bound”，但是这种做法代价太昂贵。文章提出的解决方案：对查询结果进行打分排序，只返回得分最高的前 r 条数据。 4、请求列表、查询效率：客户端可以一次向服务端发送多个请求，也就请求列表。特别地，其中包含插入和删除得操作时，执行效率就尤其重要。文章解决方案：提出一个称为 DOSM (doubly-oblivious sorted multimap) 的数据结构，支持高效的范围查询、插入、删除等操作。它实际上是一个树形的数据结构，插入和删除的复杂度是 O(logN) 而不是 O(n)。另外再结合特定的查询算法来保证范围查询的高效性。 三、总结 文章主要针对存取模式的漏洞来进行修复。攻击虽然无法直接攻击飞地，无法直接攻击加密数据，但是可以通过应用对主存的存取模式进行分析从而恢复加密数据。这种存取模式在文章中是指例如索引这样的负责映射数据的数据结构。 ","date":"2019-01-19","objectID":"/2019-01-19-%E8%AE%BA%E6%96%87-oblix/:0:0","tags":null,"title":"Oblix","uri":"/2019-01-19-%E8%AE%BA%E6%96%87-oblix/"},{"categories":["Math"],"content":"一、随机变量 随机变量就是“其值随机会而定”的变量。随机变量主要分为两类，一类是离散型随机变量，它有若干个不同的可枚举的取值，这些取值以不同（或相同）的概率出现，例如扔一个骰子可能会出现的点数。另一类是连续型随机变量，它的全部可能取值不仅是无穷多的，并且还不能无遗漏地逐一排列，而是充满一个区间，例如称量一个物体重量的误差。 ","date":"2019-01-16","objectID":"/2019-01-16-%E6%A6%82%E7%8E%87-%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/:0:1","tags":null,"title":"一维随机变量","uri":"/2019-01-16-%E6%A6%82%E7%8E%87-%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/"},{"categories":["Math"],"content":"二、离散型随机变量 1、离散型随机变量的概率分布：设 $X$ 为离散型随机变量，其全部可能取值 ${a_1,a_2,\\cdots}$。则 $$p_i=P(X=a_i) \\quad (i=1,2,\\cdots)$$ 称为 $X$ 的概率分布。根据概率的定义，显然有 $$p_i \\geq 0$$ $$p_1+p_2+\\cdots=1$$ 用分布表的形式给出为 $$ \\begin{array}{c|lcr} \\text{可能值} \u0026 a_1 \u0026 a_2 \u0026 \\cdots \u0026 a_i \\cdots \\\\ \\hline \\text{概率} \u0026 p_1 \u0026 p_2 \u0026 \\cdots \u0026p_i \\cdots \\end{array} $$ 2、离散型随机变量的分布函数：设 $X$ 为一离散型随机变量，则函数 $$F(x)=P(X \\leq x)=\\sum_{{i|a_i \\leq x}}p_i \\quad (-\\infty \u003c x \u003c +\\infty)$$ 称为 $X$ 的分布函数。实质上就是随机变量 $X$ 的部分可能取值的概率之和。分布函数有下面的一般性质： 当 $x_1 \u003c x_2$ 时，有 $F(x_1) \\leq F(x_2)$ 当 $x \\to +\\infty$ 时，$F(x) \\to 1$；当 $x \\to -\\infty$ 时，$F(x) \\to 0$ 3、常见的离散型随机变量 (1)、二项分布：设在某一个试验中，有且仅有两种可能结果：事件 $A$ 发生，或者 $A$ 不发生。其中 $A$ 发生的概率记为 $p$，则 $A$ 不发生的概率显然为 $1-p$。现把这个试验独立地重复 $n$ 次，记 $A$ 发生的次数为 $X$ ，则显然 $X$ 的可能取值为 $0,1,2,\\cdots,n$。显然这是一个组合问题，$n$ 个试验结果中设有 $i$ 次为 $A$，那么必定 $n-i$ 次为 $\\bar{A}$。因此 $X=i$ 的概率即为从 $n$ 个试验中取 $i$ 个 $p$，$n-i$ 个 $1-p$，即 $$P(X=i)=C_{n}^{i} p^i (1-p)^{n-i} \\quad (i=0,1,\\cdots,n)$$ 此时 $X$ 所遵从的概率分布称为二项分布，记为 $B(n,p)$。而 $X$ 服从二项分布记为 $X \\sim B(n,p)$ (2)、泊松分布：若随机变量 $X$ 的可能取值为 $0,1,2,\\cdots$，且概率分布为 $$P(X=i)=\\frac{e^{-\\lambda}\\lambda^i}{i!}$$ 则称 $X$ 服从泊松分布，记为 $X \\sim P(\\lambda)$。此处，$\\lambda \u003e 0$ 是某一常数。该分布是最重要的离散型分布之一，它多出现在当 $X$ 表示某时间或空间内出现的事件个数或者事件的频率分布。该分布可以看做是二项分布的极限形式，即若取 $\\lambda$ 为二项分布 $B(n,p)$ 的期望，即 $\\lambda=pn$，则当二项分布的试验次数 $n \\to +\\infty$ 时， $$ \\begin{aligned} P(X=i) \u0026=C_{n}^{i} (\\frac{\\lambda}{n})^i (1-\\frac{\\lambda}{n})^{n-i}\\\\ \u0026=\\frac{C_n^i}{n^i} \\cdot \\lambda^i \\cdot (1-\\frac{\\lambda}{n})^n \\\\ \u0026=\\frac{1}{i!} \\cdot \\lambda^i \\cdot e^{-\\lambda} \\\\ \u0026=\\frac{e^{-\\lambda}\\lambda^i}{i!} \\end{aligned} $$ 在实际应用中， $\\lambda$ 往往就取随机变量的期望，例如某用户在某段时间内重复购买某件商品的次数的期望，例如 2 次。确定了 $\\lambda$ 后，该用户重复购买商品次数的泊松分布模型就可以完全确定，从而可以计算用户在同样的时间段内重复购买 0 次、1 次、3 次、5次等次数的概率。 ","date":"2019-01-16","objectID":"/2019-01-16-%E6%A6%82%E7%8E%87-%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/:0:2","tags":null,"title":"一维随机变量","uri":"/2019-01-16-%E6%A6%82%E7%8E%87-%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/"},{"categories":["Math"],"content":"三、连续型随机变量 1、连续型随机变量的概率分布：对于连续型随机变量的概率分布，不能用像离散型随机变量那种方法去描述，因为连续型变量的取值是充满一个区间的，无法一一枚举，并且要研究连续型变量恰好取某个精确的值意义也不大，例如“射击靶面恰好精准命中某一个点”是几乎不可能也毫无意义的事，一般认为这样概率为 0。对于连续型随机变量，我们研究的要点是“该变量的取值落在某个区间内的概率多大”，而不是“该变量的取值恰好等于某个无限精确的值的概率有多大”。 2、连续型随机变量的分布函数：设 $X$ 为一连续型随机变量，则函数 $$F(x)=P(X \\leq x)=\\int_{-\\infty}^{x} f(t) dt$$ 称为 $X$ 的分布函数。其中 $f(x)=F’(x)$ 称为 $X$ 的概率密度函数。引用陈希孺教授《概率论与数理统计》中对“概率密度”的解释： “概率密度”这个名词的来由可以解释如下：取定一个点 $x$，则按分布函数的定义，事件 ${x \u003c X \\leq x+\\Delta x}$ 的概率应为 $F(x+\\Delta x)-F(x)$，所以比值 $\\frac{F(x+\\Delta x)-F(x)}{\\Delta x}$ 可解释为在 $x$ 点附近 $\\Delta x$ 长度的区间内，单位长度占有的概率。当 $\\Delta x \\to 0$ 时，这个比值的极限就是 $F'(x)=f(x)$，也就是在 $x$ 点处无穷小区段内单位长度的概率，它反映了概率在 $x$ 点处的“密集程度”。可以设想一条极细的无穷长的金属杆，总质量为 1，概率密度相当于杆上各点的质量密度。 连续型随机变量 $X$ 的概率密度函数 $f(x)$ 有以下基本性质： $f(x) \\geq 0$ $\\int_{-\\infty}^{+\\infty} f(x)dx=1$ 对任何常数 $a \u003c b$，有 $P(a \\leq X \\leq b)=F(b)-F(a)=\\int_{a}^{b}f(x)dx$ 3、常见的连续型随机变量： (1)、正态分布：若随机变量 $X$ 具有概率密度函数 $$f(x)=\\frac{1}{\\sqrt{2\\pi}\\delta}e^{-\\frac{(x-\\mu)^2}{2\\delta^2}} \\quad (-\\infty \u003c x \u003c +\\infty)$$ 则称 $X$ 为正态随机变量，记为 $X \\sim N(\\mu, \\delta)$。该密度函数没有显式的原函数，也即正态分布没有显式的分布函数。但是正态分布有一个重要的性质，就是任何正态分布可以转化成标准正态分布 $N(0,1)$，然后通过查标准正态分布概率表来计算原正态分布的概率。即 $$\\text{ 若 } X \\sim N(\\mu, \\delta), \\text{ 则 } Y=\\frac{X-\\mu}{\\delta} \\sim N(0,1)$$ 正态分布具有非常重要的意义，它的图形是关于 $x=\\mu$ 对称，并在该点取得最大概率值，然后向两端逐渐降低。自然界大部分的数据都是呈现这样的分布：接近均值的概率大，接近极端值的概率小。例如一个学校的身高分布、随机采样得到的样本的误差分布等等。而更重要的是，根据中心极限定理，当随机变量的数量趋向无穷的时候，它们的和的极限分布就是正态分布。另外多维的高斯分布也是对各种真实世界中的事件进行建模的良好工具。 (2)、指数分布：若随机变量 $X$ 具有概率密度函数 $$f(x)= \\begin{cases} \\lambda e^{-\\lambda x}, \u0026 x0 \\\\ 0, \u0026 x \\leq 0 \\end{cases} $$ 则称 $X$ 服从指数分布，其中 $\\lambda\u003e0$ 为参数。利用微积分知识易求得其分布函数为 $$F(x)=\\int_{-\\infty}^{x}f(t)dt= \\begin{cases} 1-e^{-\\lambda x}, \u0026 x 0 \\\\ 0, \u0026 x \\leq 0 \\end{cases} $$ 指数分布最常见的一个场合就是寿命分布，寿命越长概率越小。 (3)、均匀分布：若随机变量 $X$ 具有概率密度函数 $$f(x)= \\begin{cases} \\frac{1}{b-a}, \u0026 a\\leq x\\leq b \\\\ 0, \u0026 \\text{otherwise} \\end{cases} $$ 则称 $X$ 服从区间 $[a,b]$ 上的均匀分布，记为 $X \\sim R(a,b)$。这里 $a,b$ 都是常数且 $-\\infty \u003c a \u003c b \u003c +\\infty$。易求其分布函数为 $$F(x)= \\begin{cases} 0, \u0026 x \\leq a \\\\ \\frac{x-a}{b-a}, \u0026 a \\leq x \\leq b \\\\ 1, \u0026 x \\geq b \\end{cases} $$ 因为均匀分布的特点是区间中各点的概率密度都是相同的，因此一般用来产生均匀的随机数。 ","date":"2019-01-16","objectID":"/2019-01-16-%E6%A6%82%E7%8E%87-%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/:0:3","tags":null,"title":"一维随机变量","uri":"/2019-01-16-%E6%A6%82%E7%8E%87-%E4%B8%80%E7%BB%B4%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F/"},{"categories":["Thesis"],"content":"一、背景及问题 这篇文章针对的问题和前面看过的 《Dependence-Preserving Data Compaction for Scalable Forensic Analysis》和 这篇文章类似，目标都是减小日志文件的体积，以减少存储空间并提高攻击侦查的速度。 这篇文章主要针对的是分布式服务器集群下的 APT 攻击。在这样的情景中，服务器集群有一个中心监控节点（Monitor node），以及大量的工作节点（Worker nodes）。监控节点与所有的工作节点保持通信。工作节点在生产环境中不断地产生日志记录。当发起监控节点攻击侦查分析时，它需要从多个工作节点上拷贝日志文件到本地，再进行攻击分析。问题就出现在这里，大量的工作节点每天都产生大量的日志记录，监控节点几乎不可能从所有工作节点上拷贝全部的日志记录，因为这样的拷贝任务不但需要消耗大量的网络资源，并且需要消耗大量的存储空间。 二、解决方案 这篇文章和上述的那篇文章有点类似，其核心思想都是通过对资源图（Provenance Graph）进行剪枝、压缩、优化，从而减小对应的日志记录体积。不同的只是他们使用的剪枝算法不同而已，这篇文章使用的算法是一个称为 Deterministic Finite Automata (DFA) 的算法，这个算法可以对一个复杂的图中的每一个节点、边（对应系统日志记录中的事件以及信息流向）进行推断，判断出它是不是冗余的，如果是冗余的就可以去掉，从而极大提精简了日志记录。 文章提出的模型称为 Winnower，具体剪枝算法较复杂，主要分为图抽象（Provenance Graph Abstraction）和图推断（Provenance Graph Induction）两部分，其中涉及一些图语义学习（Graph Grammar Learning）的知识，以前没怎么接触过，感觉不太懂，以后有时间再补一下这方面的学习。 ","date":"2019-01-12","objectID":"/2019-01-12-%E8%AE%BA%E6%96%87-winnower/:0:0","tags":null,"title":"Winnower","uri":"/2019-01-12-%E8%AE%BA%E6%96%87-winnower/"},{"categories":["Math"],"content":"我们知道，线性空间中，在给定基的情况下，对任意一个向量 $\\vec{v}$ 施予一个线性变换 $T$ 时，就让该向量的坐标向量左乘某个矩阵 $A$ 即可。例如，设在线性空间 $V_n(F)$ 中，取一组基为 ${\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}}$，向量 $\\vec{w}$ 在该基下的坐标为 $\\vec{Y}$，向量 $\\vec{v}$ 在该基下的坐标为 $\\vec{X}$，即： $$\\vec{w}=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\vec{Y}$$ $$\\vec{v}=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\vec{X}$$ 如果向量 $\\vec{w}$ 是由向量 $\\vec{v}$ 经过线性变换 $T$ 得到的，即 $$\\vec{w}=T(\\vec{v})$$ 那么它们的坐标关系为： $$\\vec{Y}=A \\vec{X}$$ 并且，线性空间中任一个线性变换 $T$ 可以由某个矩阵 $A$ 唯一对应： $$T \\iff A$$ 为什么？这个矩阵 $A$ 如何得到？接下来复习一下 $A$ 的推导。需要的预备知识： 线性空间定义 线性空间的基与坐标 线性变换定义 记 $T$ 为线性空间 $V_n(F)$ 上的线性变换。记 ${\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}}$ 为 $V_n(F)$ 的一组基，则对任意向量 $\\forall \\pmb\\beta \\in V_n(F)$，由基的定义可得 $$\\vec{v}=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\vec{X}=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} =x_1\\vec{\\alpha_1}+x_2\\vec{\\alpha_2}+\\cdots+x_n\\vec{\\alpha_n}$$ 对 $\\vec{v}$ 施予线性变换 $T$： $$ \\begin{aligned} T(\\vec{v}) \u0026=T(x_1\\vec{\\alpha_1}+x_2\\vec{\\alpha_2}+\\cdots+x_n\\vec{\\alpha_n}) \\\\ \u0026=T(x_1\\vec{\\alpha_1})+T(x_2\\vec{\\alpha_2})+\\cdots+T(x_n\\vec{\\alpha_n}) \\\\ \u0026=x_1T(\\vec{\\alpha_1})+x_2T(\\vec{\\alpha_2})+\\cdots+x_nT(\\vec{\\alpha_n}) \\\\ \u0026=(T(\\vec{\\alpha_1}),T(\\vec{\\alpha_2}),\\cdots,T(\\vec{\\alpha_n})) \\begin{pmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\end{aligned} $$ 到这里可以清楚看到，对向量 $\\vec{v}$ 施予线性变换 $T$，可以转化为对基 ${ \\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n} }$ 施予线性变换，并且在基变换后，向量 $\\vec{v}$ 的坐标保持不变！因此，对空间中任意向量的线性变换，就转化到了该空间下一组基到另一组“基”的线性变换（注意，由于实际上线性变换只能保持线性相关，不能保持线性无关，因此无法保证对基进行线性变换后的向量组 ${ T(\\vec{\\alpha_1}),T(\\vec{\\alpha_2}),\\cdots,T(\\vec{\\alpha_n}) }$ 是否仍能作为一组基。准确来讲，$T(\\vec{\\alpha_{i}})$ 只能是原基的“像”。但为了便于陈述和理解，暂且称这组“像”为“新基”）。 那么接下来的问题是，对基 ${ \\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n} }$ 施予线性变换 $T$ 后，“新基” ${ T(\\vec{\\alpha_1}),T(\\vec{\\alpha_2}),\\cdots,T(\\vec{\\alpha_n}) }$ 与原基的关系是怎么样的？ 我们知道，对一个向量施予线性变换后仍是一个向量，因此“新基”中的每个元素也仍是一个向量。既然是向量，就一定可以用基和坐标来表示。那么我们就尝试用原基来表示“新基”中的每一个向量吧（因为我们希望知道“新基”与原基的关系），假设向量 $T(\\vec{\\alpha_1}),T(\\vec{\\alpha_2}),\\cdots,T(\\vec{\\alpha_n})$ 在原基 $(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n})$ 下的表达分别为： $$T(\\vec{\\alpha_1})=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\begin{pmatrix} x_{11} \\\\ x_{21} \\\\ \\vdots \\\\ x_{n1} \\end{pmatrix}$$ $$T(\\vec{\\alpha_2})=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\begin{pmatrix} x_{12} \\\\ x_{22} \\\\ \\vdots \\\\ x_{n2} \\end{pmatrix}$$ $$\\cdots$$ $$T(\\vec{\\alpha_n})=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\begin{pmatrix} x_{1n} \\\\ x_{2n} \\\\ \\vdots \\\\ x_{nn} \\end{pmatrix}$$ 把以上式子写到一起： $$(T(\\vec{\\alpha_1}),T(\\vec{\\alpha_2}),\\cdots,T(\\vec{\\alpha_n}))=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\begin{pmatrix} x_{11} \u0026 x_{12} \u0026 \\cdots \u0026 x_{1n} \\\\ x_{21} \u0026 x_{22} \u0026 \\cdots\u0026 x_{2n} \\\\ \\vdots \u0026 \\vdots \u0026 \\ddots \u0026 \\vdots \\\\ x_{n1} \u0026 x_{n2} \u0026 \\cdots \u0026 x_{nn} \\\\ \\end{pmatrix} $$ 为简便表达，用 $A$ 来表达矩阵 $[x_{ij}]$，因此 $$(T(\\vec{\\alpha_1}),T(\\vec{\\alpha_2}),\\cdots,T(\\vec{\\alpha_n}))=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) A$$ 这就是线性变换后 “新基” ${ T(\\vec{\\alpha_1}),T(\\vec{\\alpha_2}),\\cdots,T(\\vec{\\alpha_n}) }$ 与原基 ${ \\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n} }$ 的关系表达式。我们称矩阵 $A$ 为变换矩阵。 从上述过程可以知道，变换矩阵 $A$ 的第 $i$ 列就是原基中第 $i$ 个向量 $\\vec{\\alpha_i}$ 的像 $T(\\vec{\\alpha_i})$ 在原基下的坐标表示，坐标的唯一性决定了 $A$ 的唯一性。也就是说，只要选定了一组基，线性变换 $T$ 对这组基的作用（“新基”的坐标），在这组基下是唯一的。因此，在选定了一组基的情况下，线性变换 $T$ 与矩阵 $A$ 是一一对应的，即： $$T \\iff A $$ 回到本文开头： $$\\vec{w}=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\vec{Y}$$ $$\\vec{v}=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) \\vec{X}$$ 如果向量 $\\vec{w}$ 是由向量 $\\vec{v}$ 经过线性变换 $T$ 得到的，即有 $$\\vec{w}=T(\\vec{v})=(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n}) A \\vec{X}$$ 通过对比，显然有： $$\\vec{Y}=A \\vec{X}$$ 特别地，当基 $(\\vec{\\alpha_1},\\vec{\\alpha_2},\\cdots,\\vec{\\alpha_n})$ 为 $R^n$ 中的自然基，即 $\\vec{\\alpha_1}= \\begin{pmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix}$，$\\vec{\\alpha_2}= \\begin{pmatrix} 0 \\\\ 1 \\\\ \\vdo","date":"2019-01-12","objectID":"/2019-01-12-%E6%95%B0%E5%AD%A6-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E7%9A%84%E7%9F%A9%E9%98%B5/:0:0","tags":null,"title":"线性变换的矩阵","uri":"/2019-01-12-%E6%95%B0%E5%AD%A6-%E7%BA%BF%E6%80%A7%E5%8F%98%E6%8D%A2%E7%9A%84%E7%9F%A9%E9%98%B5/"},{"categories":["Math"],"content":"泰勒公式是怎么被推导出来的，以及为什么可以用多项式来逼近任意函数？ 根据微积分的“以直代曲”的思想，对一个连续可导的函数 $f(x)$，当因变量 $x$ 在某点 $x_0$ 附近的改变值为 $\\Delta x$ 时，过该点的切线上对应的纵坐标增量为 $dy=f'(x_0) \\cdot \\Delta x$。虽然 $f(x)$ 实际上的纵坐标增量是 $\\Delta y=f(x_0+\\Delta x)-f(x_0)$，但是如果 $\\Delta x$ 非常小，那么可以近似认为切线的纵坐标增量等于 $f(x)$ 的纵坐标增量。即： $$\\Delta y=f(x_0+\\Delta x)-f(x_0) \\approx f'(x_0) \\cdot \\Delta x$$ 那么对于 $x_0$ 附近的任意一点 $x$，有 $\\Delta x=x-x_0$，代入上式得： $$f(x)-f(x_0) \\approx f'(x_0) \\cdot (x-x_0)$$ 将 $f(x_0)$ 移到右边，凑出关于 $x$ 的函数表达形式： $$f(x) \\approx f(x_0)+f'(x_0) \\cdot (x-x_0)$$ 观察这个式子，显然它就是利用已知的一个点 $(x_0,f(x_0))$ 及在该点处的一阶导数 $f'(x_0)$ 来近似表达了在 $x_0$ 附近的原函数 $f(x)$，而且它是一个线性的函数，非常便于我们计算和处理，特别是在原函数 $f(x)$ 是一个非常复杂的函数的情况下。 到了这里，严谨的数学家肯定不满足于如此，因为这只是一个非常粗略的近似。那么如何进一步逼近原函数 $f(x)$？甚至什么时候可以取等号？这就是泰勒以及众多数学家的贡献了。 既然一阶导数和一次多项式可以用来近似代替函数 $f(x)$，而现在希望能进一步提高近似的精度，那么一个非常自然、直接的想法就是使用更高阶的导数以及更高阶的多项式。因为多项式函数无论是连续求导、变形、还是数值计算，都要比其他一些函数例如三角函数、指数函数、对数函数、幂函数等容易得多，所以自然选择多项式来作为基本构件。这其实在生活中是很常见的思路： 当遇到一个复杂的、无法直接使用的东西时，我们自然想办法用其他一些简单的、方便使用的东西去近似模拟它，得到和它基本相近的功能 当我们要去近似模拟时，要直接一步到位是很难的，一般是先模拟一个大体的近似，然后在这个基础上，一步步去完善细节，一步步提高精度 如第一点所说，我们肯定是使用简单的零部件去构建整体，并且越简单越好，因为越简单则越容易理解和改造、控制。其实大自然亦如此，复杂如人的神经系统、大脑组织，也是由一个个十分简单、有效的神经细胞组建而成 相信上面的陈述应该可以说服你，像数学家那样使用高阶导数和高阶多项式来进行近似代替某个复杂但连续并且可导的函数 $f(x)$。我们知道，最简单的多项式函数表达形式如下： $$g(x)=a_0+a_1x+a_2x^2+\\cdots+a_nx^n$$ 回顾一阶近似的时候，近似的函数形式是 $$f(x) \\approx f(x_0)+f'(x_0) \\cdot (x-x_0)$$ 结合两者，我们依样画葫芦地写出： $$f(x)=a_0+a_1(x-x_0)+a_2(x-x_0)^2+ \\cdots + a_n(x-x_0)^n$$ 至此泰勒展开已经完成了一半了，只剩各项的系数需要确定。对比一阶近似，一次项的系数是 $f'(x_0)$，这暗示着各项的系数与函数 $f(x)$ 在 $x_0$ 处的导数存在某种关系，启发我们往这个方向去假设和推测。以下是推导过程。 对 $f(x)$ 求 $1$ 至 $n$ 阶导： $$f'(x)=a_1+2a_2(x-x_0)+3a_3(x-x_0)^2+\\cdots+na_n(x-x_0)^{n-1}$$ $$f''(x)=2a_2+6a_3(x-x_0)+\\cdots+n(n-1)a_n(x-x_0)^{n-2}$$ $$f'''(x)=6a_3+\\cdots+n(n-1)(n-2)a_n(x-x_0)^{n-3}$$ $$\\cdots$$ $$f^{(n)}(x)=n(n-1)(n-2)\\cdots \\cdot 2 \\cdot 1 \\cdot a_n$$ 令 $x=x_0$ 代入以上式子，得 $$f'(x_0)=1 \\cdot a_1 = 1! \\cdot a_1$$ $$f''(x_0)=2 \\cdot 1 \\cdot a_2=2! \\cdot a_2$$ $$f'''(x_0)=3 \\cdot 2 \\cdot 1 \\cdot a_3=3! \\cdot a_3$$ $$\\cdots$$ $$f^{(n)}(x_0)= n \\cdot (n-1) \\cdots \\cdot 2 \\cdot 1 \\cdot a_n=n! \\cdot a_n$$ 因此即得 $$a_i=\\frac{f^{(i)}(x_0)}{i!},i \\in [1,n]$$ 因此泰勒展开式为 $$ \\begin{aligned} f(x) \u0026=f(x_0)+f'(x_0)(x-x_0)+\\frac{f''(x_0)}{2!}(x-x_0)+\\cdots+\\frac{f^{(n)}(x_0)}{n!}(x-x_0) \\\\ \u0026 =f(x_0)+\\sum_{i=1}^{n}\\frac{f^{(i)}(x_0)}{i!}(x-x_0)^i \\\\ \u0026 = \\sum_{i=0}^{n}\\frac{f^{(i)}(x_0)}{i!}(x-x_0)^i \\end{aligned} $$ 其实严格来说以上都不正确，因此近似始终不是等于，无论 $n$ 有多大，式子右边始终不是 $f(x)$，上述式子应该取约等号。如果要取等号，那么需要把每一项近似的误差也补完整，完整的泰勒展开为 $$f(x)=\\sum_{i=0}^{n}\\frac{f^{(i)}(x_0)}{i!}(x-x_0)^i+R_n(x)$$ 其中 $R_n(x)$ 则为近似的总误差。一共有两位数学家先后尝试推导这个误差项 $R_n(x)$，分别为佩亚诺和拉格朗日。其中佩亚诺给出的结果称为佩亚诺余项： $$R_n(x)=o((x-x_0)^n)$$ 佩亚诺余项只是定性地说明误差项是 $(x-x_0)^n$ 高阶的无穷小，并不能具体估算误差大小，因此后来拉格朗日进一步给出了具体形式的余项，称为拉格朗日余项： $$R_n(x)=\\frac{f^{n+1}(\\xi)}{(n+1)!}(x-x_0)^{n+1}$$ 其中 $\\xi$ 是 $x$ 与 $x_0$ 之间的某个值。要推导这个余项需要用到柯西中值定理，具体推导过程不再详述，可参考资料： 《高等数学》同济版 怎样更好地理解并记忆泰勒展开式？ 如何通俗地解释泰勒公式？ 泰勒公式最初是如何想到的? ","date":"2019-01-11","objectID":"/2019-01-11-%E6%95%B0%E5%AD%A6-%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E5%BC%8F/:0:0","tags":null,"title":"泰勒展开式","uri":"/2019-01-11-%E6%95%B0%E5%AD%A6-%E6%B3%B0%E5%8B%92%E5%B1%95%E5%BC%80%E5%BC%8F/"},{"categories":["Math"],"content":"一、概率 1、概率的公理化定义（柯氏理论体系）：事件有概率，其大小随事件而异。也即，概率是事件的函数：事件 $A$ 的概率记为 $P(A)$。 2、古典概率：设一个试验有 $N$ 个等可能的结果，而事件 $E$ 恰包含其中的 $M$ 个结果，则事件 $E$ 的概率，记为 $P(E)$，定义为 $$P(E)=\\frac{M}{N}$$ 3、古典概率计算归结为计算两个数 $M$ 和 $N$。这种计算大多涉及排列组合。排列组合公式： (1) $n$ 个相异物件取 $r(1\\leq r \\leq n)$ 个的不同排列总数为 $$A_{n}^{r}=n(n-1)(n-2) \\cdots (n-r+1)=\\frac{n!}{(n-r)!}$$ (2) $n$ 个相异物件取 $r(1\\leq r \\leq n)$ 个的不同组合总数为 $$C_{n}^{r}=\\frac{A_{n}^{r}}{r!}=\\frac{n!}{r!(n-r)!}$$ (3) 组合系数 $C_{n}^{r}$ 又常称为二项式系数，因为它出现在二项式展开的公式中： $$(a+b)^n=\\sum_{i=0}^{n}C_{n}^{i}a^{i}b^{n-i}$$ (4) $n$ 个相异物分成 $k$ 堆，各堆物件数分别为 $r_1,\\cdots,r_k$，分法种数为： $$\\frac{n!}{r_1! \\cdots r_k!}$$ ","date":"2019-01-09","objectID":"/2019-01-09-%E6%A6%82%E7%8E%87-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/:0:1","tags":null,"title":"事件与概率","uri":"/2019-01-09-%E6%A6%82%E7%8E%87-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/"},{"categories":["Math"],"content":"二、事件的运算、条件概率与独立性 1、事件的蕴含、包含以及相等：在同一试验下的两个事件 $A$ 和 $B$，如果： (1) 当 $A$ 发生时 $B$ 必发生，则称 $A$ 蕴含 $B$，或者 $B$ 包含 A，记为 $A \\subset B$ (2) 若 $A$、$B$ 相互蕴含，即 $A \\subset B$ 且 $B \\subset A$，则称 $A$、$B$ 两事件相等，记为 $A=B$ 2、事件的互斥和对立 (1) 若两事件 $A$、$B$ 不能在同一次试验中都发生（但可以都不发生），则称它们是互斥的 (2) 若 $A$、$B$ 互斥，并且当其中一个不发生时，另一个必定发生，那么称它们是对立的。记 $A$ 的对立事件为 $\\bar A$ 3、事件的和：设有两个事件 $A$、$B$，定义一个新事件 $C$ 为 $C$={ $A$ 发生或 $B$ 发生}={ $A$、$B$ 至少发生一个}，则称事件 $C$ 为 事件 $A$、$B$ 的和，记为 $C=A+B$ 4、事件的积：设有两个事件 $A$、$B$，定义一个新事件 $C$ 为 $C$={ $A$、$B$ 都发生}，则称事件 $C$ 为 事件 $A$、$B$ 的积，记为 $C=AB$ 5、事件的差：设有两个事件 $A$、$B$，定义一个新事件 $C$ 为 $C$={ $A$ 发生、$B$ 不发生}，则称事件 $C$ 为 事件 $A$、$B$ 的差，记为 $C=A-B$ 6、条件概率：设有两个事件 $A$、$B$，其中 $P(B) \\neq 0$。则 “在给定 $B$ 发生的条件下 $A$ 的条件概率”，记为 $P(A \\mid B)$，定义为 $P(A \\mid B)=\\frac{P(AB)}{P(B)}$ 7、事件的独立性：设有两个事件 $A$、$B$，若 $P(A)=P(A \\mid B)$，则说明 $B$ 的发生与否对 $A$ 发生的可能性毫无影响。这时在概率上称 $A$、$B$ 独立。此时根据条件概率公式可推得 $P(AB)=P(A)P(B)$ ","date":"2019-01-09","objectID":"/2019-01-09-%E6%A6%82%E7%8E%87-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/:0:2","tags":null,"title":"事件与概率","uri":"/2019-01-09-%E6%A6%82%E7%8E%87-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/"},{"categories":["Math"],"content":"三、概率定理 1、$0 \\leq P(A) \\leq 1$。其中当 $$ 等于 $0$ 时 $A$ 是不可能事件，等于 $1$ 时是必然事件 2、加法定理：若干个互斥事件之和的概率，等于各事件的概率之和： $$P(A_1+A_2+\\cdots)=P(A_1)+P(A_2)+\\cdots$$ 3、乘法定理：若干个独立事件之积的概率，等于各事件概率的乘积： $$P(A_1 \\cdots A_n)=P(A_1) \\cdots P(A_n)$$ ","date":"2019-01-09","objectID":"/2019-01-09-%E6%A6%82%E7%8E%87-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/:0:3","tags":null,"title":"事件与概率","uri":"/2019-01-09-%E6%A6%82%E7%8E%87-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/"},{"categories":["Math"],"content":"四、全概率公式与贝叶斯公式 1、全概率公式：设 $B_1,B_2,\\cdots$ 为有限或无限个事件，它们两两互斥且在每次试验中至少发生一个。即 $$B_{i}B_{j} (i \\neq j) = \\emptyset \\text{(不可能事件)}$$ $$B_1+B_2+\\cdots=\\Omega \\text{(必然事件)}$$ 有时，把具有这些性质的一组事件称为一个“完备事件群”。现考虑任一事件 $A$，因为 $\\Omega$ 为必然事件，有 $A=A\\Omega=AB_1+AB_2+\\cdots$。因 $B_1$、$B_2$、$\\cdots$ 两两互斥，所以 $AB_1$、$AB_2$、$\\cdots$ 也两两互斥。故依加法定理有 $$P(A)=P(AB_1)+P(AB_2)+\\cdots$$ 再由条件概率的定义，有 $P(AB_i)=P(A \\mid B_i)P(B_i)$，代入上式得 $$ \\begin{aligned} P(A) \u0026=P(A \\mid B_1)P(B_1)+P(A \\mid B_2)P(B_2)+\\cdots \\\\ \u0026=\\sum_{i} P(A \\mid B_i)P(B_i) \\end{aligned} $$ 可理解为事件 $A$ 的概率是从各个 $B_i$ 中 “收集” 回来的。也可以理解为 $B_i$ 是导致 $A$ 发生的一种可能 “途径”，或者 称为“原因”。 2、贝叶斯公式：在全概率公式的基础上，可以推导出： $$P(B_i \\mid A)=\\frac{P(AB_i)}{P(A)}=\\frac{P(A\\mid B_i)P(B_i)}{\\sum_{j} P(B_j)P(A\\mid B_j)}$$ 其实就是对分子使用条件概率公式，对分母使用全概率公式。但是它所包含的哲理意义却不普通，引用陈希儒教授的《概率论与数理统计》一书中的解释： 如果我们把事件 $A$ 看做“结果”，把诸事件 $B_1$、$B_2$、$\\cdots$ 看做导致这个结果的“原因”，则可以形象地把全概率公式看作“由原因推导结果”；而贝叶斯公式则恰好相反，其作用在于“由结果推原因”：现在有一个“结果” $A$ 已经发生了，在众多可能的“原因”中，到底是哪一个导致了这个“结果”？ ","date":"2019-01-09","objectID":"/2019-01-09-%E6%A6%82%E7%8E%87-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/:0:4","tags":null,"title":"事件与概率","uri":"/2019-01-09-%E6%A6%82%E7%8E%87-%E4%BA%8B%E4%BB%B6%E4%B8%8E%E6%A6%82%E7%8E%87/"},{"categories":["Thesis"],"content":"一、背景及问题 APT（Advanced Persistent Threat）攻击是一种长期、多步骤的攻击，它往往会涉及多个系统事件和文件。因此一旦发现系统收到这种攻击时，必须执行攻击因果分析（Attack Causality Analysis ），即从一个受到攻击的事件或者文件作为入口，向前追踪其攻击源头，并且向后分析所有被攻击牵涉中的其他文件或数据。 一般来说，攻击因果分析是基于系统资源的依赖图进行的。依赖图记录了系统中各种事件的先后次序和信息流，因此依赖图往往十分巨大，特别对于拥有大量机器的大型企业来说。以往的研究都是关注于如何对这个巨大的依赖图进行剪枝，以减少需要分析的数据量。但是这些研究都没有考虑执行分析时的时间限制，只对所有数据一视同仁地依次执行分析。文章提出，时效性对与攻击因果分析来说是非常重要的，因为在发现受到攻击后，越快地分析出攻击源以及受到污染的数据，就能越快地采取隔离措施或者修复措施，减少企业损失。据统计，90% 的 APT 攻击都是在数分钟内完成的。因此目前的问题就是，如何在保证攻击不会被遗漏和 CPU 运算能力一定的前提下，如何尽可能快地执行攻击因果分析？ 二、解决方案 文章提出一个称为 PriorTracker 的模型， 其基本思路是，以往的研究没有办法加速攻击因果分析的的原因是，他们没有办法区分出异常的系统事件和正常的系统事件，因此他们只能对依赖图中的所有路径一视同仁地去执行分析，包括一些复杂但实际上与攻击无关的正常的系统调用事件，这就导致了分析的时候花费了大量的时间和算力在无关的数据上。如果我们能够有办法区分出异常的系统事件和正常的系统事件，那么就可以优先对异常事件进行分析，最快地命中攻击。 那么如果甄别出系统中的异常事件？文章提出的一个方案是，用算法对每个系统事件（System Event）计算一个优先权（Priority Score），优先权高的事件先执行分析。优先权的计算由如下公式给出 $$PriorityScore(e)=\\alpha \\cdot RarenessScore(e) + \\beta \\cdot FanoutScore(e)$$ 其中 $e$ 表示资源依赖图中的一个系统事件；$RarenessScore$ 表示事件的稀有得分，因为异常事件必定是正常系统的过往历史中很少出现的事件；$FanoutScore$ 表示扇出得分，如果一个事件的扇出（也即在资源依赖图中的一个节点的出度）越多，那么执行这个事件的分析所花费的时间越多。另外从统计经验知道，一般来说一些正常的系统事件例如更新进程数据等等的扇出度是非常巨大的，但是这类数据对攻击分析是没什么贡献的，只会浪费 CPU 时间。因此一个自然的想法是优先计算扇出较小的事件节点，别免浪费时间。当然这也会导致分析覆盖面的减小，但是这样的 tradeoff 总的来说是需要的。而 $\\alpha$ 和 $\\beta$ 是两者的平滑系数，文章中使用机器学习方法来训练这两个参数。两者的计算公式如下： $$RarenessScore(e)= \\begin{cases} 1, \u0026 \\text{if e has not been observed by reference model} \\\\ \\frac{1}{ref(e)}, \u0026 \\text{otherwise} \\end{cases}$$ $$FanoutScore(e)= \\begin{cases} 0,\u0026 \\text{if e reaches a read-only file in backtracking} \\\\ \\delta,\u0026 \\text{if e reaches a write-only file in forward tracking} \\\\ \\frac{1}{fanout(e)}, \u0026 \\text{otherwise} \\end{cases} $$ 其中 $ref(e)$ 由文章定义的一个称为 $Reference Model$ 的模型给出，其主要思想是统计一个系统事件在一个时间段（一周或者一个月，这个可以根据企业的生产情况自行调节）内的出现总次数。而 $fanout(e)$ 文章没有给出计算公式，估计就是节点出度？ 现在有了每个事件的优先权后，就可以加速因果分析了。具体做法很简单，就是维护一个优先队列，类似于广度优先遍历，先把以捕获到的一个确认被攻击的结点放入队列作为分析起点，然后不断从队列中取出队头节点，分析它的前驱节点或者后继节点，以及这些节点的优先权，然后加入优先队列中去。如此循环，直至到达限定的时间或者依赖图的所有节点都被分析完。 三、总结 文章提出了一种利用优先权的方法来优先分析计算可疑的系统事件，加速攻击因果分析，使得能在最短时间内命中受到攻击的系统事件和数据。其中，虽然优先权的计算是比较启发式的、经验导向的公式，但是它们的平滑系数是通过机器学习来确定。 ","date":"2019-01-05","objectID":"/2019-01-05-%E8%AE%BA%E6%96%87-priortracker/:0:0","tags":null,"title":"PriorTracker","uri":"/2019-01-05-%E8%AE%BA%E6%96%87-priortracker/"},{"categories":["Thesis"],"content":"一、背景及问题 最近一些广告平台（Data Broker）例如 Facebook、Google 等推出了新的广告投放策略：广告投放商（Advertiser）可以在平台中上传目标用户的个人身份信息（Personally Identifying Information，PII）如用户的姓名、邮箱地址、电话号码、住址、生日等，以在该平台中直接定位目标用户并针对性地投放广告。与传统的数据经销商直接把数据访问权限卖给给广告投放商的方式不同，这些数据平台只允许广告投放商上传目标用户的 PII，然后平台在内部匹配这些用户，返回一个匹配成功并进行舍入（Rounding）后的广告投放目标用户（Audience）总数给广告投放商。 然而这样的策略会威胁到用户的隐私安全。虽然广告投放商不能直接访问用户的数据，但是依然可以利用这个策略的漏洞来推测用户的其他隐私信息，例如恶意的广告投放商可以利用它们手中的一些用户邮箱地址，通过平台以及某些手段来推出这些用户的姓名、电话号码、住址等等其他隐私信息。 这个策略的漏洞就是上述平台返回的匹配成功的用户总数。虽然 Facebook 等平台对该数进行了舍入（Rounding），返回的用户总数是模糊的，但是只要攻击者通过不断提交差别很小的请求，依然可以把取整策略的阈值测试出来，然后就可以反推出在阈值边界上的目标用户（Threshold audiences）的信息。例如当攻击者在第二次提交请求时，只比第一次请求多添加一个用户，而返回的成功匹配用户数从原来的 80 变成了 90，那么很显然这个用户就是在阈值边界上并且符合攻击者的匹配目标的用户。整篇文章所提的攻击都是通过此漏洞来进行的，论文原话：“We use threshold audiences throughout the paper to enable our attacks” 二、解决方案 具体地说，文章介绍的的攻击手段主要有以下三种： 1、逆匿名网页浏览用户（De-anonymizing web visitors）：广告投放商可以在一些外部网页上安装一个 Facebook 的 JavaScript 插件，Facebook 可以通过该插件追踪浏览过此网页的用户，然后将这些用户记录下来，作为广告目标用户集合（注意，这些用户对广告投放商来说是透明的，只有 Facebook 知道这些用户）。现在，攻击者（恶意的广告投放商）可以利用上述的漏洞来反推出某个用户 $V$ 是否访问了这个网页。攻击方法是： (1) 先确定 $V$ 是否是 Facebook 平台中的用户：提交一系列不含攻击目标 $V$ 的请求 ${ L_1,L_2,\\cdots,L_n }$，每次请求只比上一次请求多一个用户，Facebook 返回的匹配用户总数分别为 ${A_1,A_2,\\cdots,A_n }$。先测试出一个舍入阈值下界，例如，假设 $A_1$、$A_2$ 是 810，而 $A_3$ 是 820，那么 $A_2=810$ 是一个舍入下界。然后我们再在 $L_2$ 这个集合中加入攻击目标 $V$ 即 ${L_2 \\bigcup V }$ 并提交给 Facebook，如果返回的是 810，那么 $V$ 不是 Facebook 平台中的用户，如果返回的是 820，那么说明 $V$ 是 Facebook 中的一个用户。 (2) 接下来，确定 $V$ 是否在 Facebook 追踪的网页浏览者名单中：同样地，提交一系列包含 $V$ 的请求，每个请求只比前一个请求多一个用户，然后测试出舍入上界，例如 ${L_1 \\bigcup V}$、${L_2 \\bigcup V}$ 是 930，而 ${L_3 \\bigcup V}$ 是 940，那么 $A_3 = 940$ 就是攻击者要找的上界。此时，再提交不含 $V$ 的 ${L_4}$，看返回的结果是否下降到 930，如果下降了，说明 $V$ 就在浏览者名单中，即可以确定攻击目标 $V$ 浏览过了攻击者的网页。 2、推断用户的身份信息（Inferring a victim’s PII）：Facebook 对多次提交的请求集合，只会返回它们命中的用户的并集的总数，即不会对同一个用户多次计数。利用这一点，以及上述的舍入规则，攻击者只要拥有某个用户 $V$ 的其中某一项 PII 例如电子邮箱地址（或者其他任意一项），就可以推断出该用户的其他 PII 如电话号码。假设攻击者现在有某个 PII 集合 $L$，上传到 Facebook 后生成一个广告目标用户集合 $A$（注意，这个集合是维护在 Facebook平台内部的，对攻击者透明的），得到返回的一个舍入处理后的用户总数。假设攻击者现在要攻击的某个用户是 $V$（攻击者已知 $V$ 的邮箱地址），攻击方法是： (1) 先确定攻击目标 $V$ 是否在 $A$ 中：最简单的办法就是再上传一个只包含 $V$ 的集合，然后向 Facebook 查询这个集合与 $A$ 的交集，如果返回的交集结果不为 0，那么就可以直接确定 $V$ 在 $A$ 中了。然而 Facebook 不允许这样做，它要求查询交集的集合必须大于 1000，并且对交集结果的 $5%$ 进行舍入。因此攻击者需要稍微复杂一些的操作。取两个额外集合 ，分别为含 1949 条记录的集合 $R$、含 50 条记录的集合 $J$，要求是 $R、J、L$ 三个集合两两不相交。现在取 $C_1=R \\bigcup L$，$C_2=R \\bigcup J \\bigcup V$，提交这个两个并集，然后向 Facebook 查询交集大小。显然，如果 $V$ 在 $A$ 中，那么结果将是 $|R|$（1949 舍入后是 1900）；否则结果是 $|R|+1$（1950 舍入后是 2000）。 (2) 推断用户的电话号码：将整个电话号码的空间划分成若干个子集，例如，如果电话号码有 11 位，那么将得到 11×10 个子集，第一个子集表示电话号码第一位是 0 的所有电话号码，第二个子集表示电话号码第一位是 1 的子集，由于每一位有 0 ~ 9 共 10 种数字，因此一共 11×10 个子集。每个子集由于只固定一位数字，其余 10 位的每一位都有 10 种可能，因此每个子集的大小是 10^10。只要对这 11×10 个大小为 10^10 的电话号码集合，不断利用步骤 (1) 的方法，判断 $V$ 是否在该电话号码集合 $L$ 生成的 $A$ 中，就可以逐位推断出 $V$ 的 11 位电话号码 3、逆匿名全体用户（De-anonymizing users en masse）：意思和前面两点差不多，通过结合前两种攻击方法，可以获取所有访问过攻击者页面的用户的 PII，如电话号码等。限于篇幅不展开了。 针对以上三种攻击，文章提出了一些修补漏洞的技术方案，并提交给了 Facebook，得到了 Facebook 的采纳。由于修复方案比较多而且较多数学理论，就不展开了。由于文章提出的解决方案过于复杂，最后 Facebook 也只是采取了最简单暴力的修复方式——直接取消返回任何统计数字给广告投放商，关闭交集查询功能…… 三、总结 文章针对 Facebook 的提供给广告投放商的应用接口，发现了基于这些接口实现的用户隐私攻击，并给出了详细复杂的解决方案。最后得到了 Facebook 的采纳。 ","date":"2018-12-29","objectID":"/2018-12-29-%E8%AE%BA%E6%96%87-pii/:0:0","tags":null,"title":"PII","uri":"/2018-12-29-%E8%AE%BA%E6%96%87-pii/"},{"categories":["Thesis"],"content":"差分隐私（Differential Privacy）模型是微软研究院的 Cynthia Dwork 等人于 2006 年提出的一个严谨的数学模型，目标在于提出用以修改隐私数据的技术，使得修改后的数据可以安全发布(以供第三方进行研究)，而不会遭受去匿名化等隐私攻击。同时，修改后的数据要在保护隐私的前提下最大限度地保留原数据的整体信息，否则被发布的数据将毫无研究价值。 该论文主要提出了一个称为 Set-Value Item Mining (SVIM) 的协议，用于在隐私保护约束条件下提高本地差分隐私数据的精确度。由于差分隐私主要是纯理论的模型，没有具体的实现和应用，限于篇幅此文不展开论述。此处主要了解差分隐私的一些背景即可。 相关资料： 差分隐私学习总结 苹果的 Differential Privacy 差分隐私技术是什么原理？ 大数据下的信息安全-差分隐私保护技术 ","date":"2018-12-22","objectID":"/2018-12-22-%E8%AE%BA%E6%96%87-svim/:0:0","tags":null,"title":"SVIM","uri":"/2018-12-22-%E8%AE%BA%E6%96%87-svim/"},{"categories":["Math"],"content":"一、定积分的概念与性质 1、定义：设函数 $f(x)$ 在 $[a,b]$ 上有界，在 $[a,b]$ 中任意插入若干分点 $a=x_0 \u003c x_1 \u003c x_2 \u003c \\cdots \u003c x_{n-1} \u003c x_n=b$，把区间 $[a,b]$ 分成 $n$ 个小区 $[x_0, x_1]$，$[x_1,x_2]$，$\\cdots$，$[x_{n-1},x_n]$，各个小区间的长度依次为 $\\Delta x_1=x_1-x_0$, $\\Delta x_2=x_2-x_1$, $\\cdots$, $\\Delta x_n=x_n-x_{n-1}$。在每个小区间 $[x_{i-1},x_i]$ 上任取一点 $\\xi_i (x_{i-1} \\leq \\xi \\leq x_i)$，作函数值 $f(\\xi)$ 与小区间长度 $\\Delta x_i$ 的乘积 $f(\\xi_i) \\cdot \\Delta x_i$，并作和 $$S=\\sum_{i=1}^{n}f(\\xi)\\Delta x_i$$ 记 $\\lambda=\\max { \\Delta x_1, \\Delta x_2, \\cdots, \\Delta x_n }$，如果当 $\\lambda \\to 0$ 时，$S$ 的极限总存在，且与闭区间 $[a,b]$ 的分法及点 $\\xi_i$ 的取法无关，那么称这个极限 $I$ 为函数 $f(x)$ 在区间 $[a,b]$ 上的定积分，记作 $\\int_{a}^{b} f(x) dx$，即 $$\\int_{a}^{b} f(x) dx=I=\\lim_{\\lambda \\to 0} \\sum_{i=1}^{n} f(\\xi_i) \\Delta x_i$$ 其中 $f(x)$ 叫做被积函数，$f(x)dx$ 叫做被积表达式，$x$ 叫做积分变量，$a$ 叫做积分下限，$b$ 叫做积分上限，$[a,b]$ 叫做积分区间 2、定理：设 $f(x)$ 在区间 $[a,b]$ 上连续，则 $f(x)$ 在 $[a,b]$ 上可积 3、定理：设 $f(x)$ 在区间 $[a,b]$ 上有界，且只有有限个间断点，则 $f(x)$ 在 $[a,b]$ 上可积 4、性质 (1) 设 $\\alpha$ 与 $\\beta$ 均为常数，则 $$\\int_{a}^{b}[\\alpha f(x) + \\beta g(x)]dx=\\alpha \\int_{a}^{b} f(x)dx+ \\beta \\int_{a}^{b} g(x) dx$$ (2) 设 $a \u003c c \u003c b$，则 $$\\int_{a}^{b} f(x) dx=\\int_{a}^{c}f(x)dx+\\int_{c}^{b}f(x)dx$$ (3) 如果在区间 $[a,b]$ 上 $f(x) \\equiv 1$，那么 $$\\int_{a}^{b} 1 dx=\\int_{a}^{b}dx=b-a$$ (4) 如果在区间 $[a,b]$ 上 $f(x) \\geq 0$，那么 $$\\int_{a}^{b} f(x) dx \\geq 0$$ (5) 设 $M$ 及 $m$ 分别是函数 $f(x)$ 在区间 $[a,b]$ 上的最大值及最小值，则 $$m(b-a) \\leq \\int_{a}^{b} f(x) dx \\leq M(b-a)$$ (6) 定积分中值定理：如果函数 f(x) 在积分区间 $[a,b]$ 上连续，那么在 $[a,b]$ 上至少存在一个点 $\\xi$，使用下式成立： $$\\int_{a}^{b} f(x)dx=f(\\xi)(b-a)$$ ","date":"2018-12-18","objectID":"/2018-12-18-%E6%95%B0%E5%AD%A6-%E5%AE%9A%E7%A7%AF%E5%88%86/:0:1","tags":null,"title":"定积分","uri":"/2018-12-18-%E6%95%B0%E5%AD%A6-%E5%AE%9A%E7%A7%AF%E5%88%86/"},{"categories":["Math"],"content":"二、微积分基本公式 1、定理：如果函数 $f(x)$ 在区间 $[a,b]$ 上连续，那么积分上限的函数 $$\\Phi(x)=\\int_{a}^{x}f(t)dt$$ 在 $[a,b]$ 上可导，并且它的导数 $$\\Phi'(x)=\\frac{d}{dx} \\int_{a}^{b} f(t)dt=f(x)$$ 2、定理：如果函数 $f(x)$ 在区间 $[a,b]$ 上连续，那么函数 $$\\Phi(x)=\\int_{a}^{x} f(t)dt$$ 就是 $f(x)$ 在 $[a,b]$ 上的一个原函数 3、定理（牛顿莱布尼茨公式、微积分基本定理）：如果函数 $F(x)$ 是连续函数 $f(x)$ 在区间 $[a,b]$ 上的一个原函数，那么 $$\\int_{a}^{b} f(x) dx=F(b)-F(a)=[F(x)]_{a}^{b}$$ 这个公式表明：一个连续函数在区间 $[a,b]$ 上的定积分等于它的任一个原函数在区间 $[a,b]$ 上的增量 ","date":"2018-12-18","objectID":"/2018-12-18-%E6%95%B0%E5%AD%A6-%E5%AE%9A%E7%A7%AF%E5%88%86/:0:2","tags":null,"title":"定积分","uri":"/2018-12-18-%E6%95%B0%E5%AD%A6-%E5%AE%9A%E7%A7%AF%E5%88%86/"},{"categories":["Math"],"content":"三、定积分的换元法和分部积分法 1、定理（换元公式）：假设函数 $f(x)$ 在区间 $[a,b]$ 上连续，函数 $x=\\varphi(t)$ 满足条件： (1) $\\varphi(\\alpha)=a, \\varphi(\\beta)=b$ (2) $\\varphi(t)$ 在 $[\\alpha,\\beta]$（ 或 $[\\beta,\\alpha]$ ） 上具有连续导数，且其值域 $R_{\\varphi}=[a,b]$，则有 $$\\int_{a}^{b}f(x)dx=\\int_{\\alpha}^{beta}f[\\varphi(t)] \\cdot \\varphi ‘(t) dt$$ 应用换元公式时有两点值得注意：(1) 用 $x=\\varphi(x)$ 把原来变量 $x$ 代换成新变量 $t$ 时，积分限也要换成相应于新变量 $t$ 的积分限；(2) 求出 $f[\\varphi(t)] \\cdot \\varphi’(t)$ 的一个原函数 $\\Phi(t)$ 后，不必像计算不定积分那样再要把 $\\Phi(t)$ 变换成原来变量 $x$ 的函数，而只要把新变量 $t$ 的上、下限分别代入 $\\Phi(t)$ 中然后相减即可 换元公式也可以反过来使用。为使用方便起见，把换元公式中左右两边对调位置，同时把 $t$ 改记为 $x$，而 $x$ 改记为 $t$， 得 $$\\int_{a}^{b}f[\\varphi(x)] \\cdot \\varphi'(x) dx=\\int_{\\alpha}^{\\beta} f(t) dt$$ 其中 $t=\\varphi(x)$，$\\alpha=\\varphi(a)$，$\\beta=\\varphi(b)$ 2、分部积分： $$\\int_{a}^{b} uv’dx=[uv]_{a}^{b}-\\int_{a}^{b}vu’dx$$ 简记为 $$\\int_{a}^{b} udv=[uv]_{a}^{b}-\\int_{a}^{b}vdu$$ ","date":"2018-12-18","objectID":"/2018-12-18-%E6%95%B0%E5%AD%A6-%E5%AE%9A%E7%A7%AF%E5%88%86/:0:3","tags":null,"title":"定积分","uri":"/2018-12-18-%E6%95%B0%E5%AD%A6-%E5%AE%9A%E7%A7%AF%E5%88%86/"},{"categories":["Math"],"content":"四、反常积分 ","date":"2018-12-18","objectID":"/2018-12-18-%E6%95%B0%E5%AD%A6-%E5%AE%9A%E7%A7%AF%E5%88%86/:0:4","tags":null,"title":"定积分","uri":"/2018-12-18-%E6%95%B0%E5%AD%A6-%E5%AE%9A%E7%A7%AF%E5%88%86/"},{"categories":["Thesis"],"content":"一、背景及问题 目前一些云端的数据处理服务经常受到可能来自数据库管理员、服务器管理员、利用操作系统漏洞的黑客等的攻击。虽然一般意义上的数据加密可以为静态数据和数据传输提供强力高效的保护，但是却无法为数据处理系统提供足够的保护，因为数据处理系统在处理查询的时会在内存解密敏感数据。而内存是容易被攻击者入侵并掌控的区域，因此这会导致数据的不安全。 一种实现安全查询（secure query）的方法是，一些系统如 CryptDB，Monomi 以及 Seabed 等通过使用 property-preserving encryption 技术来进行在已加密数据上的安全查询。然而这种方法只能处理十分有限的查询请求，并且有信息泄漏的风险。 另一种实现安全查询的方法是在受信任的执行环境（trusted execution environment）或者内飞地（enclaves） 中执行查询。系统的内飞地如 Intel Software Guard Extensions (SGX) 可以保护敏感数据和代码，即使攻击者控制了整个操作系统或者主机。虽然如此，使用内飞地存在几个问题： 为了实现安全保护，需要仔细地对应用程序进行重构，分解出可信任部分和不可信任部分 为了实现高层次的安全特性如机密性（confidentiality）、完整性（integrity）、时新性（freshness），需要增加额外的逻辑和机制来确保机密信息在进入和离开内飞地的时候不会被泄漏 这些问题对于一些简单的应用程序来说并不困难，但是对于一些大型复杂系统如数据库系统来说则非常不容易。先前的一些研究例如 CipherBase、TrustedDB 等采取的方式是将查询引擎（query engine）的一小部分放置于受信任的硬件之上，这种方式的问题是无法提供上述的机密性、完整性、时新性。另一种选择是将整个数据库放置于内飞地之中，但这显然会导致大量的 trusted computing base (TCB) 以及服务性能的下降，并且无法阻止来自数据库管理员的攻击。 【根据维基百科，飞地指在某个地理区划境内有一块隶属于他地的区域。根据地区与国家之间的相对关系，飞地又可以分为“外飞地”（Exclave）与“内飞地”（Enclave）两种概念，其关系如下：内飞地（enclave）：意指某个国家境内有块土地，其主权属于另外一个国家，则该地区称为此国家的内飞地。外飞地（exclave）：某国家拥有一块与本国分离开来的领土，该领土被其他国家包围，则该领土称为某国的外飞地。】 二、解决方案 文章提出了一个称为 EnclaveDB 的数据库，它可以为查询和数据提供机密性、完整性、时新性。它与传统关系型数据库一样提供了数据处理、SQL查询、存储过程等基本功能。与传统数据库不同的是： EnclaveDB 通过把所有敏感数据（tables, indexes, queries and other intermediate state）维护在内飞地的内存中来保护数据库状态 EnclaveDB 使用可信任的 EnclaveDB 客户端来预编译查询代码，并进行加密 预编译代码后，受信任的 EnclaveDB 客户端与不受信任的服务器上的内飞地直接建立一个安全的加密通道，然后将已加密的查询和参数发送过去 不受信任的服务器上的内飞地收到查询请求后，对请求进行验证、解密、执行查询、对查询结果进行加密、返回给客户端 EnclaveDB 重新设计了数据库的日志与恢复机制，以确保服务器无法对数据库的日志进行攻击 EnclaveDB 采取了一些优化手段，减少线程上下文切换，减少服务器开销 三、总结 文章提出了一个通过使用内飞地来实现不信任服务器上的安全数据库。通过可信任客户端的预编译、加密安全通道的通信、内飞地的隔离保护、重新设计数据库的日志与恢复机制等一系列措施，实现了机密性、完整性、时新性这三个重要的安全特性。 ","date":"2018-12-15","objectID":"/2018-12-15-%E8%AE%BA%E6%96%87-enclavedb/:0:0","tags":null,"title":"EnclaveDB","uri":"/2018-12-15-%E8%AE%BA%E6%96%87-enclavedb/"},{"categories":["Math"],"content":"一、不定积分的概念与性质 1、定义：如果在区间 $I$ 上，可导函数 $F(x)$ 的导函数为 $f(x)$，即对任一 $x \\in I$，都有 $F'(x)=f(x)$ 或 $dF(x)=f(x)dx$，那么函数 $F(x)$ 就称为 $f(x)$（或 $f(x)dx$）在区间 $I$ 上的一个原函数 2、定理：如果函数 $f(x)$ 在区间 $I$ 上连续，那么在区间 $I$ 上存在可导函数 $F(x)$，使得对任一 $x \\in I$ 都有 $F'(x)=f(x)$，也就是说连续函数一定有原函数 3、定义：在区间 $I$ 上，函数 $f(x)$ 的带有任意常数项的原函数称为 $f(x)$ （或 $f(x)dx$）在区间 $I$ 上的不定积分，记作 $\\int f(x)dx$。其中记号 $\\int$ 称为积分号，$f(x)$ 称为被积函数，$f(x)dx$ 称为被积表达式，$x$ 称为积分变量 4、由于 $\\int f(x)dx$ 是 $f(x)$ 的原函数，所以 $\\frac{d}{dx}[\\int f(x)dx]=f(x)$ 或 $d[\\int f(x)dx]=f(x)dx$；又由于 $F(x)$ 是 $F'(x)$ 的原函数，所以 $\\int F'(x) dx=F(x)+C$ 或 $\\int dF(x)=F(x)+C$。由此可见，微分运算（以记号 $d$ 表示）与积分运算（以记号 $\\int$ 表示）是互逆的。当此两个记号连在一起时，或者抵消，或者抵消后差一个常数。 $$\\int F'(x) dx = \\int dF(x)$$ 5、既然积分运算是微分运算的逆运算，那么很自然地可以从导数公式得到相应的积分公式。基本积分表： $\\int k dx=kx+C$ ($k$ 是常数) $\\int x^\\mu dx=\\frac{x^{\\mu+1}}{\\mu+1}+C$ ($\\mu \\neq -1$) $\\int \\frac{1}{x} dx=\\ln|x|+C$ $\\int \\frac{1}{1+x^2} dx=\\arctan x +C$ $\\int \\frac{1}{\\sqrt{1-x^2}} dx=\\arcsin x+C$ $\\int \\cos x dx=\\sin x+C$ $\\int \\sin x dx=- \\cos x +C$ $\\int \\frac{1}{\\cos^{2} x} dx=\\int \\sec^2 x dx=\\tan x+C$ $\\int \\frac{1}{\\sin^{2} x} dx=\\int \\csc^2 x dx=- \\cot x+C$ $\\int \\sec x \\tan x dx=\\sec x+C$ $\\int \\csc x \\cot x dx=- \\csc x+C$ $\\int e^x dx=e^x+C$ $\\int a^x dx=\\frac{a^x}{\\ln a}+C$ 6、性质1：设函数 $f(x)$ 及 $g(x)$ 的原函数存在，则 $$\\int [f(x)+g(x)]dx=\\int f(x)dx + \\int g(x)dx$$ 7、性质2：设函数 $f(x)$ 的原函数存在，$k$ 为非零常数，则 $$\\int k \\cdot f(x)dx = k \\cdot \\int f(x)dx$$ ","date":"2018-12-14","objectID":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/:0:1","tags":null,"title":"不定积分","uri":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/"},{"categories":["Math"],"content":"二、换元积分法 1、定理1：设 $f(x)$ 具有原函数，$u=\\varphi(x)$ 可导，则有换元公式 $$\\int f[\\varphi(x)] \\cdot \\varphi'(x) dx=[\\int f(u) du]_{u=\\varphi(x)}$$ 2、定理2：设 $x=\\psi(t)$ 是单调的可导函数，并且 $\\psi(t) \\neq 0$。又设 $f[\\psi(t)] \\cdot \\psi(t)$ 具有原函数。则有换元公式 $$\\int f(x) dx=[\\int f[\\psi(t)] \\cdot \\psi'(t) dt]_{t=\\psi^{-1}(x)}$$ 其中 $\\psi^{-1}(x)$ 是 $x=\\psi(t)$ 的反函数 3、基本积分表续（以下 $a \u003e 0$） $\\int \\tan x dx=-\\ln |\\cos x|+C$ $\\int \\cot x dx=\\ln |\\sin x|+C$ $\\int \\sec x dx=\\ln |\\sec x+\\tan x|+C$ $\\int \\csc x dx=\\ln |\\csc x-\\cot x|+C$ $\\int \\frac{1}{a^2+x^2} dx=\\frac{1}{a} \\arctan \\frac{x}{a}+C$ $\\int \\frac{1}{x^2-a^2} dx=\\frac{1}{2a} \\ln |\\frac{x-a}{x+a}|+C$ $\\int \\frac{1}{\\sqrt{a^2-x^2}} dx=\\arcsin \\frac{x}{a}+C$ $\\int \\frac{1}{\\sqrt{a^2+x^2}} dx=\\ln(x+\\sqrt{a^2+x^2})+C$ $\\int \\frac{1}{\\sqrt{x^2-a^2}} dx=\\ln |x+\\sqrt{x^2-a^2}|+C$ ","date":"2018-12-14","objectID":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/:0:2","tags":null,"title":"不定积分","uri":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/"},{"categories":["Math"],"content":"三、分部积分法 设函数 $u=u(x)$ 及 $v=v(x)$ 具有连续导数，则两个函数乘积的导数公式为 $$(uv)'=u’v+uv'$$ 移项，得 $$uv'=(uv)'-u’v$$ 两边同时求不定几积分，得 $$\\int uv' dx=uv-\\int u’v dx$$ 也可以写成 $$\\int udv=uv-\\int vdu$$ ","date":"2018-12-14","objectID":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/:0:3","tags":null,"title":"不定积分","uri":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/"},{"categories":["Math"],"content":"四、有理函数的积分 1、定义：两个多项式子的商 $\\frac{P(x)}{Q(x)}$ 称为有理函数，又称有理分式（假定分子多项式与分母之间没有公因式）。当分子多项式 $P(x)$ 的次数小于分母多项式 $Q(x)$ 的次数时，称这有理函数为真分式，否则为假分式。 2、对于假分式，利用多项式除法，总可以将一个假分式化成一个多项式与一个真分式之和的形式 3、对于真分式 $\\frac{P(x)}{Q(x)}$，如果分母可分解为两个多项式的乘积 $Q(x)=Q_1(x)Q_2(x)$ 且 $Q_1(x)$ 与 $Q_2(x)$ 没有公因式，那么它可拆分为两个真分式之和 $$\\frac{P(x)}{Q(x)}=\\frac{P_1(x)}{Q_1(x)}+\\frac{P_2(x)}{Q_2(x)}$$ 上述步骤称为把真分式化成部分分式之和。如果 $Q_1(x)$ 或 $Q_2(x)$ 还能分解成两个没有公因式的多项式的乘积，那么如此继续拆分。最后，有理函数的分解式中只出现多项式、$\\frac{P_1(x)}{(x-a)^k}$、$\\frac{P_2(x)}{(x^2+px+q)^l}$ 等三类函数，在进行求解。 ","date":"2018-12-14","objectID":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/:0:4","tags":null,"title":"不定积分","uri":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/"},{"categories":["Math"],"content":"五、积分表的使用 ","date":"2018-12-14","objectID":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/:0:5","tags":null,"title":"不定积分","uri":"/2018-12-14-%E6%95%B0%E5%AD%A6-%E4%B8%8D%E5%AE%9A%E7%A7%AF%E5%88%86/"},{"categories":["Math"],"content":"一、微分中值定理 1、费马引理：设函数 $f(x)$ 在点 $x_0$ 的某邻域 $U(x_0)$ 内有定义，并且在 $x_0$ 处可导，如果对任意的 $x \\in U(x_0)$ 有 $f(x) \\leq f(x_0)$ （或 $f(x) \\geq f(x_0)$），那么 $f'(x_0)=0$ 2、罗尔定理：如果函数 $f(x)$ 满足：(1) 在闭区间 $[a,b]$ 上连续；(2) 在开区间 $(a,b)$ 内可导；(3) 在区间端点处的函数值相等，即 $f(a)=f(b)$，那么在 $(a,b)$ 内至少有一点 $\\xi (a \u003c \\xi \u003c b)$，使得 $f'(\\xi)=0$ 3、拉格朗日中值定理：如果函数 $f(x)$ 满足：(1) 在闭区间 $[a,b]$ 上连续；(2) 在开区间 $(a,b)$ 内可导，那么在 $(a,b)$ 内至少有一点 $\\xi (a \u003c \\xi \u003c b)$，使得 $f(b)-f(a)=f'(\\xi) \\cdot (b-a)$。其几何意义是，如果连续曲线 $y=f(x)$ 的弧 $\\widetilde{AB}$ 上除端点外处处具有不垂直于 $x$ 轴的切线，那么这弧上至少有一点 $C$，使得曲线在 $C$ 处的切线平行于弦 $AB$。该定理也称为微分中值定理 4、柯西中值定理：如果函数 $f(x)$ 及 $F(x)$ 满足：(1) 在闭区间 $[a,b]$ 上连续；(2) 在开区间 $(a,b)$ 内可导；(3) 对任一 $x \\in (a,b)$，$F'(x) \\neq 0$，那么在 $(a,b)$ 内至少有一点 $\\xi (a \u003c \\xi \u003c b)$，使得 $\\frac{f(b)-f(a)}{F(b)-F(a)}=\\frac{f'(\\xi)}{F'(\\xi)}$。该定理是参数方程形式下的拉格朗日中值定理的更一般性的表达形式，当取 $F(x)=x$ 时便得到拉格朗日中值定理 ","date":"2018-12-11","objectID":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/:0:1","tags":null,"title":"微分中值定理与导数的应用","uri":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/"},{"categories":["Math"],"content":"二、洛必达法则 1、定理：设 (1) 当 $x \\to a$ 时，函数 $f(x)$ 及 $F(x)$ 都趋向于零；(2) 在点 $a$ 的某去心邻域内 $f'(x)$ 及 $F'(x)$ 都存在且 $F'(x) \\neq 0$；(3) $\\lim_{x \\to a} \\frac{f'(x)}{F'(x)}$ 存在（或为无限大），则 $$\\lim_{x \\to a} \\frac{f(x)}{F(x)}=\\lim_{x \\to a} \\frac{f'(x)}{F'(x)}$$ 2、定理：设 (1) 当 $x \\to \\infty $ 时，函数 $f(x)$ 及 $F(x)$ 都趋于零；(2) 当 $|x| \u003e N$ 时，$f'(x)$ 与 $F'(x)$ 都存在且 $F'(x) \\neq 0$；(3) $\\lim_{x \\to \\infty} \\frac{f'(x)}{F'(x)}$ 存在（或为无穷大），则 $$\\lim_{x \\to \\infty} \\frac{f(x)}{F(x)}=\\lim_{x \\to \\infty} \\frac{f'(x)}{F'(x)}$$ ","date":"2018-12-11","objectID":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/:0:2","tags":null,"title":"微分中值定理与导数的应用","uri":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/"},{"categories":["Math"],"content":"三、泰勒公式 1、泰勒中值定理1：如果函数 $f(x)$ 在 $x_0$ 处具有 $n$ 阶导数，那么存在 $x_0$ 的一个邻域，对于该邻域的任一 $x$，有 $$f(x)=f(x_0)+f'(x_0)(x-x_0)+\\frac{f''(x_0)}{2!}(x-x_0)^2+…+\\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)$$ 其中 $R_n(x)=o((x-x_0)^n)$ 2、泰勒中值定理2：如果函数 $f(x)$ 在 $x_0$ 处具有 $n+1$ 阶导数，那么对于任一 $x \\in U(x_0)$，有 $$f(x)=f(x_0)+f'(x_0)(x-x_0)+\\frac{f''(x_0)}{2!}(x-x_0)^2+…+\\frac{f^{(n)}(x_0)}{n!}(x-x_0)^n+R_n(x)$$ 其中 $R_n(x)=\\frac{f^{(n+1)}(\\xi)}{(n+1)!}(x-x_0)^{n+1}$，这里 $\\xi$ 是 $x_0$ 与 $x$ 之间的某个值 ","date":"2018-12-11","objectID":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/:0:3","tags":null,"title":"微分中值定理与导数的应用","uri":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/"},{"categories":["Math"],"content":"四、函数的单调性与曲线的凹凸性 1、定理：设函数 $y=f(x)$ 在 $[a,b]$ 上连续，在 $(a,b)$ 内可导 (1) 如果在 $(a,b)$ 内 $f'(x) \\geq 0$，且等号仅在有限多个点处成立，那么函数 $y=f(x)$ 在 $[a,b]$ 上单调增加 (2) 如果在 $(a,b)$ 内 $f'(x) \\leq 0$，且等号仅在有限多个点处成立，那么函数 $y=f(x)$ 在 $[a,b]$ 上单调减少 2、定义：设 $f(x)$ 在区间 $I$ 上连续，如果对 $I$ 上任意两点 $x_1$，$x_2$，恒有 $f(\\frac{x_1+x_2}{2}) \u003c \\frac{f(x_1)+f(x_2)}{2}$，那么称 $f(x)$ 在 $I$ 上的图形是凹的；如果恒有 $f(\\frac{x_1+x_2}{2}) \u003e \\frac{f(x_1)+f(x_2)}{2}$，那么称图形是凸的 3、定理：设 $f(x)$ 在 $[a,b]$ 上连续，在 $(a,b)$ 内具有一阶和二阶导数，那么 (1) 若在 $(a,b)$ 内 $f''(x) \u003e 0$，则 $f(x)$ 在 $[a,b]$ 上的图形是凹的 (2) 若在 $(a,b)$ 内 $f''(x) \u003c 0$，则 $f(x)$ 在 $[a,b]$ 上的图形是凸的 ","date":"2018-12-11","objectID":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/:0:4","tags":null,"title":"微分中值定理与导数的应用","uri":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/"},{"categories":["Math"],"content":"五、函数的极值与最大值最小值 1、定义：设函数 $f(x)$ 在点 $x_0$ 的某个去心邻域 $U(x_0)$ 内有定义，如果对于该去心邻域内的任一 $x$，有 $f(x) \u003c f(x_0)$，那么就称 $f(x_0)$ 是函数 $f(x)$ 的一个极大值；如果 $f(x) \u003e f(x_0)$，那么称为极小值 2、定理：设函数 $f(x)$ 在 $x_0$ 处可导，且在 $x_0$ 处取得极值，那么 $f'(x_0)=0$ 3、定理：设函数 $f(x)$ 在 $x_0$ 处连续，且在 $x_0$ 的某去心邻域 $U(x_0,\\delta)$ 内可导. (1) 若 $x \\in (x_0-\\delta, x_0)$ 时 $f'(x) \u003e 0$，而 $x \\in (x0, x_0+\\delta)$ 时 $f'(x) \u003c 0$，则 $f(x)$ 在 $x_0$ 处取得极大值 (2) 若 $x \\in (x_0-\\delta, x_0)$ 时 $f'(x) \u003c 0$，而 $x \\in (x0, x_0+\\delta)$ 时 $f'(x) \u003e 0$，则 $f(x)$ 在 $x_0$ 处取得极小值 4、定理：设函数 $f(x)$ 在 $x_0$ 处具有二阶导数且 $f(x_0)=0$，$f''(x_0) \\neq 0$，则 (1) 当 $f''(x_0) \u003c 0$ 时，函数 $f(x)$ 在 $x_0$ 处取得极大值 (2) 当 $f''(x_0) \u003e 0$ 时，函数 $f(x)$ 在 $x_0$ 处取得极小值 ","date":"2018-12-11","objectID":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/:0:5","tags":null,"title":"微分中值定理与导数的应用","uri":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/"},{"categories":["Math"],"content":"六、曲率 ","date":"2018-12-11","objectID":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/:0:6","tags":null,"title":"微分中值定理与导数的应用","uri":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/"},{"categories":["Math"],"content":"七、方程的近似解 ","date":"2018-12-11","objectID":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/:0:7","tags":null,"title":"微分中值定理与导数的应用","uri":"/2018-12-11-%E6%95%B0%E5%AD%A6-%E5%BE%AE%E5%88%86%E4%B8%AD%E5%80%BC%E5%AE%9A%E7%90%86%E4%B8%8E%E5%AF%BC%E6%95%B0%E7%9A%84%E5%BA%94%E7%94%A8/"},{"categories":["Thesis"],"content":"一、背景及问题 目前一些高级的网络攻击例如 APT (Advanced Persistent Threat) 引起了工业界和学术界的广泛关注和研究。这种攻击的特点是隐性、长期、多步骤。当一次攻击被发现后，鉴定分析 (Forensic Analysis) 系统就会开始被执行以确定攻击的入口以及攻击的影响范围。鉴定分析是基于系统日志进行的，它通过分析日志文件（Log）中记录的所有系统操作（例如网络事件、文件读写事件、进程间通信事件）之间的信息流，来追踪攻击源头，以及被攻击所污染的所有实体。由此可知，如果要使的分析结果越精确，那么日志文件则需要越详细地记录系统发生的一切事件，如此一来日志文件就不得不变得更大。再考虑到 APT 的一次攻击可能持续几个月的时间，以及大型企业一般都会有成千上万台主机（Host），在这种情况下日志文件可以轻松达到 PB 级别，不但需要占用巨大的存储空间，而且还会拖慢鉴定分析的速度。 此问题吸引了众多研究团体的兴趣，他们主要关注于如何在保证不影响鉴定分析的精确率的前提下，减小日志体积，提高鉴定分析的效率。 二、解决方案 文章提出了一种基于依赖图（Dependence Graph）的日志压缩技术，在不影响鉴定分析准确性的前提下，可以大幅度减少日志记录。大体思路如下： 系统日志记录的系统事件可以用依赖图表示。系统内的实体（如进程、文件、网络等）用结点表示，系统事件（如系统调用，读写文件、进程间消息通讯、网络数据传输等）用带时间戳的有向边表示，其指向方向与信息/数据流动方向一致 只针对 read、write、load 三类系统事件进行压缩，其他系统事件如 fork、execve、remove、rename、chmod 等等不考虑压缩直接保留。因为根据经验统计，系统中超过 95% 的事件是读写事件，只要这部分的日志减下来即可。并且只对读写事件进行压缩可以降低算法模型的复杂度，提高可读性 根据前向可达性（Forward Reachability）和后向可达性（Backward Reachability），定义两种子图保留方案：全依赖 (Full Dependence FD) 保留，资源依赖（Source Dependence，SD）保留 由于依赖图的边（即系统事件）是带时间戳的，当执行可达性计算时，需要对一个节点按时间戳的先后顺序多次计算，而不能像标准的图算法那样对所有结点一视同仁地一次性计算并进行中间结果缓存。这样显然会提高算法的复杂度，因此文章提出了一个算法，将带时间戳的依赖图转化标准图（边不带时间戳），具体方法是对结点进行多版本化（Versioning） 但是如果只简单地进行结点的多版本化，又会增大图的复杂度，增大日志的体积，因此在多版本化后，文章提出了几种剪枝优化算法来简化依赖图：Redundant edge optimization (REO)、Global Redundant Edge Optimization (REO*)、Redundant node optimization (RNO)、 Redundant node optimization (RNO) 通过以上步骤对依赖图进行简化之后，就可以将依赖图还原成日志文件，此时的日志文件中只保留了关键的信息，体积大大减小。但是文章认为日志文件原来的格式不够节省空间，于是提出了一种新的压缩文件格式 CSR ，可以进一步减少空间占用 三、总结 文章主要针对鉴定分析需要的日志文件过大问题，提出了一种在保证不影响侦查分析准确性的前提下，优化依赖子图、减少日志文件体积、提高鉴定分析效率的方案，其主要思路是通过前后可达性和后向可达性来保证依赖子图保留了关键的结点和边。 ","date":"2018-12-08","objectID":"/2018-12-08-%E8%AE%BA%E6%96%87-fdsd/:0:0","tags":null,"title":"FDSD","uri":"/2018-12-08-%E8%AE%BA%E6%96%87-fdsd/"},{"categories":["Math"],"content":"一、导数定义 1、函数在一点处的导数：设函数 $y=f(x)$ 在点 $x_0$ 的某个邻域内有定义，当自变量 $x$ 在 $x_0$ 处取得增量 $\\Delta x$（点 $x_0 + \\Delta x$ 仍在该邻域内）时，相应地，因变量取得增量 $\\Delta y=f(x_0+\\Delta x)-f(x_0)$。如果 $\\Delta y$ 与 $\\Delta x$ 之比当 $\\Delta x \\to 0$ 时的极限存在，那么称函数 $y=f(x)$ 在点 $x_0$ 处可导，并称这个极限为函数 $y=f(x)$ 在点 $x_0$ 处的导数，记为 $f'(x_0)$，即 $$f'(x_0)=\\lim_{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x}=\\lim_{\\Delta x \\to 0} \\frac{f(x_0+\\Delta x)-f(x_0)}{\\Delta x}$$ 2、导函数：如果函数 $y=f(x)$ 在开区间的每点处都可导，那么就称函数 $f(x)$ 在开区间 $I$ 内可导。这时，对于任一 $x \\in I$ 都对应着 $f(x)$ 的一个确定的导数值，这样就构成了一个新的函数，这个函数叫做函数 $y=f(x)$ 的导函数，记作 $y'$, 或 $f'(x)$。导函数 $f'(x)$ 简称导数，$f'(x_0)$ 是 $f(x)$ 在 $x_0$ 处的导数，或导数 $f'(x)$ 在 $x_0$ 处的值，即 $f'(x_0)=f'(x)|_{x=x_0}$ ","date":"2018-12-06","objectID":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/:0:1","tags":null,"title":"导数与微分","uri":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/"},{"categories":["Math"],"content":"二、函数的求导法则 1、定理：如果函数 $u=u(x)$ 和 $v=v(x)$ 都在点 x 具有导数，那么它们的和、差、积、商（除分母为零的点外）都在 x 点具有导数，且 $[u \\pm v]' = u' \\pm v'$ $[uv]' = u’v+uv'$ $[\\frac{u}{v}]'=\\frac{u’v-uv'}{v^2}$ ($v(x)\\neq 0$) 2、定理：如果函数 $x=f(y)$ 在区间 $I_y$ 内单调、可导且 $f'(y)\\neq 0$，那么它的反函数 $y=f^{-1}$ 在区间 $I_{x}={ x|x=f(y),y\\in I_y }$ 内也可导，且其导数等于直接函数的导数的倒数，即 $$[f^{-1}(x)]'=\\frac{1}{f'(y)}$$ 3、定理：如果 $u=g(x)$ 在点 $x$ 可导，而 $y=f(u)$ 在点 $u=g(x)$ 可导，那么复合函数 $y=f[g(x)]$ 在点 $x$ 可导，且其导数为 $$y'=f'(u)\\cdot g'(x)$$ 4、基本初等函数求导公式 $(C)'=0$ $(x^{\\mu})'=\\mu x^{\\mu -1}$ $(a^x)'=a^x \\ln a$ ($a\u003e0, a \\neq 1$) $(e^x)'=e^x$ $(\\log_a x)'=\\frac{1}{x \\ln a}$ ($a\u003e0, a \\neq 1$) $(\\ln x)'=\\frac{1}{x}$ $(\\sin x)'=\\cos x$ $(\\cos x)'=- \\sin x$ $(\\tan x)'=\\sec^{2} x$ $(\\cot x)'=- \\csc^{2} x$ $(\\sec x)'=\\sec x \\cdot \\tan x$ $(\\csc x)'=- \\csc x \\cdot \\cot x$ ","date":"2018-12-06","objectID":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/:0:2","tags":null,"title":"导数与微分","uri":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/"},{"categories":["Math"],"content":"三、高阶导数 ","date":"2018-12-06","objectID":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/:0:3","tags":null,"title":"导数与微分","uri":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/"},{"categories":["Math"],"content":"四、隐函数的导数、由参数方程所确定的函数的导数、相关变化率 ","date":"2018-12-06","objectID":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/:0:4","tags":null,"title":"导数与微分","uri":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/"},{"categories":["Math"],"content":"五、函数的微分 1、定义：设函数 $y=f(x)$ 在某区间内有定义，$x_0$ 及 $x_0+\\Delta x$ 在这区间内，如果函数的增量 $\\Delta y=f(x_0+\\Delta x)-f(x_0)$ 可表示为 $\\Delta y=C \\cdot \\Delta x+ o(\\Delta x)$，其中 $C$ 是不依赖于 $\\Delta x$ 的常数，那么称函数 $y=f(x)$ 在点 $x_0$ 处是可微的，而 $C \\cdot \\Delta x$ 叫做函数的微分，记作 $dy$，即 $$dy=C \\cdot \\Delta x$$ 2、可微的条件：函数 $f(x)$ 在点 $x_0$ 处可微的充分必要条件是函数 $f(x)$ 在点 $x_0$ 处可导，并且其导数 $f'(x_0)=C$。因为由导数的定义 （$\\frac{\\Delta y}{\\Delta x}$ 在 $x_0$ 处的极限存在，记为 $f'(x_0)$），以及极限与无穷小的关系（在自变量的同一变化过程 $x \\to x_0$ 中，函数 $g(x)$ 具有极限 $A$ 的充分必要条件是 $g(x)=A+\\alpha$，其中 $\\alpha$ 是无穷小），可得 $\\frac{\\Delta y}{\\Delta x}=f'(x_0)+\\alpha$，因此 $$\\Delta y=f'(x_0) \\cdot \\Delta x+\\alpha \\cdot \\Delta x=dy+o(\\Delta x)$$ 3、自变量的微分：通常把自变量 $x$ 的增量 $\\Delta x$ 称为自变量的微分，记作 $dx$，即 $$dx=\\Delta x$$ 于是，函数 $y=f(x)$ 的微分 $dy=f'(x) \\cdot \\Delta x$ 又可以记作 $dy=f'(x) \\cdot dx$。从而有 $\\frac{dy}{dx}=f'(x)$，也就是说，函数的微分 $dy$ 与自变量的微分 $dx$ 之商，等于该函数的导数。因此，导数也可叫做“微商”。 $$\\frac{dy}{dx}=\\lim_{\\Delta x \\to 0}\\frac{\\Delta y}{\\Delta x}$$ 那么在 $x_0$ 处的导数定义为 $$f'(x_0) = \\frac{dy}{dx} |{x=x_0}=\\lim{\\Delta x \\to 0} \\frac{\\Delta y}{\\Delta x} |{x=x_0} =\\lim{\\Delta x \\to 0} \\frac{f(x_0+\\Delta x)-f(x_0)}{\\Delta x}$$ 4、微分的几何意义：对于可微函数 $y=f(x)$ 而言，当 $\\Delta y$ 是曲线 $y=f(x)$ 上的点的纵坐标的增量时，$dy$ 就是曲线的切线上的点的纵坐标的相应增量。当 $|\\Delta x|$ 很小时，$|\\Delta y-dy|$ 比 $|\\Delta x|$ 小得多。因此在某点的邻近，我们可以用切线段来近似代替曲线段。（在局部范围内用线性函数近似代替非线性函数，在几何上就是局部用切线段近似代替曲线段，这在数学上称为非线性函数的局部线性化，这是微分学的基本思想方法之一） 5、微分的运算法则：从函数的微分的表达式 $dy=f'(x)dx$ 可以看出，要计算函数的微分，只要计算函数的导数，再乘以自变量的微分即可。 ","date":"2018-12-06","objectID":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/:0:5","tags":null,"title":"导数与微分","uri":"/2018-12-06-%E6%95%B0%E5%AD%A6-%E5%AF%BC%E6%95%B0%E4%B8%8E%E5%BE%AE%E5%88%86/"},{"categories":["Math"],"content":"一、映射与函数 1、映射：设 $X$、$Y$ 是两个非空集合，如果存在一个法则 $f$，使得对 $X$ 中每个元素 $x$，按法则 $f$，在 $Y$ 中有唯一确定的元素 $y$ 与之对应，那么称 $f$ 为从 $X$ 到 $Y$ 的映射，记作 $f:X \\to Y$，其中 $y$ 称为元素 $x$ 在映射 $f$ 下的像，记作 $y=f(x)$，而 $x$ 称为元素 $y$ 在映射 $f$ 下的一个原像。集合 $X$ 称为映射 $f$ 的定义域，$X$ 中所有元素的像组成的集合称为映射 $f$ 的值域。 2、函数：设数集 $D \\subset R$，$R$ 为实数集，则称映射 $f:D \\to R$ 为定义在 $D$ 上的函数，记作 $y=f(x), x \\in D$，其中 $x$ 称为自变量，$y$ 称为因变量，$D$ 称为定义域。 五类基本初等函数： 幂函数：$y=x^{\\mu}$ ( $\\mu \\in R$ 是常数 ) 指数函数：$y=a^{x}$ ( $a\u003e0$ 且 $a \\neq 1$ ) 对数函数：$y=\\log_ax$ ( $a\u003e0$ 且 $a \\neq 1$ ) 三角函数：$y=\\sin x$，$y=\\cos x$，$y=\\tan x$ 等等 反三角函数：$y=\\arcsin x$，$y=\\arccos x$，$y=\\arctan x$ 等等 ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:1","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Math"],"content":"二、数列的极限 1、定义：设 {$x_n$} 为一数列，如果存在常数 $a$，对于任意给定的正数 $\\varepsilon$（不论它多么小），总存在正整数 $N$，使得当 $n\u003eN$ 时，不等式 $|x_n - a| \u003c \\varepsilon$ 都成立，那么称常数 $a$ 是数列 $x_n$ 的极限，或者称数列 $x_n$ 收敛于 $a$，记为 $\\lim_{n \\to \\infty} x_n=a$。上述定义可简记为： $$\\lim_{n \\to \\infty} x_n=a \\iff \\forall \\varepsilon\u003e0, \\exists N \\in N^{+}, \\text{ 当 } n\u003eN \\text{ 时 }, \\text{ 有 } |x_n-a|\u003c\\varepsilon $$ 2、定理（数列极限唯一性）：如果数列 $x_n$ 收敛，那么它的极限唯一 3、定理（收敛数列有界性）：如果数列 $x_n$ 收敛，那么数列 $x_n$ 一定有界 4、定理（收敛数列保号性）：如果 $\\lim_{n \\to \\infty}x_n=a$，且 $a\u003e0$（或 $a \u003c 0$），那么存在正整数 $N$，当 $n\u003eN$ 时，都有 $x_n\u003e0$（或 $x_n \u003c 0$） 5、定理（子数列收敛性）：如果数列 $x_n$ 收敛于 $a$，那么它的任一子数列也收敛于 $a$ ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:2","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Math"],"content":"三、函数的极限 1、定义(1)：设函数 $f(x)$ 在点 $x_0$ 的某一点去心邻域内有定义。如果存在常数 $A$，对于任意给定的正数 $\\varepsilon$（不论它多么小），总存在整数 $\\delta$，使得当 $x$ 满足不等式 $0\u003c|x-x_0|\u003c\\delta$ 时，对应的函数值 $f(x)$ 都满足不等式 $|f(x)-A|\u003c\\varepsilon$，那么常数 $A$ 就叫做函数 $f(x)$ 当 $x \\to x_0$ 时的极限，记作 $\\lim_{x \\to x_0}f(x)=A$。定义可简记为： $$\\lim_{x \\to x_0}f(x)=A \\iff \\forall \\varepsilon\u003e0, \\exists \\delta\u003e0, \\text{ 当 } 0\u003c|x-x_0|\u003c\\delta \\text{ 时 }, \\text{ 有 } |f(x)-A|\u003c\\varepsilon$$ 2、定义(2)：设函数 $f(x)$ 当 $|x|$ 大于某一正数时有定义。如果存在常数 $A$，对于任意给定的正数 $\\varepsilon$（不论它多么小），总存在着正数 $X$，使得当 $x$ 满足不等式 $|x|\u003eX$ 时，对应的函数值 $f(x)$ 都满足不等式 $|f(x)-A|\u003c\\varepsilon$，那么常数 $A$ 就叫做函数 $f(x)$ 当 $x \\to \\infty$ 时的极限，记作 $\\lim_{x \\to \\infty} f(x)=A$。定义可简记为： $$\\lim_{x \\to \\infty}f(x)=A \\iff \\forall \\varepsilon\u003e0, \\exists X\u003e0, \\text{ 当 } |x|\u003eX \\text{ 时 }, \\text{ 有 } |f(x)-A|\u003c\\varepsilon$$ 3、定理（函数极限唯一性）：如果 $\\lim_{x \\to x_0}f(x)$ 存在，那么这极限唯一 4、定理（函数极限局部有界性）：如果 $\\lim_{x \\to x_0}f(x)=A$，那么存在常数 $M\u003e0$ 和 $\\delta \u003e0$，使得当 $0\u003c|x-x_0|\u003c\\delta$ 时，有 $|f(x)| \\leq M$ 5、定理（函数极限局部保号性）：如果 $\\lim_{x \\to x_0}f(x)=A$ 且 $A\u003e0$（或 $A \u003c 0$），那么存在常数 $\\delta \u003e0$，使得当 $0\u003c|x-x_0|\u003c\\delta$ 时，有 $f(x)\u003e0$（或 $f(x)\u003c 0$） ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:3","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Math"],"content":"四、无穷小与无穷大 1、无穷小：如果函数 $f(x)$ 当 $x \\to x_0$（或 $x \\to \\infty$）时的极限为零，那么称函数 $f(x)$ 为当 $x \\to x_0$（或 $x \\to \\infty$）时的无穷小 2、无穷大：设函数 $f(x)$ 在 $x_0$ 的某一去心邻域内有定义（或 $|x|$ 大于某一正数时有定义）。如果对于任意给定的正数 $M$（不论它多大），总存在正数 $\\delta$（或正数 $X$），只要 $x$ 适合不等式 $0\u003c|x-x_0|\u003c\\delta$（或 $|x|\u003eX$），对应的函数值 $f(x)$ 总满足不等式 $|f(x)|\u003eM$，那么称函数 $f(x)$ 是当 $x \\to x_0$（或 $x \\to \\infty$）时的无穷大 3、定理：在自变量的同一变化过程 $x \\to x_0$（或 $x \\to \\infty$）中，函数 $f(x)$ 具有极限 $A$ 的充分必要条件是 $f(x)=A+\\alpha$，其中 $\\alpha$ 是无穷小 4、定理：在自变量的同一变化过程中，如果 $f(x)$ 为无穷大，那么 $\\frac{1}{f(x)}$ 为无穷小；反之，如果 $f(x)$ 为无穷小，且 $f(x) \\neq 0$，那么 $\\frac{1}{f(x)}$ 为无穷大 ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:4","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Math"],"content":"五、极限运算法则 1、定理：有限个无穷小的和是无穷小 2、定理：有界函数与无穷小的乘积是无穷小 常数与无穷小的乘积是无穷小 有限个无穷小的乘积是无穷小 3、定理：如果 $\\lim f(x)=A$，$\\lim g(x)=B$，那么 $\\lim [f(x) \\pm g(x)]=\\lim f(x) \\pm \\lim g(x) = A \\pm B$ $\\lim [f(x) \\cdot g(x)]=\\lim f(x) \\cdot \\lim g(x) = A \\cdot B$ 若 $B \\neq 0$，则 $\\lim \\frac{f(x)}{g(x)}=\\frac{\\lim f(x)}{\\lim g(x)}=\\frac{A}{B}$ $\\lim [cf(x)]=c \\lim f(x)$ $\\lim [f(x)]^n=[\\lim f(x)]^n$ 4、数列极限的四则运算法则同上 5、定理：如果 $f(x) \\geq g(x)$，而 $\\lim f(x)=A$，$\\lim g(x)=B$，那么 $A \\geq B$ 6、定理：设函数 $y=f[g(x)]$ 是由函数 $u=g(x)$ 与 $y=f(u)$ 复合而成，$f[g(x)]$ 在点 $x_0$ 的某去心邻域内有定义，若 $\\lim_{x \\to x_0}=u_0$，$lim_{u \\to u_0}=A$，且存在 $\\delta \u003e0$，当 $x \\in U(x_0,\\delta_0)$ 时，有 $g(x) \\neq u_0$，则 $lim_{x \\to x_0}f[g(x)]=\\lim_{u \\to u_0}f(u)=A$ ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:5","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Math"],"content":"六、极限存在准则 1、准则(I)-数列：如果数列 {$x_n$}，{$y_n$} 及 {$z_n$} 满足： (i) 从某项起，即 $\\exists n_0 \\in N^{+}$，当 $n\u003en_0$ 时，有 $y_n \\leq x_n \\leq z_n$ (ii) $\\lim_{n \\to \\infty} y_n=a$，$\\lim_{n \\to \\infty}z_n=a$ 那么数列 {$x_n$} 的极限存在，且 $\\lim_{n \\to \\infty}x_n=a$ 2、准则(I)-函数：如果 (i) 当 $x \\in U(x_0, r)$ （或 $|x|\u003eM$）时，$g(x) \\leq f(x) \\leq h(x)$ (ii) $\\lim_{x \\to x_0/\\infty} g(x)=A$，$\\lim_{x \\to x_0/\\infty}h(x)=A$ 那么 $\\lim_{x \\to x_0/\\infty} f(x)=A$ 3、准则(II)-数列：单调有界函数必有极限 4、准则(II)-函数：设函数 $f(x)$ 在点 $x_0$ 的某个左邻域内单调并且有界，则 $f(x)$ 在 $x_0$ 的左极限 $f(x_0^{-})$ 必定存在 5、柯西极限存在准则：数列 {$x_n$} 收敛的充分必要条件是：对于任意给定的正数 $\\varepsilon$，存在正整数 $N$，使得当 $m\u003eN$，$n\u003eN$ 时，有 $|x_n-x_m| \u003c \\varepsilon$ ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:6","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Math"],"content":"七、无穷小的比较 ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:7","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Math"],"content":"八、函数的连续性 1、定义：设函数 $y=f(x)$ 在点 $x_0$ 的某一邻域内有定义，如果 $$\\lim_{\\Delta x \\to 0} \\Delta y = \\lim_{\\Delta x \\to 0}[f(x_0+\\Delta x)-f(x_0)]=0 $$ ，那么就称函数 $y=f(x)$ 在点 $x_0$ 连续 2、定义：设函数 $y=f(x)$ 在点 $x_0$ 的某一邻域内有定义，如果 $$\\lim_{x \\to x_0} f(x)=f(x_0)$$ ，那么就称函数 $f(x)$ 在点 $x_0$ 连续 ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:8","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Math"],"content":"九、连续函数的运算与初等函数的连续性 ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:9","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Math"],"content":"十、闭区间上连续函数的性质 ","date":"2018-12-02","objectID":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/:0:10","tags":null,"title":"函数与极限","uri":"/2018-12-02-%E6%95%B0%E5%AD%A6-%E5%87%BD%E6%95%B0%E4%B8%8E%E6%9E%81%E9%99%90/"},{"categories":["Thesis"],"content":"一、背景及问题 目前一些高级的网络攻击如 APT（advanced persistent threats）威胁并损害着很多公司的信息安全。它们利用主机的各种弱点，通过一系列步骤来进行攻击。 为了应对这些攻击，一些基于系统监控（System Monitoring）的方法被提出，用于快速侦查异常，或者定位存在风险的系统事件（System Event）。这些方法主要通过对主机的系统调用（system calls）进行监控，收集系统事件的信息，来实现系统异常的侦查。然而，目前这些方法存在几点问题和困难： 时间要求严格。要避免主机受到进一步的伤害导致无法修复，该监控系统必须做到实时在线保护主机，实时侦查主机内的异常事件，这就犹如进行实时的“大海捞针” 如何让监控系统实时地结合系统管理员、安全专家、数据分析专家们的领域知识（domain knowledge）？因为这些领域知识对于系统异常的侦查是非常有帮助的，甚至是必要的 实时的系统监控自然会产生海量的日志数据，这就要求分析数据侦查异常的算法必须非常高效，因为是实时的 遗憾的是目前没有任何一个监控系统能够解决以上问题，尤其是第二点——已有的监控系统往往只专注于特定的异常侦查，无法提供足够的支持以便用户可以灵活地引入专家的领域知识。 二、解决方案 论文提出了一个基于流的异常查询系统，以及一种基于流的异常查询语言（Streambased Anomaly Query Language，SAQL）。该系统主要面向 4 种异常： Rule-based Anomaly:基于规则的异常，某个进程在短时间内读取大量的命令日志文件，这种异常通常意味着某个非法的用户想要探测合法用户的常用命令或者操作习惯。为了侦查这种异常，SAQL 使用事件模式（event patterns）来表达操作系统中的各种活动：定义一个事件表示为 {subject-operation-object}，例如 {process p1 write file f2}。有了如此统一定义的形式，用户就可以在查询的时间灵活地定义领域知识，甚至可以定制基于特定规则的异常检测 Time-Series Anomaly:时间序列的异常，某个进程在一段时间内不正常地传送了大量的数据。为了探测到这种异常，SAQL 构造带状态的滑动窗口，将数据流进行分段，那么只要通过与历史滑动窗口的状态进行比较，就可以检测到某个时间段对应的数据量是否异常地增大 Invariant-based Anomaly:基于不变的异常？一个进程启动了一个异常的进程，如 apache.exe 本应该启动 httpd.exe，但却启动了一个在训练模型时从来未出现过的进程 java.exe。这种情况同样可以使用上述带状态滑动窗口的模型，只要提前训练学习好正常情况下的系统状态（可用窗口状态表达），便可检测此类异常。 Outlier-based Anomaly:基于异常的异常，通过与同等的进程的比较来确定异常。例如，通过比较发现，某个进程向某个 IP 地址传送的数据量比同等进程向其他 IP 地址传送的数据量大太多，那么这个进程就值得怀疑。SAQL 可以使用聚类算法对事件进行聚类，以便在同类的事件中进行比较并发现异常。 除此之外，系统往往需要同时处理大批量的异常检测查询，而目前一般的检测系统如 Siddhi、Esper、Flink 采取的做法都是为每一个 query 复制一份数据，由于操作系统的日志文件是非常巨大的，这样的复制操作会使得系统非常低效。针对这个问题，论文提出一个名为 master-dependent-query 的查询模式，它把用户提交的各种查询进行兼容性分析，然后将兼容的查询分到同一个组，每个组的查询都共用一份数据，这样避免了大量的数据复制，提高了查询分析的效率。 三、总结 总的来说，文章的主要工作是提出了一个用于描述操作系统事件的形式化语言 SAQL，使得用户和系统可以统一地描述各种正常和异常的事件，从而可以灵活地加入专家领域知识并进行异常检测，无需改动系统本身。另外提高为查询效率，文章提出了查询分组的模型，让可以相互兼容的查询共用日志数据，避免了为每个查询复制一次数据的低效模式。 ","date":"2018-11-30","objectID":"/2018-11-30-%E8%AE%BA%E6%96%87-saql/:0:0","tags":null,"title":"SAQL","uri":"/2018-11-30-%E8%AE%BA%E6%96%87-saql/"},{"categories":["Thesis"],"content":"一、背景及问题 目前出现一些高级的多主机攻击手段，它们通过使用“command-and-control(C\u0026C)”通道或者代理服务器来实现攻击者隐藏和攻击方式隐藏。例如 GitPwnd 攻击，它利用了 Git 的同步机制来泄漏被攻击者的私有数据，大致过程如下： 攻击者事先在一个流行的 git repository 的镜像中 放置一个钩子脚本，这个脚本会指向一个恶意的 C\u0026C repository 当被攻击者用 git 复制该 repository 镜像时，git 会自动地连带那个恶意 C\u0026C repository 一起复制到被攻击者的 git 服务器 当被攻击者在 repository 上执行某些特定操作如 git commit 时，恶意 C\u0026C repository 被触发，把被攻击者的数据私下发送到攻击者的主机而不会被察觉 针对这类攻击，一种称为动态信息流追踪（Dynamic information flow tracking , DIFT）的技术被提出并应用于攻击侦查。然而，目前存在的大多数攻击侦查系统只能对单个主机的攻击进行侦查，例如对某个主机下的 system-call-level 或 instruction-level 的事件进行监控，而无法侦查如上述 GitPwnd 之类的跨主机攻击。 二、解决方案 论文提出了一种基于记录-回放的信息流标签和追踪系统（a record-and-replay-based data flow tagging and tracking system, called RTAG）。RTAG 首先通过隔断标签依赖关系（主机之间的信息流）来摆脱多个主机之间信息流顺序的约束，使 DIFT 可以并行执行。其次，RTAG 通过精心设计的数据结构来减少 DIFT 的内存消耗（90%）和时间消耗（60%-90%）。 标签系统 RTAG 的大致设计思路如下： 使用 \u003c主机 mac 地址 + inode + dev + crtime + byte-offset\u003e 来实现全局唯一标签（global tag），标识感兴趣文件中的每个字节，实现对文件在多个主机之间的信息流的追踪 使用与 RAIN 系统一样的资源图（provenance graph） 来追踪程序之间的信息流，如程序到文件、程序到程序、文件到程序 使用可达性分析算法，对多主机的全局资源图进行剪枝，得到只与攻击相关的资源子图，减少后续分析的计算量。为实现此跨主机的可达性追踪，RAIN 需要在 socket 层次（TCP \u0026 UDP）上对主机之间的网络通讯进行监控 采用与 RAIN 系统一样的记录-回放策略（record-and-replay），即平时只记录信息流动的信息，只有在执行攻击分析查询时再通过回放记录来执行 DIFT（因为计算量和内存消耗较大，无法实时在线执行） 使用 32-bit 的数据版本号来隔断 tags 之间的依赖关系，使得每个 replay 可以单独执行而无需执行所依赖的数据版本的 replay。数据版本号在数据（即文件的数据）被执行 write() 之类的系统调用时自动增加 使用 global tag 和 local tag 转换技术来减低资源消耗量。因为在单个主机内执行攻击分析时，没有必要因为使用 global tag 而耗费内存资源。因此在执行这种单个主机内的 DIFT 之前，RTAG 会先把 global tag 转换成 local tag，待执行完后再换回来 三、总结 总的来说，文章提出的 RTAG 系统是在原有的 RAIN 系统上，对其标签系统（tagging system）进行扩展而得到的。它通过使用全局标签来进行跨主机的信息流追踪，使用全局-局部标签转换来降低计算量等等，使得原有的单主机 DIFT 系统 RAIN 扩展到了多主机的攻击侦查。 ","date":"2018-11-24","objectID":"/2018-11-24-%E8%AE%BA%E6%96%87-rtag/:0:0","tags":null,"title":"RTAG","uri":"/2018-11-24-%E8%AE%BA%E6%96%87-rtag/"},{"categories":["Interview"],"content":"一面（2018-09-27 下午3点，约半个小时） 自我介绍 自我介绍中提到了我跟导师做过情感分析和兴趣点推荐的课题，面试官好像比较感兴趣中文怎么做情感分析，一直问我有没有什么思路，幸好记得一点 讲了阿里天池无人机的那个项目，问的不是很细 讲了给实验室老板做的那个erp项目，问得也不是很细，问了些常规问题，框架啊带团队啊什么的 还问了会不会安卓，我讲了本科做过的那个安卓项目，简历上没有写 最近有没有什么学习计划，看什么书 去过深圳吗，愿意来深圳工作吗 二面（2018-09-29 下午2点，约一个多小时） 感觉部门老大应该是做管理的，问的技术不多，问的也不多，就常规地简单地问了下项目，没有深入问。反而给我很详细地介绍了他们团队的业务和方向，非常耐心非常nice。 ","date":"2018-09-29","objectID":"/2018-09-29-%E9%9D%A2%E7%BB%8F-%E8%85%BE%E8%AE%AF%E9%9D%A2%E7%BB%8F/:0:0","tags":null,"title":"腾讯面经","uri":"/2018-09-29-%E9%9D%A2%E7%BB%8F-%E8%85%BE%E8%AE%AF%E9%9D%A2%E7%BB%8F/"},{"categories":["Interview"],"content":"9月18号下午4点，学校附近的某个酒店，现场面。 一面 约30分钟 自我介绍 挑个项目讲下 有什么特别深入研究的技术 职业规划 感觉面试官问得比较少，主要是我自动扩展开来讲，一直讲个不停。 二面 约5分钟？？ 自我介绍 我自我介绍都没说完，二面面官就说行，我没什么问题了，你有什么问我的。？？我愣了好久+黑人问号？？面官笑了下说，我看你一面评价挺高，我这边就不问什么问题，直接给你过了 我问了下华为云的产品和发展规划，面官很耐心介绍，态度很nice，感觉挺不错的 ","date":"2018-09-18","objectID":"/2018-09-18-%E9%9D%A2%E7%BB%8F-%E5%8D%8E%E4%B8%BA%E9%9D%A2%E7%BB%8F/:0:0","tags":null,"title":"华为面经","uri":"/2018-09-18-%E9%9D%A2%E7%BB%8F-%E5%8D%8E%E4%B8%BA%E9%9D%A2%E7%BB%8F/"},{"categories":["Interview"],"content":"一面（2018-09-13，下午5点，80分钟） 讲无人机项目。巨详细，从没被问这么细，还问了有没有看到前几名的方案，他们做得好的地方是在哪里，如果再让你们做一次会怎么优化 O(1) 获取栈中的最大元素。答使用一个辅助栈。追问如何进一步优化，在面试官一步步引导下，想出对相同的元素进行压缩，再申请一个map来对相同的元素计数。最后要我分析了这样申请一个map的开销和原来的不压缩的开销如何权衡，我说如果是对于有大量重复的数据流，那么压缩是值得的，否则没必要，例如连续增大没有重复的数据流 对一个字符串中的单词的位置反转，要求是不能开辟新的空间，如要原地反转 股票买入卖出一次。两次呢？两次的没答上。 如何实现OOM，方法区的呢（CGLIB） i++是原子的吗，并发情况下有什么问题，使用volatile不能保证线程安全？都有哪些方法使他线程安全 mysql的常见优化方案 让一个线程等待另一个线程怎么实现。join() 10亿个数分布在不同的机器上，如何选出 top k。答了最小堆。可能是想问mapreduce。 redis的五种基本数据类型，平时张口就来的竟然不记得set和zset了，尴尬 为什么想投安全研发工程师，了解安全相关的技术吗，讲下XSS 有什么兴趣爱好，怎么带团队，职业规划，对工作城市有什么要求 有什么要问的。介绍了部门团队。最后直接说我通过了，有点惊讶，我感觉后面几个问题答得不太好 二面（2018-09-14，下午3点，1个小时） 自我介绍，讲本科那个美国数学建模大赛，面试官小哥说挺有意思，问得比较细，讲了好久 讲排序算法，快排 二分查找，以及改进版本 最长公共子序列 代码量估计有多少，兴趣爱好，其他offer情况等待 介绍了下他们部门 三面总监面（2018-09-18，下午2点，50分钟） 自我介绍 挑一个项目，讲了ERP那个。总监对我怎么带团队很感兴趣，一直讲带团队的事情 问了一些性格啊，其他特长之类的 职业规划 有啥要问的，详细介绍了部门团队 ","date":"2018-09-18","objectID":"/2018-09-18-%E9%9D%A2%E7%BB%8F-%E7%99%BE%E5%BA%A6%E9%9D%A2%E7%BB%8F/:0:0","tags":null,"title":"百度面经","uri":"/2018-09-18-%E9%9D%A2%E7%BB%8F-%E7%99%BE%E5%BA%A6%E9%9D%A2%E7%BB%8F/"},{"categories":["Interview"],"content":"一面（2018-09-11上午，学校附近的丽顿酒店，约50分钟）： 自我介绍，挑一个最熟悉或者最有亮点的项目讲一下。讲了阿里天池那个，估计面试官不太懂，全程在听。 面试官想挑一个后台的项目来问，看来看去挑了我给实验室老板做的那个ERP，我说了一顿后，面试官也抓不住什么点来问，就说问基础吧 数据库索引都有哪些。能不能画一下B+索引的结构。很简单，画出来后，我说我再给你画出插入1到10的分裂过程吧。面试官有点懵逼，说也行。然后问了叶子结点的指针指向的是什么地址，答数据行在磁盘的地址。然后追问数据库是怎么通过这个地址去磁盘找数据的，我修正了下，答这个地址是磁盘块的地址，数据库会把整个块以及相邻的块一起读到内存（预读）。然后具体怎么去磁盘寻找块的过程我说不知道，面试官说没关系，他也忘了。 问联合索引（a，b，c），where a=? and b\u003e? and c=? 这样的语句为什么会失效 讲下JVM内存分块和GC策略那些，我画出来，给他展开很详细讲，他说行了行了 熟悉的设计模式有哪些。我配合JDK的具体应用来讲。然后手画观察者模式的UML图，不太记得UML图的细节了，我画了个大概，但是把思路讲清楚了 范型的实现原理是什么。类型擦除。用范型来重载方法会怎么样（拒绝编译） 手撕两个无环链表的第一公共结点 手撕最低公共祖先 好像又想回到项目问，但是也没问出个啥，就问了下为啥不投算法，而是研发 愿不愿意来北京？他说看到我投的上海，我说北京生活压力太大，有点不敢去 有啥要问的。我就问了是不是基础架构部，那个大名鼎鼎的美团技术博客是不是你们部门做的，吹了一波。不过我确实觉得那个博客比较厉害 二面（一面结束后的5分钟，约50分钟） 自我介绍 挑一个认为最有亮点的项目讲，还是讲了阿里天池无人机那个。 为什么转后台，职业规划是什么，到目前准备到那一步了，开启吹比模式，扯了很久。 JVM分代和GC算法了解吗。我说一面讲过了。然后他说那你可以讲讲觉得比别人深入的地方，然后我详细讲了CMS和G1的四个阶段. 思想. 优缺点。 画一下观察者模式的ER图，讲一下。 最近都在看哪些书，研究什么，我说并发，然后又扯了很久很久 Hadoop和Spark了解到什么程度，我说只搭建过一次，没有看到源码级别，就没问了。 看我简历上写了Mysql. Redis，让我挑一个讲。挑了Mysql，又是一顿扯。 手撕代码，给一个二维矩阵，每个元素表示地面高度，给其中一个坐标表示下雨，问最后水会流到哪些洼点（高度最低的点）。简单的二维矩阵搜索，从下雨那个点一直向比自己低的周围点扩散出去搜索，当找到一个点比四周都低的时候，它就是个洼地，加入一个全局的List即可。 好像又问了一次职业规划，想做什么方向，愿意来北京不。 有什么要问 三面（总监面，9月13日，约50分钟） 自我介绍，讲无人机项目，讲在导师公司做的项目（手画） 擅长什么方向，职业规划 简单聊了下一些对技术的看法，例如分布式，架构，平台，业务 简单问了下jvm分代以及GC算法，垃圾收集器 兴趣爱好，特长，团队经验，优缺点 有什么要问的。很详细地介绍了他们部门的情况，我表明了我是冲着他们这个技术团队去的，一直都有看他们的博客 hr面（总监面完约10分钟后开始，约20分钟） 自我介绍，讲无人机项目，讲项目 兴趣爱好特长 优缺点 怎么带团队 老虎. 大象. 孔雀. 考拉，选一个认为与自己性格比较接近的 有什么要问的 ","date":"2018-09-13","objectID":"/2018-09-13-%E9%9D%A2%E7%BB%8F-%E7%BE%8E%E5%9B%A2%E9%9D%A2%E7%BB%8F/:0:0","tags":null,"title":"美团面经","uri":"/2018-09-13-%E9%9D%A2%E7%BB%8F-%E7%BE%8E%E5%9B%A2%E9%9D%A2%E7%BB%8F/"},{"categories":["Interview"],"content":"网易云音乐，网易杭州总部现场面试，8月24日中午到下午。 一面(年轻小哥，40分钟) 讲一下索引原理，B+索引，手画插入1到10的过程 场景题，修改歌单排序后保存到数据库，怎么实现最小开销。提示是最长公共子序列，然后手撕代码 算法题，两个字符串最小编辑距离，手撕代码 操作系统基础:进程调度，死锁条件，死锁预防，检测，解除，避免 redis有序集合底层实现原理，跳跃表怎么查找一个数 看了下简历，问为什么不继续做机器学习而是投java研发，以后想做哪方面 有什么要问。我问了歌单保存那题跟最长公共子序列有啥关系，具体怎么实现，感觉小哥也没解释得很清楚，说是他们有一定的策略来实现什么的。 二面(小组leader，谈笑风生1个多小时) 自我介绍，顺带介绍了下在导师公司给导师做的项目 mysql，redis用过没，在项目里怎么用。答没，是自己看的书，了解过一些。然后扯了一些使用方式。 讲redis的时候扯到了缓存和分布式缓存。问了缓存淘汰策略。然后又问我对分布式了解都有哪些，刚好前一天看了，就说了分布式锁，分布式事务。然后问我怎么理解CAP，具体有哪些权衡C和A的案例。估计我扯的还算沾边，面试官来了兴致，开始进入侃侃而谈状态，大谈分布式理论，思想。 不记得怎么就扯到了锁(估计是说了redis分布式锁)，问了AQS实现方式。然后他问我有没有了解过“无锁队列”，然后他又是一顿介绍。感觉已经不是在面试了，纯属聊天。 又扯回到mysql，问我了解到什么程度，我就把所有知道的都说了下，例如innodb和mysian对比，innodb的mvcc，索引，幻读，next-key lock，让我手画实现锁住一个范围的原理。场景题:有两个查询，select … where a=？ and b =? ，和 select … where a=? and b\u003e? 如何分别建立索引，为什么，有什么区别。乱扯了一通，然后他也没给答案。 我又讲了下mysql事务。问了实现原理。讲了redo和undo，然后问为什么要这样做。我说扯了一通，他说我没说到点上，关键是因为写缓冲的存在，断电会丢失。 知道哪些设计模式。装饰者和模板、代理有什么区别。jdk里面都有哪些模式。 solid原则 未来几年想做哪方面，技术还是业务，有什么职业规划 有什么要问(中途他接了个电话忙了一会儿)，我问了个技术问题，然后他又是一顿侃侃而谈。 hr面(40分钟左右) 自我介绍 总结自己优缺点，性格 最自豪或者最有成就感的一件事 有什么别的特长，说了弹吉他，然后聊了好久弹吉他的事，说了毕业晚会表演自弹自唱 在导师公司怎么带团队的，有什么经验总结 怎么看待云音乐，和qq音乐对比呢(送分题，反正狂吹就是了) 有什么梦想 平常有没有关注一些其他领域或者什么资讯，说了中兴事件和中国芯，确实是我一直在关注的 都有哪些offer了，为什么不投BAT，又是送分题 职业规划 有什么要问的，问了最后会怎么定岗位 让我先不要走，等5分钟，可能还有一面 总监面(不愧大佬，高冷，短裤拖鞋，霸气侧漏，40分钟) 自我介绍 怎么理解互联网这个行业 又是索引。问了一天的索引。问主键索引和唯一索引的原理区别，什么情况下使用，性能上有何区别。没答上。 jvm怎么垃圾回收。c++学过吧，你觉得它应该怎么实现类似jvm垃圾回收的功能？你怎么知道一个对象在什么时候是否该被回收？没答上。 你觉得你有什么特点或者特长可以帮助你成功，举个事例证明 职业规划，想做什么 有什么问 ","date":"2018-08-24","objectID":"/2018-08-24-%E9%9D%A2%E7%BB%8F-%E7%BD%91%E6%98%93%E9%9D%A2%E7%BB%8F/:0:0","tags":null,"title":"网易面经","uri":"/2018-08-24-%E9%9D%A2%E7%BB%8F-%E7%BD%91%E6%98%93%E9%9D%A2%E7%BB%8F/"},{"categories":["Interview"],"content":"一面（2018-07-24 下午三点半，电话面约20分钟） 自我介绍 介绍现在做的项目，因为在自我介绍中提到了在导师公司做的ERP。我简单介绍了一下Opentaps，以及我负责的采购模块的工作流后，就转移话题到其他项目上。 介绍阿里天池的无人机大赛，还有本科搞的美赛，讲一下遇到什么问题，当时怎么解决的，取得了什么成绩。基本是我自己在说，面试官很少打断和提问。 为什么投的java研发，但是简历上大部分是数据分析类的比赛和项目。 有没有用过hadoop. spark处理过大批量数据 愿不愿意转Python，然后面试官介绍了一下他们部门的业务，主python，也有部分java 感觉都答得挺不错，面试官也比较认可吧，面完过了十几分钟查了下，状态为一面通过，等待hr联系安排二面 二面（2018-07-27 下午三点半，一直等到五点，视频面约20分钟） 自我介绍 由于再次给自己挖坑，自我介绍的时候提到了在导师公司做ERP，面试官就开始问ERP。简单介绍了一下采购工作流，面试官就一直问有什么难点，扯了好久，最终结论是面试管认为这不算难点，只算业务繁琐，工作量的问题而已 问了一下工业AI那个比赛，答得不好，说也没什么难点，只是常规的数据预处理+使用常用模型进行预测，就得到比较好的成绩 有什么要问的，我一看时间才过了10多分钟，感觉凉了一半，就赶紧扯了下前一位面试官让我转python的事，然后说了一下职业规划，然后让面试官提点建议，面试官很耐心很nice，简单介绍了一下部门业务，硬是被我扯着聊了10分钟后结束 面完感觉是凉了， 吃完饭回来查了下，状态变为二面通过，但是没有“等待hr联系三面”，不知道什么情况 三. 终面（2018-08-16 下午三点半，学校现场面，约20分钟） 自我介绍。问在导师公司做什么 看着简历问，让讲了无人机，还有ERP 职业规划 介绍了他部门，然后跟我解释了半天，做业务也可以技术积累 有没有其他offer ","date":"2018-08-16","objectID":"/2018-08-16-%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E7%96%86%E9%9D%A2%E7%BB%8F/:0:0","tags":null,"title":"大疆面经","uri":"/2018-08-16-%E9%9D%A2%E7%BB%8F-%E5%A4%A7%E7%96%86%E9%9D%A2%E7%BB%8F/"},{"categories":["Interview"],"content":"一面（2018-08-02 晚上7点半，视频面，约30分钟） 自我介绍 讲一下在导师公司做的项目，表示第一次听到opentaps，介绍一下。遇到什么难点，怎么解决 讲一下无人机路线规划那个比赛 职业规划 有什么要问的 二面（2018-08-11下午两点半，视频面，约20分钟） 自我介绍 介绍一下在导师公司做的项目，自己负责什么 怎么带领团队，怎么沟通 对导师的公司怎么看 了解哪些机器学习算法 有什么要问的 ","date":"2018-08-11","objectID":"/2018-08-11-%E9%9D%A2%E7%BB%8F-%E9%A1%BA%E4%B8%B0%E9%9D%A2%E7%BB%8F/:0:0","tags":null,"title":"顺丰面经","uri":"/2018-08-11-%E9%9D%A2%E7%BB%8F-%E9%A1%BA%E4%B8%B0%E9%9D%A2%E7%BB%8F/"},{"categories":["Interview"],"content":"一面（2018-08-01 晚上8点，50分钟） 自我介绍 讲一下在导师公司做的项目，有什么技术难点。说了数据库的用版本控制实现一个乐观锁。然后问了下工业AI比赛，简单讲了下思路（分析工序间的联系，筛选数据，然后预测） jvm：内存模型、垃圾回收算法、垃圾收集器（没答好）；有什么类加载器（没答好）、双亲委派模型、有什么优缺点（没答好）；jvm如何调优（不会） java：HashMap的内部机制、初始大小与扩容、为什么选择这个数值（没答好，给了答案是保持2的幂作性能开销小？）、为什么要用拉链法（我答的是为了解决碰撞问题。如果用线性探测开放地址法，每次删除一个元素，后面的所有碰撞的元素全部都要重新散列，代价非常大）、是否线程安全、哪个是线程安全、HashTable和ConcurrentHashMap如何实现线程安全、ConcurrentHashMap的内部机制、分段锁的思想。HashMap1.7之后有什么变化？红黑树。为什么？加快查询速度。 分布式了解吗，场景题：假设有100台服务器，争抢一个任务，应该怎么做？不太明白什么意思，反问是什么任务，是不是客户端请求，然后把反向代理和均衡负载说了下，面试官说不是，然后我又讲了paxos协议，他说也不是，实在想不出来，他说是用锁。。？没细说 spring：ioc，aop，能有多详细讲多详细 springMVC如何分析请求，然后如何交给handler处理，能有多详细讲多详细（不会） mybatis怎么用（没答好），除了mapper还有什么（不会），对比hibernate有什么优点（不会） redis讲一下，有什么应用场景（答得不好，答了说存session，面试官说不是，然后我说缓存？面试官好像没说啥），与mencache对比有什么优缺点 场景题，有一些的jvm参数的历史数据（例如gc次数，gc时间等等），如何预测jvm是否会发生故障（特别是往年的京东618时候的jvm参数），说思路。我想了好久，面试官一直降低难度甚至换题目，我实在想不出，就随便后来说了下，先分析出都有哪些jvm的影响因素，然后用历史数据训练得到每个因素的权重，然后就可以使用这些权重来预测是否发生故障。面试官说这不就是我刚开始想问的嘛，笑了笑说让等二面 二面（2018-08-02下午五点半，约10分钟） 自我介绍 简单讲一下项目 什么框架使用最熟悉，会不会HQL 4、你是如何分配机器学习和java研发的精力 如何利用机器学习预测jvm的故障，和一面的问题差不多，我乱扯了一些 说一下你最强势或者突出的技术特长，（我直接承认框架使用比较少，因为接触的项目不多，但是我对jdk、jvm、计算机基础比较扎实，也比较感兴趣，还以为他要开始狂问，但结果没问） 能不能来实习（他表示，不能来实习就很难办了） 简单介绍了他的部门 感觉像是hr？没怎么问技术，听声音好像年纪比一面的面试官应该大不少，还问华科是不是华工。。 三面（8月8日下午6点，约21分钟） 自我介绍 n个已排序队列，选出队列中最大的top k个数。显然先对每个队列取top k，然后再选出其中的top k。说了用小根堆，(nk)logk。面试官问还有没有更快的，没答上。后来和同学讨论了下，最好应该分情况答，如果n远大于k，应该用归并，复杂度是 klog(n)。如果k远大于n，则应该用小根堆。 讲一下阿里天池无人机的那个比赛，是怎么做的。对深度学习，图像处理有没有了解，答没有。 讲一下职业规划 有什么问题 四面（8月9日下午7点，感觉也不算面试，就是做了道题） 面试官给了链接，一道自他己出的小题目，对一个文件的数据进行求和统计然后排序输出，大概十五分钟写完，然后面试官不见了，不知道啥情况，等了一个多小时后我自己下线了 HR面（8月10日中午2点，约7分钟） 简单介绍下项目 一边看我简历一边说，保研成绩啊，比赛名次啊，一直在夸我都不好意思了 家在哪里，北京有没有亲戚 对京东了解吗？我笑了笑说在中国还有谁不了解京东的吗，然后她也笑了。然后我就说买东西基本都是京东买什么的。 说大概一周之后会答复我，走后面的流程什么的 有什么要问的 ","date":"2018-08-10","objectID":"/2018-08-10-%E9%9D%A2%E7%BB%8F-%E4%BA%AC%E4%B8%9C%E9%9D%A2%E7%BB%8F/:0:0","tags":null,"title":"京东面经","uri":"/2018-08-10-%E9%9D%A2%E7%BB%8F-%E4%BA%AC%E4%B8%9C%E9%9D%A2%E7%BB%8F/"},{"categories":["Algorithm"],"content":"1、最长递增子序列（返回长度） 给定数组 arr，返回它的最长递增子序列的长度。 思路：显然对于数组的任何一个子序列，它必定以数组的某个元素结尾，因此可用 dp[i] 表示以 arr[i] 结尾的最长递增子序列的长度。那它是如何得来的呢？显然它就是在前面 i-1 个最长递增子序列后面追加或者不追加 arr[i] 而得到的 i-1 个新的递增子序列中，最长的那个。如果 arr[i] 大于 arr[j]，那么 arr[i] 就可以追加到以 arr[j] 结尾的最长递增子序列。 边界条件：如果 arr[i] 比它前面的 i-1 个元素都小，那么以 arr[i] 结尾的最长递增子序列就是它自己了，此时 dp[i]=1。 $$ dp[i]=\\max{ dp[j]+1 } \\text{,} \\quad (0 \\leq j \u003c i \\text{,} arr[j]\u003carr[i]) $$ private static int LIS(int[] arr) { if (arr == null || arr.length == 0) return 0; int[] dp = new int[arr.length]; for (int i = 0; i \u003c arr.length; i++) { dp[i] = 1; for (int j = 0; j \u003c i; j++) { if (arr[i] \u003e arr[j]) { dp[i] = Math.max(dp[i], dp[j] + 1); } } } int maxLen = 0; for (int i = 0; i \u003c dp.length; i++) maxLen = Math.max(maxLen, dp[i]); return maxLen; } 2、最长递增子序列（返回序列） 给定数组 arr，返回它的最长递增子序列。 思路：先按上面所述求出 dp 数组，然后再根据 dp 数组恢复出最长的子序列。恢复过程很简单：先找到 dp 中最大的元素的位置，假设是 i ，那么说明数组 arr 的最大递增序列以 arr[i] 结尾，也就是说它是所求序列的最后一个元素。然后从 arr[i] 开始往前扫描，如果遇到一个元素满足 dp[j]=d[i]-1 并且 arr[j] 小于 arr[i]，那么 arr[j] 便是倒数第二个元素。依次类推即可。 private static int[] LIS2(int[] arr, int[] dp) { int maxLen = 0; int lastIndex = 0; for (int i = 0; i \u003c dp.length; i++) { if (dp[i] \u003e maxLen) { maxLen = dp[i]; lastIndex = i; } } int[] LIS = new int[maxLen]; int LISIndex = maxLen - 1; LIS[LISIndex--] = arr[lastIndex]; for (int j = lastIndex; j \u003e= 0; j--) { if (arr[j] \u003c arr[lastIndex] \u0026\u0026 dp[j] == dp[lastIndex] - 1) { LIS[LISIndex--] = arr[j]; lastIndex = j; } } return LIS; } 3、最长公共子序列（返回长度） 给定两个数组，返回两个数组的最长公共子序列的长度。 思路：用 dp[i][j] 表示 X[0…i] 与 Y[0…j] 的最长公共子序列长度。那么 dp[i][j] 是如何得到的？我们只需观察两个数组的最后一个元素 X[i] 和 Y[j]。如果 X[i] 和 Y[j] 相等，那么说明它就是 dp[i][j] 对应的公共子序列的最后一个元素。此时 dp[i][j] 由 dp[i-1][j-1]+1 得到。如果不相等，说明这个元素不是公共子序列的最后一个元素。那么此时 dp[i][j] 要么等于 dp[i-1][j]，要么等于 dp[i][j-1]。 边界：注意dp[0][0]，dp[i][0] 和 dp[0][j]。 $$ dp[i][j]= \\begin{cases} dp[i-1][j-1]+1，\u0026 \\text{if $x_{i}=y_{j}$} \\\\ max\\{dp[i-1][j], \\, dp[i][j-1]\\}，\u0026 \\text{if $x_{i} \\neq y_{j}$} \\end{cases} $$ private static int LCS(int[] X, int[] Y) { if (X == null || X.length == 0 || Y == null || Y.length == 0) return 0; int[][] dp = new int[X.length][Y.length]; if (X[0] == Y[0]) dp[0][0] = 1; for (int i = 1; i \u003c X.length; i++) dp[i][0] = Math.max(X[i] == Y[0] ? 1 : 0, dp[i - 1][0]); for (int j = 1; j \u003c Y.length; j++) dp[0][j] = Math.max(X[0] == Y[j] ? 1 : 0, dp[0][j - 1]); for (int i = 1; i \u003c X.length; i++) { for (int j = 1; j \u003c Y.length; j++) { if (X[i] == Y[j]) dp[i][j] = dp[i - 1][j - 1] + 1; else dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]); } } return dp[X.length - 1][Y.length - 1]; } 4、最长公共子序列（返回序列） 给定两个数组，返回两个数组的最长公共子序列。 思路：想按上题求出 dp 数组，然后根据它来恢复序列。思路很简答，根据上面的思路倒推回去即可。 private static int[] LCS2(int[] X, int[] Y, int[][] dp) { int len = dp[X.length - 1][Y.length - 1]; int[] lsc = new int[len]; int m = X.length - 1; int n = Y.length - 1; int index = len - 1; while (index \u003e= 0) { if (n \u003e 0 \u0026\u0026 dp[m][n] == dp[m][n - 1]) n--; else if (m \u003e 0 \u0026\u0026 dp[m][n] == dp[m - 1][n]) m--; else { // X[m]==Y[n] lsc[index--] = X[m]; m--; n--; } } return lsc; } 5、最长公共子数组/子串 给定两个数组，返回最长公共子数组。 思路：思路类似公共子序列，但是因为子数组/子串必须是元素连续的，因此略有不同。同样构造 dp 数组，dp[i][j] 表示以 X[i] 和 Y[j] 结尾的最长公共子数组/子串的长度。如果这两个元素不相等，那么 dp[i][j]=0，如果相等则 dp[i][j]=dp[i-1][j-1]+1。 边界：dp[0][0]，dp[i][0]，dp[0][j]。 $$ dp[i][j]= \\begin{cases} dp[i-1][j-1]+1，\u0026 \\text{if $x_{i}=y_{j}$} \\\\ 0，\u0026 \\text{if $x_{i} \\neq y_{j}$} \\end{cases} $$ private static int[][] LCSA(int[] X, int[] Y) { if (X == null || Y == null || X.length == 0 || Y.length == 0) return null; int[][] dp = new int[X.length][Y.length]; if (X[0] == Y[0]) dp[0][0] = 1; for (int i = 0; i \u003c X.length; i++) dp[i][0] = X[i] == Y[0] ? 1 : 0; for (int j = 0; j \u003c Y.length; j++) dp[0][j] = X[0] == Y[j] ? 1 : 0; for (int i = 1; i \u003c X.length; i++) { for (int j = 1; j \u003c Y.length; j++) { if (X[i] == Y[j]) dp[i][j] = dp[i - 1][j - 1] + 1; else dp[i][j] = 0; } } return dp; } 现在根据 dp 数组来恢复出公共子数组/子串。由于 dp[i][j] 表示的是以 X[i] 和 Y[j] 结尾的最长公共子数组/子串，只有当 X[i] 和 Y[j] 相等时才会增长 dp[i][j]。因此可以看到 dp 数组必定是沿着右下方增长的。只要遍历 dp 找到最大的元素 dp[m][n]，其值即为最长公共子数组/子串的长度，并且该子数组/子串以 X[m] 或者 Y[n] 结尾。直接从 X 或者 Y 里面截取即可。 ","date":"2018-05-01","objectID":"/2018-05-01-%E7%AE%97%E6%B3%95-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E9%A2%982/:0:0","tags":null,"title":"动态规划题(2)","uri":"/2018-05-01-%E7%AE%97%E6%B3%95-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E9%A2%982/"},{"categories":["Algorithm"],"content":"1、矩阵的最小路径和 给定一个矩阵 m ，从左上角开始，每次只能向右或者向下走，最后到达右下角的位置，定义路径上所有的数字之和为路径和。求所有路径中最小路径和。 思路：因为题目规定了每次只能向右走一步或者向下走一步，因此每一个位置的上一步只能是来自左边一个位置或者上边一个位置。那么走到某个位置的最小路径，只能是左边位置路径和与上边位置路径和中最小的那个，加上本位置的数字。 考虑边界条件：第一行的所有位置只能从左边走过来，第一列的所有位置只能从上面走下来。 用 $dp[i][j]$ 表示第 $i$ 行第 $j$ 列位置上的最小路径和，则 $$ dp[i][j]=min{dp[i][j-1], dp[i-1][j]}+m[i][j] $$ private static int minPath(int[][] m) { if (m == null || m.length == 0 || m[0].length == 0) return 0; int row = m.length; int col = m[0].length; int[][] dp = new int[row][col]; dp[0][0] = m[0][0]; for (int i = 1; i \u003c row; i++) dp[i][0] = dp[i - 1][0] + m[i][0]; for (int j = 1; j \u003c col; j++) dp[0][j] = dp[0][j - 1] + m[0][j]; for (int i = 1; i \u003c row; i++) { for (int j = 1; j \u003c col; j++) { dp[i][j] = Math.min(dp[i - 1][j], dp[i][j - 1]) + m[i][j]; } } return dp[row - 1][col - 1]; } 2、硬币找零——最少硬币数（硬币数量无限） 给定数组 coins，其元素均为正整数且不重复。每个元素代表一种面值的硬币。再给定一个换零的目标正整数 target，求用 coins 进行找零的最少硬币数。每种面值的硬币可以多次使用。 思路：因为题目规定每种面值的硬币可以无限重复使用，因此在找零的过程中，每一步都可以拿起 coins 数组中任意一个硬币，即每一步都有 coins.length 种选择。假设现在手上得到找零目标为 target 的最优找零方案。那么现在倒退一步来看，在拿起某个硬币 coins[j] 而得到 target 之前，手上的方案 target-coins[j] 必定也是最优的（可用算法导论的剪切-粘贴法证明）。如前面所述，每一步都有 coins.length 种选择，那么只需找出这些选择中最优的方案即可。即对每一个目标 i，只需搜索目标为 i-coins[j] 的所有方案并取最优的方案加 1 即可。 考虑边界条件：找零目标为 0 时的最少硬币数当然是 0。另外在遍历 coins 数组时，需要注意数组越界问题，即如果某个硬币的面值比要找零的目标还大的情况下，需要跳过这种情况。可通过先对 coins 数组进行排序来减少每一步的搜索空间。 设 dp[i] 表示找零目标为 i 的最少硬币数，则 $$ dp[i]=\\min_{1 \\leq j \\leq coins.length} dp[i-coins[j]]+1 $$ private static int minCoins(int[] coins, int target) { if (coins == null || coins.length == 0 || target \u003c= 0) return 0; int[] dp = new int[target + 1]; dp[0] = 0; int max = Integer.MAX_VALUE; Arrays.sort(coins); // let \"coins[j]\u003c=i\" to skip some useless solutions for (int i = 1; i \u003c= target; i++) { dp[i] = max; for (int j = 0; j \u003c coins.length \u0026\u0026 coins[j] \u003c= i; j++) { if (dp[i - coins[j]] != max) dp[i] = Math.min(dp[i], dp[i - coins[j]] + 1); } } return dp[target] == max ? 0 : dp[target]; } 3、硬币找零——最少硬币数（硬币数量有限） 给定数组 coins，其元素均为正整数，有可能重复。每个元素代表一种面值的硬币。再给定一个换零的目标正整数 target，求用 coins 进行找零的最少硬币数。每个硬币只能使用一次。 思路：由于每个硬币只能用一次，所以不但需要记录每一步找零的目标，还需要记录用到了 coins 中哪些硬币，因此需要使用二位数组 dp[i][j]。dp[i][j] 表示使用 coins[0…i] 中的某些硬币来组成找零目标 j 的最少硬币数。主体思路和上一题差不多，只是这里不能“在每一步任意选取 coins.length 种硬币”了，而只能“要么用第 i 个硬币，要么不用”。假设现在手上已有使用前 i 个硬币得到找零目标为 j 的最优方案 dp[i][j]，那么它是如何得到的？同样倒推回去，它的前一步是：如果这一步使用了第 i 枚硬币 coins[i]，那么在使用这枚硬币前，手上的方案 dp[i-1][j-coins[i]] 必定也是一个最优的方案，此时使用这枚硬币后得到 dp[i][j]=dp[i-1][j-coins[i]]+1；如果这一步不使用 coins[i]，那么前一步手上的最优方案是 dp[i-1][j]，因为不使用这枚硬币，所以 dp[i][j]=dp[i-1][j]+0。最后 dp[i][j] 的最优解即为这两种情况的最小值。 边界条件：找零目标为 0 时最少硬币数为 0。搜索 coins 数组时同样要考虑硬币面值是否大于找零目标。如果小于找零目标，就按照上面的去两种方案中的最小值。如果大于找零目标，说明这个硬币永不上来找零，但是它的硬币数并不是0，而是直接等于不用这个硬币的方案，即 dp[i-1][j]。 $$ dp[i][j]=\\min{dp[i-1][j-coins[i]]+1, \\quad dp[i-1][j]} $$ private static int minCoinsLimit(int[] coins, int target) { if (coins.length == 0 || target \u003c= 0) return 0; int[][] dp = new int[coins.length][target + 1]; int max = Integer.MAX_VALUE; for (int j = 1; j \u003c= target; j++) dp[0][j] = max; if (target \u003e= coins[0]) dp[0][coins[0]] = 1; for (int i = 1; i \u003c dp.length; i++) { for (int j = 1; j \u003c= target; j++) { if (j - coins[i] \u003e= 0) if (dp[i - 1][j - coins[i]] == max) dp[i][j] = dp[i - 1][j]; else dp[i][j] = Math.min(dp[i - 1][j - coins[i]] + 1, dp[i - 1][j]); else dp[i][j] = max; } } return dp[coins.length - 1][target] == max ? 0 : dp[coins.length - 1][target]; } 4、硬币找零——找零方法数（硬币数量无限） 给定数组 coins， 其中所有的值均为正整数且不重复，每个值代表一种面值的硬币。现给定找零目标 target，规定每种硬币可以使用无数次，求一共有多少种找零方法。 思路：思路很简单，用 dp[i][j] 表示使用硬币 coins[0…i] 来组成找零目标 j 的方法数。同样倒回去想，dp[i][j]是怎么来的呢？显然是由 dp[i-1][x] 得来的。由于硬币数量无限，所以对第 i 种硬币，可以使用 0 到 K 个（$ K * coins[i] \\leq j $）。因此 dp[i][j] 等于 dp[i-1][j-0coins[i]]、 dp[i-1][j-1coins[i]]、dp[i-1][j-2coins[i]]、…、dp[i-1][j-Kcoins[i]] 之和。 $$ dp[i][j]=\\sum_{0\\leq k \\leq K} dp[i-1][j-k*coins[i]] $$ private static int coins(int[] coins, int target) { if (coins == null || coins.length == 0 || target \u003c= 0) return 0; int[][] dp = new int[coins.length][target + 1]; for (int i = 0; i \u003c coins.length; i++) dp[i][0] = 1; for (int j = 0; j * c","date":"2018-04-29","objectID":"/2018-04-29-%E7%AE%97%E6%B3%95-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E9%A2%981/:0:0","tags":null,"title":"动态规划题(1)","uri":"/2018-04-29-%E7%AE%97%E6%B3%95-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E9%A2%981/"},{"categories":["Projects"],"content":"一、赛题 给出5天的气象数据（风速和降雨量），预测未来5天的气象情况，并根据预测结果，为每天的10架无人机设计最优的飞行路线。要求是无人飞行器不能进入风速大于等于15、降雨量大于等于4的区域，否则立即坠毁。另外还有一些限制条件如任意两架无人飞行器起飞时间必须间隔大于等于10分钟且最大飞行时长为18个小时等等。详情见：未来已来——气象数据领航无人飞行器线路优化大赛。 其中，我负责气象预测部分，队友负责线路规划部分。由于线路规划是取决于气象预测的结果，如果预测错误导致飞行器进入实际是危险的区域而坠毁，那么将会受到严重的加时惩罚（每架次惩罚24*60分钟）。所以气象预测的准确性在这个比赛中非常重要。因此我们的目标是：尽可能避免坠毁，其次再考虑如何寻找总飞行时长最小的线路。以下主要讲述我负责的气象预测部分。 二、数据 官方给出5天的气象数据，分为风速和降雨量两部分。每个地图区域抽象成一个坐标点(x,y)，每个区域每小时一条气象数据，每天一共20个小时。拿到数据后需要进行 Merge，才能得到“每个区域每个小时一条数据”的数据格式。 Merge 的时候要注意列名是否重复，是否需要重命名新拼接的列，否则会导致错误。完成后需要检查数据是否正确。从图中可以看出，样本的特征不多，只有坐标、天、小时、真值和 10 个气象模型的预测值共 14 个特征。 三、分析 把每个小时的静态风速图和一天20个小时的风速动图画出来分析。发现每天的风速变化相当大，即便是同一天内，同一个区域不同小时的风速变化也很大，这为预测增加了不小的难度。 另一个难点是官方提供的测评方式是直接返回无人飞行器的飞行时长总得分，而不是关于气象预测的某种评价函数的得分。这让我们无法直接获得气象预测结果的反馈，为模型调优增加了难度。 还有一个难点是前5天和后5天的数据分布不一致。例如前5天平均风速在10左右，远小于危险风速值15，而后5天平均风速在15左右，每天都是大片的飞行危险区域，可行域很少，增加了线路寻优的难度。 四、初赛 1、直接二分类 刚开始的想法是二分类。因为每个区域对于飞行器来说，只有“危险”和“安全”两个状态，因此我们先想到的是把气象预测问题看作二分类问题来处理，使用 LightGBM 的 binary classification 模式直接对每个区域每个小时是否安全进行逻辑回归。尝试过的方案： 取5天的某4天作训练集，剩下一天作为验证集 尝试去掉特征 [xid, yid, date_id, hour] 中的一个或多个 把10列气象特征的平均值作为新特征 特征规范化（minmax）/ 二值化（对10列气象特征） 尝试不同的危险阈值（风速），如 13，13.5，14，14.5，15等 设置正负样本的权重，尝试不同的权重，如 1：1，1：10等 这里的思路是，为了尽量避免预测的时候出现伪安全区域，导致飞行器坠毁而被严重罚时，应该设置更低的危险阈值，更大的负样本（危险区域）权重。当然如果把过多的安全区域预测为危险区域，会导致后面的线路规划没有路可走。 经过几次提交发现效果不太理想（也可能是线路规划的问题，总之最终得分不是很好）。通过误差分析发现，直接使用二分类对那些接近危险阈值的样本的预测并不好。例如，有某个区域在某个小时的10列气象特征的平均值为14.5，应该判为危险，但是二分类模型却把它判断为安全（为什么？）。 2、先回归再按阈值二分类 由于二分类效果不理想，于是转向回归，即先通过回归预测未来5天的风速值，通过与危险阈值比较，大于等于危险阈值的判为危险，否则判为安全区域。尝试过的方案： 随机切分数据集 train、eval、test，按 98%：1%：1% 的比例 去掉特征 date_id 特征标准化（minmax） 删除异常点（就是10个气象特征均值与真值的绝对值之差达到5以上的样本） 每天单独训练，得到5个模型，每个模型对后5天的结果进行预测，再加权平均 对预测结果进行平滑。尝试使用 scipy 的高斯平滑和卷积平滑。具体步骤是先把整个地图某个小时的风速按坐标顺序 reshape 成一个“图像”矩阵，然后调用 scipy.ndimage.filters.gaussian_filter() 和 scipy.ndimage.filters.convolve() 进行平滑处理，再把矩阵平展成原来的数据格式 尝试不同的危险阈值（风速），如 13，13.5，14，14.5，15等 对每个样本，加入周围8个点的10*8共80个风速特征 先回归再按阈值二分类这个方案得到的成绩是比较好的，在线路规划的版本不变的情况下，最好的成绩来自 20000 轮训练的 ligtGBM regression 模型。高斯平滑和卷积平滑方案作用不大，成绩提高在个位数以内，可视为基本无效。 3、投票法 不跑模型不训练，直接统计预测集中每个样本的10列风速值特征中大于等于危险风速值的个数，如果10个中有5个或以上是危险的风速，那么就把这条样本预测为危险，否则预测为安全。也尝试过对九宫格（即加入周围80列特征）进行此方法，但是都效果一般，从最终得分来看提高不大。 最后还是使用了方案2，20000 轮的回归，结合线路规划的多次优化，最终以第2名的成绩进入复赛。 五、复赛 复赛加入了降雨量这个条件，并且对飞行器的飞行规则也加了一些约束，总体来说比初赛难度大了很多。由于时间只有4天，提交的次数有限，我们只尝试了一下方案： lightGBM 的 LGBMClassifier 二分类 xgboost 的 XGBClassifier 二分类 投票法 风速和降雨量合并起来一起预测 打算尝试模型融合，但是时间不够了，最终成绩是17名。最好成绩用的版本是 xgboost 的二分类版本。 六、第三名分享思路 链接：Future Chanllenge – Experience Sharing (3rd place) 他们的思路很简单，就是用 xgboost 和 lightGBM 来做回归然后再模型融合，也没有太多特征工程，就是比我们多了一个“对每一行数据，视那些比真值大太多或者小太多的数据为异常点，用剩下的点的均值替换掉些数据”。这样改数据的方法有什么理论支持？我不是很理解。另外他们做了交叉验证和很多不同的特征组合，但是我们的机器不好，跑一次训练加预测就要两个多小时，我们没有这个时间。再有一个不同的就是他们做了模型融合，我们没有做。我觉得在预测这一部分我们和他们的效果应该是差不太多的。 ","date":"2018-03-25","objectID":"/2018-03-25-%E9%A1%B9%E7%9B%AE-%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E6%B0%94%E8%B1%A1%E6%97%A0%E4%BA%BA%E6%9C%BA%E7%BA%BF%E8%B7%AF%E4%BC%98%E5%8C%96%E5%A4%A7%E8%B5%9B/:0:0","tags":null,"title":"阿里天池气象无人机线路优化大赛","uri":"/2018-03-25-%E9%A1%B9%E7%9B%AE-%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E6%B0%94%E8%B1%A1%E6%97%A0%E4%BA%BA%E6%9C%BA%E7%BA%BF%E8%B7%AF%E4%BC%98%E5%8C%96%E5%A4%A7%E8%B5%9B/"},{"categories":["MachineLearning"],"content":"特征工程（Feature Engineering）：利用领域知识和现有数据，构造出需要的特征，用于机器学习算法【参考维基百科：Feature engineering】。 俗话说，数据与特征工程决定了模型的上限，改进算法只是逼近这个上限。可见特征工程在机器学习领域里的地位之重要。针对数据挖掘以及传统的机器学习，通过人们的总结和归纳，特征工程主要包括以下方面： 目前比较流行的方案是借助一个强大的 python 类库—— sklearn 来实现特征工程。它的官方文档是教材级别的文档，值得深入学习。言归正传，下面开始总结特征工程的方法。 一、数据预处理（参考sklearn官方文档 Preprocessing data） 1、无量纲化：使不同规格的数据转换到同一规格。常用的方法有标准化、缩放化、正则化三种，它们的具体区别参见：归一化/标准化/正则化 (1)、标准化：减去均值并除以标准差。公式表达为： $$x_{new}=\\frac{x-\\bar{x}}{S}$$ 使用 sklearn 的 StandardScaler 实现： from sklearn.preprocessing import StandardScaler data_new = StandardScaler().fit_transform(data) (2)、缩放化：常用最大最小值进行缩放，把特征缩放到区间 [0，1] 内，公式表达为： $$x_{new}=\\frac{x-x_{min}}{x_{max}-x_{min}}$$ 使用 skearn 的 MinMaxScaler 实现： from sklearn.preprocessing import MinMaxScaler data_new = MinMaxScaler().fit_transform(data) (3)、正则化：对每个样本计算其 p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的 p-范数（L1-norm，L2-norm）等于1。其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。在 L2 范数下正则化公式表达为： $$x_{new}=\\frac{x}{||x||_{2}}$$ 使用 sklearn 的 Normalizer 实现： from sklearn.preprocessing import Normalizer data_new = Normalizer().fit_transform(data) 2、二值化：设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下： $$x_{new} = \\begin{cases} 1, \u0026 \\text {if $x \u003e threshold$} \\ 0, \u0026 \\text{if $x \\leq threshold$} \\end{cases}$$ 使用 sklearn 的 Binarizer 实现： from sklearn.preprocessing import Binarizer data_new = Binarizer(threshold=3).fit_transform(data) 3、独热编码：常用于类别特征（category feature）的处理，把如“男/女”这样的字符型特征转化为数值类型特征。思想是使用 N 位状态寄存器来对 N 个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。详情参见：数据预处理之独热编码。使用 sklearn 的 OneHotEncoder 实现： from sklearn.preprocessing import OneHotEncoder data_new = OneHotEncoder().fit_transform(data) 4、缺失值处理：缺失值的填补方法有多种，中位数填补、众数填补、均值填补、临近数填补。采用怎么样的填补方法，或者直接放弃该特征，需视具体情况而定。可参考：浅谈数据挖掘中的数据处理（缺失值处理以及异常值检测）。使用 sklearn 的 Imputer 实现： from sklearn.preprocessing import Imputer data_new = Imputer(strategy='mean').fit_transform(data) 5、构造多项式特征：假设有数据集有两个特征 $[X_1, X_2]$，则可以利用 sklearn 的 PolynomialFeatures 构造二次多项式特征如 $[1,X_1,X_2,X_1^2,X_1*X_2,X_2^2]$。也可以构造更高次数的多项式特征，只需要调整参数即可。 from sklearn.preprocessing import PolynomialFeatures data_new = PolynomialFeatures(2).fit_transform(data) 6、函数转换：利用 Python 的函数对特征进行转换。可以用 sklearn 的 FunctionTransformer 实现： import numpy as np from sklearn.preprocessing import FunctionTransformer data_new = FunctionTransformer(np.log1p).transform(X) 二、特征选择（参考sklearn官方文档 Feature selection） 1、Filter：过滤法或称选择法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。 (1)、方差选择法：先计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。这样可以去掉一些方差特别小的特征如常量特征，这种特征对于模型的训练和预测不会有太多贡献。方差的数学公式表达： $$\\delta^{2}=\\frac{(X-\\mu)^2}{N}$$ 使用 sklearn 的 VarianceThreshold 实现： from sklearn.feature_selection import VarianceThreshold data_new = VarianceThreshold(threshold=3).fit_transform(data) (2)、Pearson 相关系数选择法：计算各个特征对目标值的相关系数以及相关系数的P值，然后选择相关性最高的 k 个。Pearson 相关系数是用两个变量的协方差除以两个变量的标准差得到的，详情参考 知乎：如何理解皮尔逊相关系数 以及 维基百科：皮尔逊积矩相关系数。数学公式表达： $$P(X,Y) =\\frac{Cov(X,Y)}{\\sigma_X \\sigma_Y} =\\frac{E[(X-\\mu_{X})(Y-\\mu_{Y})]}{\\sigma_X \\sigma_Y} $$ 使用 sklearn 的 SelectKBest 与 scipy 的 pearsonr 结合实现： from sklearn.feature_selection import SelectKBest from scipy.stats import pearsonr data_new = SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(data, target) (3)、卡方检验法：经典的卡方检验（Chi-Square）是检验定性自变量对定性因变量的相关性，详情参考：卡方检验。使用 sklearn 的 SelectKBest 与 chi2 结合实现： from sklearn.feature_selection import SelectKBest from sklearn.feature_selection import chi2 X_new = SelectKBest(chi2, k=2).fit_transform(X, y) (4)、互信息法：皮尔逊系数只能衡量线性相关性，而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些。互信息是用来评价一个事件的出现对于另一个事件的出现所贡献的信息量，可用来评价定性自变量对定性因变量的相关性，详情参考特征选择方法之互信息。数学描述为： $$I(X,Y)=\\sum_{x \\in X}\\sum_{y \\in Y} p(x,y) \\log \\frac{p(x,y)}{p(x)p(y)}$$ 使用 sklearn 的 SelectKBest 与 minepy 的 MINE 结合实现： from sklearn.feature_selection import SelectKBest from minepy import MINE def mic(x, y): m = MINE() m.compute_score(x, y) return (m.mic(), 0.5) X_new = SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(X, Y) 2、Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。 (1)、递归特征消除：使用一个基模型来进行多轮训练，每轮训练后，模型选择 importance 排在前列的若干特征构建新的特征集，","date":"2018-03-17","objectID":"/2018-03-17-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/:0:0","tags":null,"title":"特征工程","uri":"/2018-03-17-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"},{"categories":["MachineLearning"],"content":"一、国外 Kaggle：最著名最活跃的大数据竞赛平台，竞赛题目源源不断，种类丰富，而且有不菲的竞赛奖金。特别是上面的论坛大牛众多而且非常 nice，如果英文能力过关的话在上面可以学到最多最好的数据挖掘和机器学习、深度学习的知识。 KDD Cup：SIGKDD 是 Data Mining 领域的顶会。KDD Cup 是其下的一个比赛。含金量很高。每年都会有比较有意思的题目。难度较大，偏学术，因而全世界的DM、ML大牛、小牛都可能在做这个。这个比赛偶尔会在国内的阿里天池平台上出现，有不少清北博士参加。 二、国内 阿里巴巴天池大数据科研平台：由国内科技巨头阿里举办，应该算是国内名气最大的数据挖掘类型的比赛了。由于是阿里举办，所以比赛获得不错名次的可以获得阿里校招直通车的权利。我参加过几个，感觉不少细节问题处理得并不是很好，例如有时候会出现赛题规则混乱，数据集因为各种问题不断更换等等，和 Kaggle 的差距还是不小。 CCF大数据与计算智能大赛（BDCI）：由中国计算机学会 CCF 主办，是目前国内权威的大数据类赛事之一，组织单位都很权威，也有很多院士、专家参与评审。 DataCastle：国内的另一个比较有名气的大数据竞赛平台，不过奖金较少。DC在成都，可能是和电子科大有某些联系，评委多是电子科大的老师。 另外国内一些IT巨头例如百度，腾讯，京东等等会自己举办一些机器学习大数据和算法竞赛，例如京东金融的 京东JData算法大赛 、京东金融全球数据探索者大赛 ，腾讯的 腾讯社交广告赛（TSA），华为的 华为软件精英挑战赛 等等，大概每年一度，因为是企业自己举办的比赛，其目的就是为了招揽人才，所以这种比赛除了奖金外，一般都会发 Special Offer，十分诱人。 三、参考 天池大数据竞赛和Kaggle、DataCastle的比较，哪个比较好？ Kaggle入门，看这一篇就够了 Kaggle 的比赛在 Machine Learning 领域中属于什么地位？ 分分钟带你杀入Kaggle Top 1% 参加天池大数据竞赛对校园招聘有帮助吗？ 国内数据挖掘竞赛哪个好？ ","date":"2018-02-04","objectID":"/2018-02-04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B1%BB%E7%AB%9E%E8%B5%9B/:0:0","tags":null,"title":"机器学习类竞赛","uri":"/2018-02-04-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%B1%BB%E7%AB%9E%E8%B5%9B/"},{"categories":["Algorithm"],"content":"在 leetcode 上看到一个巨强的解法，记录一下以加深记忆。原文参见：Easiest JAVA Solution with Graph Explanation 题目：Multiply Strings 题目大意为，给定两个字符串形式的非负整数，求它的乘积，以字符串形式返回。并且： 两个字符串的长度小于 110. 两个字符串只包含 0 到 9 的数字. 两个字符串不以任何 ‘0’ 开头. 不能使用编程语言内置的大整数库函数，也不能直接转换成整数求解. leetcode 上某大神给出的思路： 位数分别为 $m$ 和 $n$ 的两个整数相乘，乘积的位数最大不超过 $m+n$ 从左至右地，乘数的第 $[i]$ 位与被乘数的第 $[j]$ 位相乘的积，将被加到最终结果的第 $[i+j]$ 位和第 $[i+j+1]$ 位 因此，整个计算过程就可以直接模拟我们小学就学的竖式乘法计算过程。这也是令我惊讶的地方，小学就学会了的竖式乘法计算，但是十几年来（我）都没有发现这两个十分有用的规律 public static String multiply(String num1, String num2) { int m = num1.length(); int n = num2.length(); int[] pos = new int[m + n]; for (int i = m - 1; i \u003e= 0; i--) { for (int j = n - 1; j \u003e= 0; j--) { int mul = (num1.charAt(i) - '0') * (num2.charAt(j) - '0'); int p1 = i + j; int p2 = i + j + 1; int sum = mul + pos[p2]; pos[p1] += sum / 10; pos[p2] = sum % 10; } } StringBuilder sb = new StringBuilder(); for (int p : pos) if (!(sb.length() == 0 \u0026\u0026 p == 0)) sb.append(p); return sb.length() == 0 ? \"0\" : sb.toString(); } ","date":"2018-01-29","objectID":"/2018-01-29-%E7%AE%97%E6%B3%95-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%A7%E6%95%B4%E6%95%B0%E7%9B%B8%E4%B9%98/:0:0","tags":null,"title":"字符串大整数相乘","uri":"/2018-01-29-%E7%AE%97%E6%B3%95-%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%A4%A7%E6%95%B4%E6%95%B0%E7%9B%B8%E4%B9%98/"},{"categories":["Algorithm"],"content":"回溯法 （Backtracking） 是一种选优搜索法，又称为试探法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。（摘自百度百科） 此方法一般适用于求“有多少种解”的题目，如N皇后问题的“一共有多少种走法”，数独问题的“一共有多少种填法”，数组和问题的“一共有多少种组合使得数组中的某些元素之和等于目标值”等等。这种问题的特点是搜索空间很大，并且往往伴有约束条件，如N皇后问题的“每个皇后不能在同一直线上”，数独问题的“每行每列每个小九宫格都由1到9组成并且不重复”，数组和问题的“元素之和等于给定目标值”。善于利用这些约束条件，往往可以跳过大量的不可能存在解的搜索空间，极大地减少计算量。此方法也称“剪枝法”。 下面是 leetcode 的一些相关题目的解法。 一、数组和 1、Combination Sum public List\u003cList\u003cInteger\u003e\u003e combinationSum(int[] nums, int target) { List\u003cList\u003cInteger\u003e\u003e ret = new ArrayList\u003c\u003e(); Arrays.sort(nums); backtrack(ret,new ArrayList\u003c\u003e(),nums,target,0); return ret; } public void backtrack(List\u003cList\u003cInteger\u003e\u003e soluList, List\u003cInteger\u003e solu, int[] nums, int remain, int start){ if(remain\u003c0) return; else if(remain==0) soluList.add(new ArrayList(solu)); else{ for(int i=start;i\u003cnums.length;i++){ solu.add(nums[i]); backtrack(soluList,solu,nums,remain-nums[i],i); solu.remove(solu.size()-1); } } } 2、Combination Sum II public List\u003cList\u003cInteger\u003e\u003e combinationSum2(int[] nums, int target) { List\u003cList\u003cInteger\u003e\u003e ret=new ArrayList\u003c\u003e(); Arrays.sort(nums); backtrack(ret,new ArrayList\u003c\u003e(),nums,target,0); return ret; } private void backtrack(List\u003cList\u003cInteger\u003e\u003e soluList, List\u003cInteger\u003e solu, int[] nums, int remain, int start){ if(remain\u003c0) return; else if(remain==0) soluList.add(new ArrayList(solu)); else{ for(int i=start;i\u003cnums.length;i++){ if(i\u003estart\u0026\u0026nums[i-1]==nums[i]) continue; solu.add(nums[i]); backtrack(soluList,solu,nums,remain-nums[i],i+1); solu.remove(solu.size()-1); } } } 二、子集组合 1、Subsets public List\u003cList\u003cInteger\u003e\u003e subsets(int[] nums) { List\u003cList\u003cInteger\u003e\u003e soluList=new ArrayList\u003c\u003e(); backtrack(soluList, new ArrayList\u003c\u003e(), nums, 0); return soluList; } private void backtrack(List\u003cList\u003cInteger\u003e\u003e soluList, List\u003cInteger\u003e solu, int[] nums, int start){ soluList.add(new ArrayList\u003c\u003e(solu)); for(int i=start;i\u003cnums.length;i++){ solu.add(nums[i]); backtrack(soluList,solu,nums,i+1); solu.remove(solu.size()-1); } } 2、Subsets II public List\u003cList\u003cInteger\u003e\u003e subsetsWithDup(int[] nums) { List\u003cList\u003cInteger\u003e\u003e soluList=new ArrayList\u003c\u003e(); Arrays.sort(nums); backtrack(soluList,new ArrayList\u003c\u003e(),nums,0); return soluList; } private void backtrack(List\u003cList\u003cInteger\u003e\u003e soluList, List\u003cInteger\u003e solu, int[] nums, int start){ soluList.add(new ArrayList\u003c\u003e(solu)); for(int i=start;i\u003cnums.length;i++){ if(i\u003estart\u0026\u0026nums[i-1]==nums[i]) continue; solu.add(nums[i]); backtrack(soluList,solu,nums,i+1); solu.remove(solu.size()-1); } } 三、全排列 1、Permutations public List\u003cList\u003cInteger\u003e\u003e permute(int[] nums) { List\u003cList\u003cInteger\u003e\u003e soluList=new ArrayList\u003c\u003e(); backtrack(soluList,new ArrayList\u003c\u003e(),nums); return soluList; } private void backtrack(List\u003cList\u003cInteger\u003e\u003e soluList, List\u003cInteger\u003e solu, int[] nums){ if(solu.size()==nums.length) soluList.add(new ArrayList\u003c\u003e(solu)); else{ for(int i=0;i\u003cnums.length;i++){ if(solu.contains(nums[i])) continue; solu.add(nums[i]); backtrack(soluList, solu, nums); solu.remove(solu.size()-1); } } } 2、Permutations II public List\u003cList\u003cInteger\u003e\u003e permuteUnique(int[] nums) { List\u003cList\u003cInteger\u003e\u003e soluList=new ArrayList\u003c\u003e(); Arrays.sort(nums); backtrack(soluList,new ArrayList\u003c\u003e(),nums,new boolean[nums.length]); return soluList; } private void backtrack(List\u003cList\u003cInteger\u003e\u003e soluList, List\u003cInteger\u003e solu, int[] nums, boolean[] used){ if(solu.size()==nums.length) soluList.add(new ArrayList\u003c\u003e(solu)); else{ for(int i=0;i\u003cnums.length;i++){ if(used[i]) continue; if(i\u003e0\u0026\u0026nums[i-1]==nums[i]\u0026\u0026!used[i-1]) continue; used[i]=true; solu.add(nums[i]); backtrack(soluList,solu,nums,used); solu.remove(solu.size()-1); used[i]=false; } } } ","date":"2018-01-24","objectID":"/2018-01-24-%E7%AE%97%E6%B3%95-%E5%9B%9E%E6%BA%AF%E6%B3%95/:0:0","tags":null,"title":"回溯法","uri":"/2018-01-24-%E7%AE%97%E6%B3%95-%E5%9B%9E%E6%BA%AF%E6%B3%95/"},{"categories":["Algorithm"],"content":"一、原始二分查找 二分查找的思想很简单，就是在一个有序序列里查找一个目标时，先与中位数比较，如果目标等于中位数，则直接返回。如果目标小于中位数，就去前半部分继续查找。如果目标大于中位数，就去后半部分查找，如此重复直至不可再分。如果找到则返回 mid，否则返回 -1 表示序列中不存在改目标。 public int BinarySearch(int[] nums, int target) { int lo = 0, hi = nums.length - 1; while (lo \u003c= hi) { int mid = lo + (hi - lo) / 2; if (nums[mid] == target) return mid; else if (nums[mid] \u003e target) hi = mid - 1; else lo = mid + 1; } return -1; } 二、半有序数组的二分查找 进阶问题，在一个部分有序（这里只考虑升序）的旋转数组里，使用二分查找来找到最大或者最小的值。所谓旋转数组，就是一个已按升序排序的数组，循环左移或者右移若干位，例如 $[4,5,6,0,1,2,3]$。先给出寻找最小值的代码。 public int BinarySearchMinForRotateArray(int[] nums) { int lo = 0, hi = nums.length - 1; while (lo \u003c hi) { int mid = (lo + hi) / 2; if (nums[mid] \u003e nums[nums.length-1]) lo = mid + 1; else hi = mid; } return lo; } 下面是寻找最大值的代码，之前我写不出来，当数组是完全有序（也即最小值是第一个数，最大值是最后一个数）的时候，程序会死循环。后发现使用一个小技巧：mid = (lo + hi + 1) / 2 就可以解决这个问题。因为这样可以使得 mid 指针向右偏，因为数组是升序的，最大值会在最小值的左边，所以能够调整到刚好在找到最大值的时候退出循环。这里有些复杂的边界调整，暂时还没整理出一个规律。 public int BinarySearchMaxForRotateArray(int[] nums) { int lo = 0, hi = nums.length - 1; while (lo \u003c hi) { int mid = (lo + hi + 1) / 2; if (nums[mid] \u003c nums[0]) hi = mid - 1; else lo = mid; } return lo; } 三、二分查找思想实现整数除法运算 应用问题，要求不能用乘法、除法和模运算来实现一个除法。为了简单表示，这里暂不考虑小数除法和溢出的问题。思路是，通过不断把除数加倍，来逼近被除数。如果某次加倍后会超过被除数，则用被除数减去当前的除数累加和，把剩下的差作为新的逼近目标，重复调用自身来递归地逼近。 // a/b public int dividedByBinarySearch(int a, int b) { if (a \u003c b) return 0; int sum = b; int multiply = 1; while (sum + sum \u003c= a) { sum += sum; multiply += multiply; } return multiply + dividedByBinarySearch(a - sum, b); } 四、二分查找寻找指定元素的起始与结束下标 给定一个升序的整型数组，里面的元素会重复若干个，使用二分查找确定指定元素的起始和结束位置。例如，给定 [1,2,3,4,4,4,4,5]，指定目标为 4，则返回 [3,6]。思路是使用两次二分查找，分别向左和向右找最顶端的指定元素。这个例子应该能比较好地说明 mid = (lo + hi + 1) / 2 这个技巧，目的是保证 mid 指针一直等于 target。 public int[] BinarySearchRange(int[] nums, int target) { int[] ret = new int[2]; ret[0] = -1; ret[1] = -1; if (nums == null || nums.length == 0) return ret; // find the left top int lo = 0, hi = nums.length - 1; while (lo \u003c hi) { int mid = (lo + hi + 1) / 2; if (target \u003c= nums[mid]) hi = mid; else lo = mid + 1; } if (nums[lo] == target) ret[0] = lo; // find the right top lo = 0; hi = nums.length - 1; while (lo \u003c hi) { int mid = (lo + hi) / 2; if (target \u003e= nums[mid]) lo = mid; else hi = mid - 1; } if (nums[lo] == target) ret[1] = lo; return ret; } ","date":"2018-01-14","objectID":"/2018-01-14-%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/:0:0","tags":null,"title":"二分查找","uri":"/2018-01-14-%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"},{"categories":["Algorithm"],"content":"一、TwoSum 给定一个整型数组，和一个 target，返回数组中两个数之和等于 target 的所有组合。不能输出重复组合。例如给定数组 [2,3,1,-4,8,3]，target = 4，则满足条件的非重复组合有 [3,1] 和 [-4,8]。 思路：因为要求输出的是数组元素而不是数组下标，因此可以对数组进行排序（注意，如果题目要求输出数组下标，就不应使用排序，而是使用 HashMap 来优化）。排序之后，使用两个指针分别从数组两头往中间移动搜索整个数组，为避免重复组合，需跳过重复的元素。当指针相遇时搜索结束，因此搜索的时间复杂度为 $O(n)$。（其实如果算上前面的排序操作那么时间复杂度不止$O(n)$。因为所有基于比较的排序算法的时间复杂度下界为 $O(nlogn)$，所以本算法的总时间复杂度实际上应是 $O(n+nlogn)=O(nlogn)$） public List\u003cList\u003cInteger\u003e\u003e TwoSum(int[] nums, int target) { List\u003cList\u003cInteger\u003e\u003e list = new ArrayList\u003cList\u003cInteger\u003e\u003e(); Arrays.sort(nums); int lo = 0, hi = nums.length - 1; while (lo \u003c hi) { int sum = nums[lo] + nums[hi]; if (sum == target) { list.add(Arrays.asList(nums[lo], nums[hi])); while (lo \u003c hi \u0026\u0026 nums[lo] == nums[lo + 1]) lo++; while (lo \u003c hi \u0026\u0026 nums[hi] == nums[hi - 1]) hi--; lo++; hi--; } else if (sum \u003e target) hi--; else lo++; } return list; } 二、ThreeSum 上述题目的改进版，给定整型数组和一个 target，输出数组中三个数之和等于 target 的所有非重复组合。 思路：可以转化成 TwoSum 问题，即对原数组中第 $i$ 个元素 $nums[i]$，用 target 减去这个元素的差作为剩下的子数组 $nums[i+1,\\cdots,n-1]$ 的 TwoSum 问题的 target。相当与对每一个元素调用一次 TwoSum 来找剩下的两个元素，因为 TwoSum 的时间复杂度为 $O(n)$，因此 ThreeSum 的时间复杂度为 $O(n^2)$。 public List\u003cList\u003cInteger\u003e\u003e ThreeSum(int[] nums, int target) { List\u003cList\u003cInteger\u003e\u003e list = new ArrayList\u003cList\u003cInteger\u003e\u003e(); if (nums == null || nums.length \u003c 3) return list; Arrays.sort(nums); // ThreeSum for (int i = 0; i \u003c nums.length - 2; i++) { if (i == 0 || nums[i] != nums[i - 1]) { // TwoSum int lo = i + 1, hi = nums.length - 1, t = target - nums[i]; while (lo \u003c hi) { int sum = nums[lo] + nums[hi]; if (sum == t) { list.add(Arrays.asList(nums[lo], nums[hi], nums[i])); while (lo \u003c hi \u0026\u0026 nums[lo] == nums[lo + 1]) lo++; while (lo \u003c hi \u0026\u0026 nums[hi] == nums[hi - 1]) hi--; lo++; hi--; } else if (sum \u003e t) hi--; else lo++; } } } return list; } 三、FourSum 再次改进版，给定一个整型数组和一个 target，求数组中四个元素之和等于 target 的所有非重复组合。 思路：类似地，先排序，然后选定一个元素，把剩下的右边子数组看作 ThreeSum 问题。推理同上，时间复杂度为 $O(n^3)$。 public static List\u003cList\u003cInteger\u003e\u003e FourSum(int[] nums, int target) { List\u003cList\u003cInteger\u003e\u003e ret = new ArrayList\u003cList\u003cInteger\u003e\u003e(); Arrays.sort(nums); // four sum for (int i = 0; i \u003c nums.length - 3; i++) { if (i == 0 || nums[i] != nums[i - 1]) { // three sum for (int j = i + 1; j \u003c nums.length - 2; j++) { if (j == i + 1 || nums[j] != nums[j - 1]) { // two sum int lo = j + 1, hi = nums.length - 1, t = target - nums[i] - nums[j]; while (lo \u003c hi) { int sum = nums[lo] + nums[hi]; if (sum == t) { ret.add(Arrays.asList(nums[i], nums[j], nums[lo], nums[hi])); while (lo \u003c hi \u0026\u0026 nums[lo] == nums[lo + 1]) lo++; while (lo \u003c hi \u0026\u0026 nums[hi] == nums[hi - 1]) hi--; lo++; hi--; } else if (sum \u003e t) hi--; else lo++; } } } } } return ret; } ","date":"2018-01-13","objectID":"/2018-01-13-%E7%AE%97%E6%B3%95-foursum/:0:0","tags":null,"title":"FourSum","uri":"/2018-01-13-%E7%AE%97%E6%B3%95-foursum/"},{"categories":["Algorithm"],"content":"一、6种基本位操作 符号 描述 运算规则 \u0026 And（按位与） 对应位上同时为 1 时，结果才为 1，否则为 0 $\\mid$ Or（按位或） 对应位上同时为 0 时，结果才为 0，否则为 1 ~ Not（取反） 1 变 0，0 变 1 ^ Xor（异或） 对应位上相同时输出 0，相异时输出 1 « Left Shift（左移） 各二进位全部左移若干位，高位丢弃，低位补 0 \u003e\u003e Right Shift（右移） 各二进位全部右移若干位，低位丢弃，高位分情况 注： 计算机内部使用补码作为机器数，正数的补码等于原码，负数的补码等于原码的符号位外取反加一，切记。另外注意机器字长，如果题目没有明确说明，一般默认是32位或64位 这6种操作符中，只有~取反是单目操作符，其它5种都是双目操作符 位操作只能用于整型数据 位操作运算优先级比较低（比加减乘除低），因此一般需要使用括号来保证正确的运算顺序，否则容易出错 右移运算（\u003e\u003e）时，对无符号数，高位补0。对有符号数，各编译器处理方法不一样，有的补符号位（算术右移），有的补0（逻辑右移） Java中的右移（\u003e\u003e）执行的是算术右移，高位补符号位；Java中的无符号（逻辑）右移有单独的运算符号：\u003e» 另外位操作还有一些复合操作符，如 \u0026=、|=、^=、«=、\u003e\u003e= 等 二、常用技巧 1、计算给定的整数对应的二进制数中 1 的个数。 public int countOnes(int n) { int count=0; while(n!=0) { n = n\u0026(n-1); count++; } return count; } 2、判断一个整数是否是 4 的 n 次方。思路很清晰，4 的 n 次方即为 2 的 2n 次方，因此其二进制形式必定是由一个 1 和偶数个 0 组成。 public boolean isPowerOfFour(int num) { int count0=0; int count1=0; while(num\u003e0){ if((num\u00261)==1){ count1++; }else{ count0++; } num\u003e\u003e=1; } return count1==1 \u0026\u0026 (count0%2==0); } 3、仅用位运算求两个整数的和。 public int getSum(int a, int b) { //return b==0? a:getSum(a^b, (a\u0026b)\u003c\u003c1); while(b!=0){ int temp=a^b; b=(a\u0026b)\u003c\u003c1; a=temp; } return a; } 4、找出缺失的数。给定一个整型数组，每个数字都出现了两次，现在缺失了一个数字，即有一个数字只出现一次，找出这个数字。思路：利用异或运算的两个特性， 自己与自己异或结果为0，异或满足交换律 public int missingNumber(int[] nums) { int ret = 0; for (int i = 0; i \u003c nums.length; ++i) { ret ^= nums[i]; } return ret; } 5、找出缺失的数。给定一个整型数组，长度为 $n$，取值范围为 $[0,…,n]$。找出缺失的那一个数。例如给定 $[0,1,3]$，返回 2。思路同上，把数组的 n 个数和完整的序列 $[0,…,n]$ 异或起来，最后结果就是只出现一次的也即在数组中缺失的那个数。 public static int missingNumber(int[] nums) { int ret = 0; for (int i = 0; i \u003c nums.length; ++i) { ret ^= i; ret ^= nums[i]; } return ret ^ nums.length; } 6、判断奇偶。只要根据最未位是 0 还是 1 来决定，为 0 就是偶数，为 1 就是奇数。因此可以用 if((a \u0026 1) == 0) 代替 if(a % 2 == 0) 来判断 a 是不是偶数。 7、不使用第三方变量交换两个数。利用一个数异或上自己等于 0 （消去这个数）的性质。 public void swap(int a, int b) { if (a != b) { a ^= b; b ^= a; a ^= b; } } 8、变换符号，即最高位 1 变 0，0 变 1。把该数取反加 1 即可。 public int signReversal(int a) { return ~a + 1; } 9、取绝对值。由上述可知对于负数可以通过对其取反加一来得到正数。因此现在只要判断输入的数是正数还是负数，如果是正数就直接输出，是负数就取反加一。对输入的数算术右移31位，由于高位补的是符号位，因此右移后如果原来是正数那么结果就是 0（0x 0000 0000），如果是负数那么右移的结果是 -1（0x FFFF FFFF）。 public int abs(int a) { int i = a \u003e\u003e 31; return i == 0 ? a : (~a + 1); } 如果进一步考虑，注意到任何数与 0 异或等于自己，与 -1 异或等于取反，而代码中的 i 要么是 0，要么是 -1，因此 a 与 i 异或再减去 i 和原代码等价，并且这样省去了判断。 public int abs(int a) { int i = a \u003e\u003e 31; return (a ^ i) - i; } 10、逆序一个二进制数。这里只关注正整数的有效位，例如，正整数 11 的二进制是 1011，逆序后得到 1101，输出 13。思路是对输入的数不断地取最低位并右移，取到的位赋到输出结果的最低位并不断左移。当输入的数减小到 0 时，算法完成。 public int reverseBit(int n) { int mask = 1, res = 0; while (n != 0) { res \u003c\u003c= 1; res |= (n \u0026 mask); n \u003e\u003e= 1; } return res; } 11、逆序一个 32 位无符号整数的二进制。我们知道机器码是最高位是用于表示符号位的，但是 Java 中没有无符号整数的数据类型，怎办呢？ 三、参考资料 位操作基础篇之位操作全面总结 leetcode-A summary: how to use bit manipulation to solve problems easily and efficiently Stack Overflow-Reverse bits in number Stack Overflow-How can I invert bits of an unsigned byte in Java? ","date":"2018-01-06","objectID":"/2018-01-06-%E7%AE%97%E6%B3%95-%E4%BD%8D%E8%BF%90%E7%AE%97%E6%8A%80%E5%B7%A7/:0:0","tags":null,"title":"位运算技巧","uri":"/2018-01-06-%E7%AE%97%E6%B3%95-%E4%BD%8D%E8%BF%90%E7%AE%97%E6%8A%80%E5%B7%A7/"},{"categories":["Algorithm"],"content":"一、机器数（Machine Data）与真值（True Value） 一个数在计算机中的二进制表示形式，叫做这个数的机器数。我们知道，计算机的物理结构决定了它内部只能存储 0 和 1，也即二进制数，那么计算机如何区分表示一个数的正负呢？一般上，计算机用机器数的最高位存放符号，如果最高位是 0 则表示这个数是一个正数。如果最高位是 1 则表示这个数是一个负数。假设计算机字长为 8 位（现在的计算机一般32位和64位），则 +3 转换成二进制机器数就是 0000 0011，-3 转换成二进制机器数是 1000 0011。 反过来，我们把当一个二进制数作为机器数出现时，它所代表的真实数值为机器数的真值。例如，机器数 0000 0011 的真值为 +000 0001 = +3，机器数 1000 0001 的真值为 –000 0011 = –3。 下面分别介绍原码，反码，补码，它们实际上就是机器数的三种不同的编码策略。其中，适合人脑理解和计算的是原码，而适合计算机使用的是补码。 二、原码（Primitive Encoding） 原码就是直接使用符号位和真值的绝对值的二进制数作为机器码。例如，计算机字长为 8 位时， $$[+3]_{原}=0000 , 0011$$ $$[-3]_{原}=1000 , 0011$$ 由于原码最高位用于表示符号，因此原码可以表示的最大正数为 0111 1111 即 +127，最小负数为 1111 1111 即 -127，因此 8 位字长的原码机器码的取值范围为 [-127, 127]。 三、反码（Invert Encoding） 反码的定义为：正数的反码等于它的原码，负数的反码等于其原码的符号位不变，其余各位取反。例如，计算机字长为 8 位时， $$[+3]_{反}=0000 , 0011$$ $$[-3]_{反}=1111 , 1100$$ 四、补码（Complement Encoding） 补码的定义为：正数的补码等于它的原码，负数的补码等于其原码的符号位不变，其余各位取反，再加一。例如，计算机字长为 8 位时， $$[+3]_{补}=0000 , 0011$$ $$[-3]_{补}=1111 , 1101$$ 五、为什么需要原码、反码、补码三种编码策略 由上面知道，对于一个正数，它的原码、反码和补码的表示是完全一样的。而对于一个负数，它的三种编码策略完全不一样。例如，计算机字长为 8 位时， $$[+3]{真值}=[0000,0011]{原}=[0000,0011]{反}=[0000,0011]{补}$$ $$[-3]{真值}=[1000,0011]{原}=[1111,1100]{反}=[1111,1101]{补}$$ 适合人脑直观理解的是原码，因为它就是以真值的绝对值的二进制加上一个符号位来表示，但是这样却不适合于计算机的计算。因为如果单独处理计算机中每一个机器数的符号位，会使得电路设计非常复杂。于是需要一种可以直接让符号位直接参与计算并且保证结果正确的编码方式，这就是补码被提出的原因。补码的推导具体参见： 原码, 反码, 补码 详解 机器数与编码 目前计算机内部使用的一般都是补码，切记。 ","date":"2017-12-31","objectID":"/2017-12-31-%E7%AE%97%E6%B3%95-%E6%9C%BA%E5%99%A8%E6%95%B0%E7%9A%84%E4%B8%89%E7%A7%8D%E7%BC%96%E7%A0%81%E7%AD%96%E7%95%A5/:0:0","tags":null,"title":"机器数的三种编码策略","uri":"/2017-12-31-%E7%AE%97%E6%B3%95-%E6%9C%BA%E5%99%A8%E6%95%B0%E7%9A%84%E4%B8%89%E7%A7%8D%E7%BC%96%E7%A0%81%E7%AD%96%E7%95%A5/"},{"categories":["MachineLearning"],"content":"一、梯度提升树 因为做比赛最近用到了 LightGBM 来实现逻辑回归，并尝试修改 LightGBM 的目标函数，于是顺便温习一下梯度提升树和逻辑回归，并简单推导了一下逻辑回归在梯度提升树中的损失函数的梯度计算。 前面的博文介绍过，当损失函数是平方损失和指数损失时，前向加法模型的提升树（回归提升树）可以很方便地进行目标函数的优化并构造决策树。但对于一般的损失函数，前向加法提升树就比较难实现优化。这时就需要使用梯度提升模型的提升树，它能适应一般的损失函数。类似最速下降法，梯度提升树利用损失函数的负梯度在当前模型的值 $$- \\left[ \\frac{\\partial L(y,f(x_i))}{\\partial f(x_i)} \\right]$$ 作为回归提升树算法中的残差的近似值，来拟合一个回归树。 二、逻辑回归 逻辑回归是比较简单而且应用非常广泛的机器学习算法。它主要用于二分类任务，但是与一般的分类器不同，逻辑回归并不直接给出未知样本的预测类别，而是给出属于某个类别的概率。 对一个二分类任务，每个训练样本都属于且只属于两种类别之中的一种，即要么是正样本，要么是负样本。习惯上，一般用 $0$ 表示负样本，用 $1$ 表示正样本。 设训练集一共有 $m$ 个样本，每个样本有 $n$ 个属性，第 $i$ 个训练样本表示为 $x_i=(x_{i}^{(1)},x_{i}^{(2)},\\cdots,x_{i}^{(n)})$，$1 \\leq i \\leq m$。对应地，每个样本有一个类别，设为 $Y$，则对第 $i$ 个样本，要么 $Y_i=1$，要么 $Y_i=0$。 现在的任务是，给定 $m$ 个已知对应类别的样本 ${(x_1,Y_1),(x_2,Y_2),\\cdots,(x_m,Y_m)}$，用来作为训练集，学习一个分类器模型 $f(x,Y)$，然后用这个模型来对未知类别的样本如 $(x_{m+1},？)$ 进行预测，预测它的类别 $Y_{m+1}$ 是等于 $1$ 还是等于 $0$。 逻辑回归使用 $sigmod$ 函数作为预测模型，它不直接判定某个样本是正样本还是负样本，而是给出一个条件概率，即在已知样本的 $n$ 个属性的情况下，该样本属于正样本或负样本的概率。数学描述为 $$P(Y=1 \\mid x)=\\frac{1}{1+e^{-w \\cdot x}}$$ $$P(Y=0 \\mid x)=1-\\frac{1}{1+e^{-w \\cdot x}}$$ 其中 $w$ 是 $n$ 维的向量，每一维对应样本 $x$ 的 $n$ 个特征，可以理解为：$w^{(i)}$ 表示训练样本 $x$ 第 $i$ 个特征 $x^{(i)}$ 的权重。$w \\cdot x$ 为两个向量的内积，即 $$w \\cdot x=w^{(1)}*x^{(1)}+w^{(2)}*x^{(2)}+\\cdots+w^{(n)}*x^{(n)}$$ 如果给定了训练集，那么模型训练的任务就是学习上面的权重参数 $w$，如果找到某个确定的向量 $w=w^{*}$ 能把训练样本最正确地分类，那么它就是我们要找的那个最优解。确定这个参数后，模型也就完全确定了。当使用该模型执行预测任务时，只需要把未知样本输入这个模型，即可得出未知样本属于正样本或负样本的概率。 那么我们如何找到这个最优的向量 $w^{*}$ ？对于逻辑回归，因为它是概率问题，所以一般使用极大似然估计法来估计模型参数。 极大似然估计法思路：“存在即合理”，使得每个已知样本（即训练样本）出现的概率最大的参数 $w^{*}$ 就是最优的参数。设给定的训练集为 $T={(x_1,Y_1),(x_2,Y_2),\\cdots,(x_m,Y_m)}$，其中 $x_i \\in R^n$，$Y_i \\in {0,1}$，由逻辑回归的定义知每个样本（作为正样本或负样本）出现的概率为 $$ P(x_i)= \\begin{cases} P(Y_i=1 \\mid x_i)=\\frac{1}{1+e^{-w \\cdot x_i}} \\\\ P(Y_i=0 \\mid x_i)=1-\\frac{1}{1+e^{-w \\cdot x_i}} \\end{cases} $$ 有了每个样本的概率 $P(x_i)$，现在只要找到一个 $w=w^{*}$ 使得所有样本的概率同时最大，应该怎么做呢？思路很简单，因为目标是使它们同时最大，因此把它们全部乘起来，得到一个关于 $w$ 的函数，只要使这个函数的值最大，那就相当于使这些概率同时最大了。这个函数称为似然函数，设为 $L(w)$，即 $$L(w)=P(x_1) * P(x_2) * \\cdots * P(x_m)=\\prod_{i=1}^{m} P(x_i)$$ 到这里其实已经基本完成学习任务的数学定义了。但是为了能让机器执行学习任务，还需要解决两个小问题，一是公式的推导化简，二是使用什么方法搜索 $w$ 的解空间。 对于第一个问题，当你尝试把 $P(x_i)$ 的公式代入似然函数 $L(w)$ 时，你就会发现非常困难，因为 $P(x_i)$ 公式针对 $Y_i$ 分了两种情况。这里用到了一个小技巧，可以把两种情况的公式在形式上统一到一个公式。如果你注意到，对于每个样本的类别属性 $Y_i$，它要么是 $1$ 要么是 $0$ ，那么就可以利用这个性质，把两种情况统一写成： $$ \\begin{aligned} P(x_i) \u0026=P(Y_i=1 \\mid x_i)^{Y_i} * P(Y_i=0 \\mid x_i)^{1-Y_i} \\\\ \u0026=(\\frac{1}{1+e^{-w \\cdot x_i}})^{Y_i} * (1-\\frac{1}{1+e^{-w \\cdot x_i}})^{1-Y_i} \\end{aligned} $$ 这个形式的 $P(x_i)$ 和上面的原始定义是完全等价的。你可以发现，当 $Y_i=1$ 时，乘号右边的部分因为指数为 $1-Y_i=0$ 就变成了 $1$ 而只剩下左边的部分，这就相当于当 $Y_i=1$ 自动时把 $P(Y_i=1 \\mid x_i)$ 这部分选择出来了，反之亦然。 有了如此统一的 $P(x_i)$ 的表达形式，就可以代入似然函数得到 $$L(w)=\\prod_{i=1}^{m} P(x_i)=\\prod_{i=1}^{m} \\left[ (\\frac{1}{1+e^{-w \\cdot x_i}})^{Y_i} * (1-\\frac{1}{1+e^{-w \\cdot x_i}})^{1-Y_i} \\right]$$ 如果进一步考虑到，因为 $P(x_i)$ 是一个概率，如果样本数目太多，大量的小数连乘对于机器来说容易出现下溢等等的精度问题，为避免这些实际问题一般对似然函数取对数，把连乘变成连加，从而得到对数似然函数 $$\\log L(w)=\\sum_{i=1}^{m} \\left[ Y_i*\\log(\\frac{1}{1+e^{-w \\cdot x_i}})+(1-Y_i)*(1-\\frac{1}{1+e^{-w \\cdot x_i}}) \\right]$$ 现在只要让机器找到一个 $w=w^{*}$ 使得 $\\log L(w)$ 最大，也即极大值，那么这就是最优的解。对第二个问题，一般常用的方法是梯度下降法或者拟牛顿法来搜索 $w$ 的解空间，这里限于篇幅不再展开。 三、逻辑回归之于梯度提升树 现在有了逻辑回归的目标函数，就可以推导出逻辑回归的损失函数在梯度提升树中的每一轮训练时的负梯度的值。 首先为了在形式上与第一节中的梯度提升树的负梯度公式保持统一，我们把上面逻辑回归的目标函数 $\\log L(w)$ 写成关于每一个样本的损失函数的形式 $$L(Y_i,P(x_i))=Y_i*\\log(\\frac{1}{1+e^{-w \\cdot x_i}})+(1-Y_i)*(1-\\frac{1}{1+e^{-w \\cdot x_i}})$$ 为了在接下来的推导过程简便表达，先设 $$z=\\frac{1}{1+e^{-w \\cdot x_i}}, \\quad 1-z=1-\\frac{1}{1+e^{-w \\cdot x_i}}$$ 为了简洁过程在这里我们把 $-w*x_i$ 看作一个整体的变量即可，现在分别对 $z$ 和 $1-z$ 求导，有 $$z'=\\frac{e^{-w \\cdot x_i}}{(1+e^{-w \\cdot x_i})^2}, \\quad (1-z)'=-\\frac{e^{-w \\cdot x_i}}{(1+e^{-w \\cdot x_i})^2} $$ 因此梯度提升树在第 $i$ 个样本处的负梯度（一阶导）： $$ \\begin{aligned} -\\frac{\\partial L(Y_i,P(x_i))}{\\partial P(x_i)} \u0026= -\\left[ Y_i * \\log(z) + (1-Y_i) * \\log(1-z) \\right]' \\\\ \u0026= -\\left[ Y_i * \\frac{1}{z} * z' + (1-Y_i) * \\frac{1}{1-z} * (1-z)' \\right]\\\\ \u0026= -\\left[ Y_i * (1+e^{-w \\cdot x}) * \\frac{e^{-w \\cdot x_i}}{(1+e^{-w \\cdot x_i})^2} + (1-Y_i) * \\frac{1+e^{-w \\cdot x}}{e^{-w \\cdot x}} * -\\frac{e^{-w \\cdot x_i}}{(1+e^{-w \\","date":"2017-12-12","objectID":"/2017-12-12-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9C%A8%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E4%B8%AD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC/:0:0","tags":null,"title":"逻辑回归在梯度提升树中损失函数的梯度推导","uri":"/2017-12-12-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E5%9C%A8%E6%A2%AF%E5%BA%A6%E6%8F%90%E5%8D%87%E6%A0%91%E4%B8%AD%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E7%9A%84%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC/"},{"categories":["MachineLearning"],"content":"一、随机森林（RandomForest） 前面学习过了 bagging 方法，我们知道该方法是通过有放回随机抽样来构建 $S$ 个和原数据集大小相等的训练集，然后分别进行训练，得到 $S$ 个分类器，再把这些分类器通过多数表决的方式组合得到最终的分类器。 随机森林算法正是 bagging 方法的一种扩展。随机森林算法不仅对训练集的样本（行）进行抽样，而且对训练集的特征（列）也进行抽样。具体来说，随机森林分为三个步骤： 1、随机采样 对于行采样，使用有放回随机采样，和常规 bagging 方法一致，前面已经介绍过，此处不再赘述。 对于列采样，假设样本有 $M$ 个特征（即训练集有 $M$ 列），那么该算法在每次决策树的节点要分裂的时候，随机选择其中 $m$ 个特征作为考察的特征子集（即只在这 $m$ 个特征之中选出最优特征作为分裂决策，而不是像传统决策树那样考察全部特征）。满足 $m « M$ ，即每次随机选择的子特征集的大小应远小于总特征集的大小，一般情况下推荐取 $m=log_2(M)$。 行采样的目的是 “用有限数据模拟无限数据”，而列采样的目的是使每个决策树专注于一小部分特征的学习，使其成为各自的 “窄领域专家”，当最终把这些 “擅长于不同领域的专家” 组合到一起时，就可以大大减少 “所有专家犯同样错误” 的可能，也即过拟合的可能。 2、完全分裂 因为有了上一步采样的过程，最终分类器的过拟合现象基本不可能发生，因此在学习各个决策树的时候就按照完全分裂的方式来构造，无须剪枝。如在执行分类任务时，分裂的决策依据就可以选择常规决策树的生成算法的决策依据，如 ID3 算法的信息增益等。 3、执行决策 当学习完成，得到 $S$ 个彼此独立的决策树后，就可以把这些决策树组合在一起，作为最终的分类器。组合的方式是常规 bagging 方式，即在对预测输出进行结合时，让各个分类器分别执行预测，得到 $S$ 个预测结果，如果预测任务是分类任务则使用投票法选择票数最多的那个类别返回，如果是回归任务则使用均值法取这些预测结果的均值返回。 二、梯度提升树（Gradient boosting decision tree，GBDT） 梯度提升树是 boosting 提升方法中的一种。它的提出是为了解决 “回归提升树在使用一般损失函数的时候，求解目标函数时每一步的优化比较困难” 这一问题。之前学习的回归提升树在使用前向分步算法求解目标函数时，使用的损失函数是指数函数，每一步的优化很简单。但如果要扩展到一般的损失函数，就不那么容易了。因此 Freidman 提出了梯度提升（gradient boosting）算法，利用最速下降法的近似方法，其关键是利用损失函数的负梯度在当前模型的值作为回归提升树中的残差的近似值，来拟合一个回归树。 梯度提升算法： 输入：训练数据集 $T$，输入空间 $X$，输出空间 $Y$，损失函数 $L(y,f(x))$ 输出：回归提升树 $f_M(x)$ 1、初始化 $$f_0(x)=argmin\\sum_{i=1}^{N}L(y_i,c)$$ 2、对 $M$ 个分类器，进行对应的第 $m=1,2,\\cdots,M$ 轮学习。对第 $m$ 轮学习： (1)、计算 $$r_{mi}=-\\left[ \\frac{\\partial L(y_i,f(x_i))}{\\partial f(x_i)} \\right]_{f(x)=f_{m-1}(x)}$$ (2)、拟合 $r_{mi}$，得到第 $m$ 个回归树的叶结点区域 $R_{mj}$，$j=1,2,\\cdots,J$ (3)、对 $j=1,2,\\cdots,J$，计算 $$c_{mj}=argmin\\sum_{x_i \\in R_{mj}} L(y_i,f(x_i)+c)$$ (4)、更新 $$f_m(x)=f_{m-1}(x)+\\sum_{j=1}^{J}c_{mj}I(x \\in R_{mj})$$ 3、执行完 $M$ 轮学习后，得到最终的回归提升树 $$f_M(x)=\\sum_{m=1}^{M}\\sum_{j=1}^{J}c_{mj}I(x \\in R_{mj})$$ 目前比较流行的 GBDT 算法实现有两个，分别是陈天奇的 xgboost（eXtreme Gradient Boosting） 和 微软的 lightGBM。关于两者的详细分析对比看这里。 参考： 机器学习中的算法：决策树模型组合之随机森林（Random Forest） 随机森林的原理分析及Python代码实现 《统计学习方法》李航 GBDT：梯度提升决策树 XGBoost, LightGBM性能大对比 《机器学习》周志华 ","date":"2017-12-05","objectID":"/2017-12-05-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-randomforest%E4%B8%8Egbdt/:0:0","tags":null,"title":"RandomForest与GBDT","uri":"/2017-12-05-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-randomforest%E4%B8%8Egbdt/"},{"categories":["MachineLearning"],"content":"一、boosting 前面已经学习过，boosting 是一种提升方法，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行组合，提高分类的性能。 boosting 方法每一轮学习一个分类器，并根据本次学习的误差，改变整个样本集合中每个样本的权重，被误分类的那些样本的权重将增大。在下一轮学习新的分类器时，这些被误分类的样本将会被赋予更大的关注。 因此可以看出，boosting 方法是通过串行训练而获得的，下一轮的学习是基于上一轮的误差进行的。另外，每一轮的训练集都是整个原数据集，数据集中的样本不变，变的是各个样本的权重。 最后，在学习基本分类器时，会同时计算得到每个分类器的权重。boosting 的最终组合分类器是这一系列基本分类器的加权组合。 目前比较典型的两种 boosting 算法是 Adaboosting （Adaptive Boosting，自适应boosting）算法，和 GBDT（gradient boosting decision tree，梯度提升决策树）算法。 二、bagging 自举汇聚法（bootstrap aggregating），也称 bagging 方法。该方法通过从原数据集中随机放回抽样，得到 $S$ 个和原数据集大小相等的数据集，来作为 $S$ 次训练的训练集，从而训练得到 $S$ 个分类器。 因为是放回抽样，所以新的数据集中可以有重复的样本，原数据集也可以有部分样本不在新数据集中出现。 得到了 $S$ 个分类器后，就将这些分类器组合成最终的分类器。执行预测时，让这 $S$ 个分类器分别对新数据进行预测，得到 $S$ 个预测结果。如果是分类任务，则采用多数表决的方式，选择这 $S$ 个结果中票数最多的那个类别返回。如果是回归任务，则取均值作为最终结果返回。 目前比较典型的 bagging 类的方法有随机森林（Random Forest）。 三、区别 1、学习方式 boosting 的每一轮学习是基于前一轮的误差，因此它的一系列基本分类器的训练是串行的。bagging 是随机不放回抽样得到若干个新的训练集进行各自的训练，彼此间没有联系，可并行。 2、训练样本 boosting 不改变样本，而是在每一轮根据误差改变每个样本的权重。bagging 不改变权重，而是从原数据集抽选其中一部分样本（可重复）来构成不同的训练集。 3、分类器组合方式 boosting 的最终分类器是基本分类器的加权之和，每个分类器有各自的权重，最终的预测结果是每个分类器的预测结果的加权之和。bagging 的分类器没有权重的概念，每个基本分类器都有相等权重的一票，最终的预测结果是票数最多的那一个类（标签）。 ","date":"2017-11-30","objectID":"/2017-11-30-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-boosting%E4%B8%8Ebagging/:0:0","tags":null,"title":"boosting与bagging","uri":"/2017-11-30-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-boosting%E4%B8%8Ebagging/"},{"categories":["MachineLearning"],"content":"一、提升树（boosting tree） 提升树是以分类树或回归树为基本分类器的提升方法。提升树被认为是统计学习中性能最好的方法之一。提升方法采用加法模型（即基函数的线性组合）与前向分步算法。对分类问题，决策树是二叉分类树；对回归问题，决策树是二叉回归树。 提升树模型可以表示为决策树的加法模型： $$f_M(x)=\\sum_{m=1}^{M}T(x;\\theta_m)$$ 其中，$T(x;\\theta_m)$ 表示决策树，$\\theta_m$ 为决策树的参数，$M$ 为决策树的个数。 二、二分类问题的提升树 对于二分类问题，提升树算法只需将 Adaboost 算法中的基本分类器限制为二分类树即可。 目前比较流行的方案是采用一种简单决策树作为基本分类器：单层决策树（decision stump，也称决策树桩）。它仅仅基于单个特征来执行决策，因此对应地只有一个树结点：根节点。它只能执行一次二分类，如 $x \u003c v$ 或 $x \u003e v$。 三、回归问题的提升树 回归树的形式化描述：设训练数据集 $T={(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，$x_i \\in X \\subseteq R^n$，$y_i \\in Y \\subseteq R$，$X$ 是输入空间，$Y$ 是输出空间。如果将输入空间 $X$ 划分为 $J$ 个不相交的区域 $R_1,R_2,\\cdots,R_J$，并且在每个区域上有确定的输出常量 $c_j$，那么回归树可以表示为： $$T(x;\\theta)=\\sum_{j=1}^{J}c_j \\cdot I(x \\in R_j)$$ 其中，参数 $\\theta={(R_1,c_1),(R_2,c_2),\\cdots,(R_J,c_J)}$ 表示回归树的区域划分和各区域上的输出常量（即输出的回归值），$J$ 是回归树的复杂度（即叶结点个数）。 回归问题的提升树算法： 输入：如上，训练数据集 $T$，输入空间 $X$，输出空间 $Y$ 输出：回归提升树 $f_M(x)$ 1、初始化 $f_0(x)=0$ 2、对 $M$ 个分类器，进行对应的第 $m=1,2,\\cdots,M$ 轮学习。对第 $m$ 轮学习： (1)、计算以前一轮为止，目前学习到的回归提升树 $f_{m-1}(x)$ 的残差 $$r_{mi}=y_i-f_{m-1}(x_i)，i=1,2,\\cdots,N$$ (2)、拟合残差 $r_{mi}$ 学习得到第 $m$ 个回归树 $T(x;\\theta_m)$ (3)、把该回归树加入 $f_{m-1}(x)$，得到新的回归提升树 $$f_m(x)=f_{m-1}(x)+T(x;\\theta_m)$$ 3、执行完 $M$ 轮学习后，得到最终的回归提升树 $$f_M(x)=\\sum_{m=1}^{M}T(x;\\theta_m)$$ ","date":"2017-11-26","objectID":"/2017-11-26-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E6%8F%90%E5%8D%87%E6%A0%91%E4%B8%8E%E5%9B%9E%E5%BD%92%E6%8F%90%E5%8D%87%E6%A0%91/:0:0","tags":null,"title":"分类提升树与回归提升树","uri":"/2017-11-26-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E5%88%86%E7%B1%BB%E6%8F%90%E5%8D%87%E6%A0%91%E4%B8%8E%E5%9B%9E%E5%BD%92%E6%8F%90%E5%8D%87%E6%A0%91/"},{"categories":["MachineLearning"],"content":"一、提升方法（boosting） 提升方法主要用于分类问题，它的基本思想是，通过改变训练样本的权重，学习多个不同的分类器，最后把这些基本分类器（也称弱分类器）通过线性组合，得到最终的强分类器。 这里所谓的提升，通俗地说其实就是将一个分类问题交给多个分类器来处理，“对于一个复杂任务来说，将多个专家的判断进行适当的综合，得出的最终判断，要比任何一个专家单独的判断要好”。虽然每一个弱分类器都是只是一个窄领域的专家，但是把一系列这样的弱分类组合到一起，得到的分类能力并不比单个强分类器差。而且直接学习一个强分类器远比学习一个弱分类器难。 目前大多数提升方法是通过不断改变训练数据的概率分布（样本权重分布）来不断学习出一系列弱分类器。那么这样需要确定两个问题： 在每一轮如何改变训练样本的权值或者概率分布 如何将这些弱分类器组合成一个强分类器 提升方法中的代表性算法有 Adaboost 算法，它的做法是： 在每一轮学习后，提高前一轮学习中被错误分类的样本的权值，降低前一轮学习中被正确分类的样本的权值。这样，后面学习的分类器将会专注于前面的分类器不能很好处理的那些样本，弥补了前面的分类器的不足（窄领域）。举个例子，小明和小红在学习分类一堆水果，小明先学习，他在学习分类的时候由于精力和时间有限，在一轮学习后，小明只能很好地对体积大的水果进行分类，对那些体积小的水果经常分错类。然后到小红学习的时候，为了保证两个人最终能够把这堆水果都正确分类，那么聪明的小红应该知道，自己应该专注于对那些小明不擅长的体积小的水果进行学习，因为如果小明能够很好地分类体积大的水果，而自己能够很好地分类体积小的水果，那么两个人组合互补起来得到的分类能力，远比两个人都强行学习对所有水果分类的效果好得多。 对于弱分类器的组合，Adaboost 采取加权多数表决的方法，即加大分类误差小的弱分类器的权值，使其在最终表决的时候起较大作用，减小分类误差较大的弱分类器的权值，使其在最终表决中起较小作用。这个很容易理解，谁分类的误差小、准确度高，当然谁在最终的表决里就有更大的话语权了。 二、Adaboost 算法 算法目标：给定一个二分类的训练数据集 $T={(x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N)}$，其中 $x_i \\in R^n$ 是样本实例，$y_i \\in {-1,+1}$ 是样本类别标记。本算法目标是从该训练数据集中，学习出 $M$ 个弱分类器，最后将这些弱分类器通过线性组合，得到最终的一个强分类器。 输入：训练数据集 $T$，弱学习算法 输出：最终分类器 $G(x)$ 1、初始化训练集 $N$ 个样本的权重分布 $$D_1=(w_{1,1},\\cdots,w_{1,i},\\cdots,w_{1,N})$$ $$w_{1,i}=\\frac{1}{N}，i=1,2,\\cdots,N$$ 2、对 $M$ 个分类器，开展对应的 $m=1,2,\\cdots,M$ 轮学习。对第 $m$ 轮学习： (1)、用训练数据集 $T$、第 $m$ 轮的样本权重 $D_m$、和弱分类器学习算法，学习得到第 $m$ 个弱分类器 $$G_m(x):X \\to {-1,+1}$$ (2)、计算 $G_m(x)$ 在训练集上的分类误差率 $$e_m=P(G_m(x_i) \\neq y_i)=\\sum_{i=1}^{N}w_{m,i}I(G_m(x_i) \\neq y_i)$$ (3)、计算 $G_m(x)$ 的权重 $$\\alpha_m=\\frac{1}{2}\\ln(\\frac{1}{e_m}-1)$$ (4)、计算下一轮学习的样本权重分布 $$D_{m+1}=(w_{m+1,1},\\cdots,w_{m+1,i},\\cdots,w_{m+1,N})$$ $$ w_{m+1,i}=\\frac {w_{m,i} \\cdot e^{-\\alpha_m y_i G_m(x_i)}} {\\sum_{i=1}^{N} w_{m,i} \\cdot e^{-\\alpha_m y_i G_m(x_i)}} ，i=1,2,\\cdots,N $$ 3、执行完 $M$ 轮学习后，得到 $M$ 个弱分类器 $G_1(x),G_2(x),\\cdots,G_M(x)$，通过线性组合得到最终表决 $$f(x)=\\sum_{m=1}^{M} \\alpha_m \\cdot G_m(x)$$ 加上符号函数 $sign()$，从而得到最终分类器 $$G(x)=sign(f(x))=sign \\left( \\sum_{m=1}^{M} \\alpha_m \\cdot G_m(x) \\right)$$ 三、Adaboost 的解释 Adaboost 算法是模型为加法模型、损失函数为指数函数、学习算法为前向分步算法时的二分类学习方法。 1、加法模型（additive model）： $$f(x)=\\sum_{m=1}^{M} \\beta_m \\cdot b(x;\\gamma_m)$$ 其中 $b(x;\\gamma_m)$ 为基函数，$\\gamma_m$ 为基函数的参数，$\\beta_m$ 为基函数的系数。 2、前向分步算法 具体参见《统计学习方法》，这里直接给出结论：由前向分步算法可以推导出 Adaboost 算法，Adaboost 算法是前向分步算法的一个特例。以后有时间再回来补这部分推导。 3、Adaboost 的训练误差 Adaboost 的训练误差可以证明是指数下降的。 ","date":"2017-11-22","objectID":"/2017-11-22-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-adaboost%E7%AE%97%E6%B3%95/:0:0","tags":null,"title":"Adaboost算法","uri":"/2017-11-22-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-adaboost%E7%AE%97%E6%B3%95/"},{"categories":["MachineLearning"],"content":"今天和队友讨论好准备做交叉验证的时候，我们竟然都搞错了对交叉验证的理解，回过头来看书的时候发现是竟然因为搞错了一些最基本的但十分重要的知识，特此记录一下。 一、交叉验证（cross validation） 机器学习中的交叉验证是一种常用的模型选择的方法。交叉验证的基本思想是重复地使用数据，把给定的数据集进行切分，分成训练集和测试集，在此基础上反复地进行训练、测试，并选择最好的模型。 1、简单交叉验证 随机地将已给数据集分为两部分，约 70% 作为训练集，剩下 30% 作为测试集。然后用训练集在不同的参数条件下训练模型，得到各个参数不同的模型，接着在测试集上评价模型的测试误差，选出测试集误差最小的模型。 这种方法只把训练集和测试集做一次划分。 2、$S$ 折交叉验证 目前应用最多的是 $S$ 折交叉验证，首先随机地将已给数据切分成 $S$ 个互不相交的大小相同的子集，然后利用其中任意 $S-1$ 个子集作为训练集训练得到一个模型，把剩下的那一个子集作为测试集测试模型得到一个测试误差。如此重复 $S$ 次，直到每一个子集都作为一次测试集，一共得出 $S$ 个不同的模型，和 $S$ 个对应的测试误差。最后，选择出这 $S$ 个测试误差中最小的那个模型，作为最终确定的最优模型。 到这里，我就不明白，为什么这样做可以避免过拟合？选择测试误差最小的那个模型，不就又过拟合了吗？后来查了一下书，其实是我把训练误差和测试误差搞混了，而且过拟合的定义没理解好。 二、过拟合（over-fitting） 所谓过拟合，是指在学习时选择的模型包含参数过多，以致于出现这一模型对已知数据预测得很好，但是对未知数据预测很差的现象。也就是说，在训练集上训练误差已经非常小，但在测试集上测试误差却非常大。 出现这个问题往往是因为在设定训练的目标函数时，只片面地追求对训练数据的预测能力。例如，假设现在有一个任务是用训练集（点集）去拟合一个多项式函数，然后用这个多项式函数去预测一些未知的样本。如果在设定训练的目标函数时，只考虑多项式函数与训练样本总误差的最小化，那么在学习的时候为了找到这个最小误差，模型会不断地提高自己的多项式次数（也即模型的复杂度），尝试用三次，四次，或者更高次的多项式，去逼近所有训练样本，达到最小误差。极端的情况下有可能找到一个极高次的多项式函数，使得所有样本都落在这个函数上，训练样本总误差达到了 $0$。但实际上，数据集中往往包含各种噪音和离群点，而模型却无法分辨，在计算总误差的时候依然包含了这些噪音，为了减小这些误差不得不提高多项式函数的次数。如下图所示，理想的模型其实是一个二次多项式函数，但是由于只考虑了使总误差最小，学习的结果是得到一个高次的多项式函数，那么如果用这个高次多项式来预测本应该是二次多项式的未知数据，那么预测效果将会非常糟糕。 可以认为，在训练误差已经比较小的情况下（模型训练完成后），测试误差是对过拟合程度的一种衡量。如果模型在测试集上的测试误差越小，说明过拟合程度越小，如果测试误差越大，则说明过拟合程度比较严重了。所以上面交叉验证中，选择测试误差最小的模型，实际上就是过拟合程度最小的模型。 三、训练误差（training error）与测试误差（test error） 统计学习的目的是使学习到的模型不仅对已知数据（训练集），而且对未知数据（测试集）都有很好的预测能力。当损失函数给定时，基于损失函数的模型的训练误差和预测误差就成为了模型训练效果评估的标准。 假设给定训练集为 $Tr={ (x_1,y_1),(x_2,y_2),\\cdots,(x_N,y_N) }$，测试集为 $Te={ (x_1,y_1),(x_2,y_2),\\cdots,(x_M,y_M) }$，其中 $x_i \\in R^n$，$y_i \\in R$，选定的损失函数是 $L(Y,f(X))$，学习到的模型是 $Y=\\hat{f}(X)$，则输入某个样本到模型得到的预测值为 $\\hat{y}_i=\\hat{f}(x_i)$，则 训练误差是模型 $Y=\\hat{f}(X)$ 在训练集 $Tr$ 上的平均损失： $$R_{emp}(\\hat{f})=\\frac{1}{N}\\sum_{i=1}^{N}L(y_i,\\hat{y}_i)$$ 测试误差是模型 $Y=\\hat{f}(X)$ 在测试集 $Te$ 上的平均损失： $$e_{test}=\\frac{1}{M}\\sum_{i=1}^{M}L(y_i,\\hat{y}_i)$$ 根据前人的经验，训练误差和测试误差与模型有着如下图的关系：当模型的复杂度增大时，训练误差会逐渐减小并趋向于 0，而测试误差会先减小，达到最小值后又增大。 当选择的模型复杂度过大时，过拟合现象就会发生（训练误差很小，但测试误差很大），这样在训练模型时，就需要防止过拟合，进行最优的模型选择，即选择复杂度适当的模型，目的是找到具有最优泛化能力（最小测试误差）的模型，因为我们训练模型的目的不是让模型完美逼近训练集，而是让模型完美预测未知数据。 而常用的模型选择方式有两种，一种是上面的交叉验证法，取预测效果最好的众多个模型中的一个，另一个方法就是在目标函数中加入正则项，惩罚模型在学习过程中增大的复杂度。例如在上面所举的例子中，可以在目标函数中加入关于模型复杂度的函数，当模型复杂度越高，这个函数的值就越大。那么这样一来，模型在学习的时候，最小化的目标就不仅仅是训练样本的总误差了，而是训练样本总误差与模型复杂度之和。如此这般就可以因避免片面追求误差最小而提高了复杂度。 ","date":"2017-11-20","objectID":"/2017-11-20-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE/:0:0","tags":null,"title":"训练误差与测试误差","uri":"/2017-11-20-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E8%AE%AD%E7%BB%83%E8%AF%AF%E5%B7%AE%E4%B8%8E%E6%B5%8B%E8%AF%95%E8%AF%AF%E5%B7%AE/"},{"categories":["MachineLearning"],"content":"分类与回归树（classification and regression tree, CART）模型是应用广泛的决策树学习方法，既可以用于分类，也可以用于回归。 回顾之前学习的 ID3 算法，它的做法是每次选取当前最佳的特征来分割数据，分割依据是该特征的所有取值。也就是说，如果这个特征有 $n$ 个取值，那么数据将立即被分割成 $n$ 分。一旦被切分之后，该特征在之后将不会再被考虑。这样有两个问题：一是切分过于迅速，如果 $n$ 很大，那么数据将被马上切分成大量的小份，影响后续切分的效果。二是，这样的决策树只能处理离散型的数值，如果特征的取值是连续型数值，则需要将其预处理转化成离散型，但这样就会人为地破坏了连续型变量的内在性质。 CART 算法解决了上述问题。CART 设定决策树是二叉树，每次切分数据只做二元切分，即把数据集切分成两份。对某个连续型的特征，设定一个切分值，大于此值的数据进入左子树，反之则进入右子树。然后递归地构建子树。 一、CART 生成 CART 决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择。 1、回归树的生成 输入：训练数据集 $D$，特征集 $A$ 输出：回归树 $T$ (1)、寻找最佳的切分特征 $A_g$ 和对应的最佳切分值 $v$：遍历特征集中每一个特征，尝试以该特征的每一个取值作为切分值，对数据集执行二元切分，然后计算切分误差，如果当前误差小于记录的最小误差，那么更新最优切分特征、最优切分值、最小误差。遍历完成后，返回最优切分特征、最优切分值。 其中误差一般使用目标变量的方差来计算，设数据集为 $D$，数据集中的目标变量为 $D.Y$，数据集在执行一次二元切分后被切分成 $D_{left}$ 和 $D_{right}$，则此次切分减小的混乱程度为 $$g(D,A_g,v)=var(D.Y)- \\left[ var(D_{left}.Y)+var(D_{right}.Y) \\right]$$ 其中 $D_{left}={D \\mid D.A_g\u003ev}$，$D_{right}={D \\mid D.A_g \\leq v}$。 (2)、如果该结点不能再分（左右两个子集方差之和比原数据集方差还大，或者大于给定的某个阈值等等），将该结点存为叶结点，把该结点上的数据集目标变量的平均值作为该结点的输出值返回；否则执行二元切分，把数据集以 $v$ 为切分值，按特征 $A_g$ 切分成 $D_{left}$ 和 $D_{right}$ 两个子集，并对应地生成左右两个子树 (3)、递归地对左右子树重复执行前面的步骤，直到满足停止条件 构造决策树： def createTree(dataSet,tolN,tolS): # 寻找最优切分特征、最优切分值 feat, value = chooseBestSplit(dataSet,tolN,tolS) # 切分特征为空，说明不能再分，直接返回value作为叶结点的输出值 if feat is None: return value # 把最优切分特征、最优切分值记录到树中 retTree = {} retTree['spInd'] = feat retTree['spVal'] = value # 执行二元切分，得到左右子树对应的两个数据集子集 leftSet, rightSet = binSplitDataSet(dataSet, feat, value) # 在左右子树继续递归地调用自己，构造决策树 retTree['left'] = createTree(leftSet, leafType, errType, ops) retTree['right'] = createTree(rightSet, leafType, errType, ops) return retTree 选择最优切分特征、最优切分值： def chooseBestSplit(dataSet,tolN,tolS): # 如果数据集的目标变量全部属于同一个值，则不用再切分 # 此时返回空特征，并把数据集的目标变量（最后一列）的平均值作为返回值 if len(set(dataSet[:, -1].T.tolist()[0])) == 1: return None, mean(dataSet[:, -1]) # 求数据集的目标变量的总方差 m, n = shape(dataSet) S = var(dataSet[:, -1]) * m # 初始化最小方差、最优特征、最优切分值 bestS = inf bestIndex = 0 bestValue = 0 # 遍历每个特征的每个取值 for featIndex in range(n - 1): for splitVal in set((dataSet[:, featIndex].T.A.tolist())[0]): # 执行一次二元切分尝试 mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal) # 计算两个子集的方差之和 newS = var(mat0[:, -1]) * shape(mat0)[0] + var(mat1[:, -1]) * shape(mat1)[0] # 如果误差小于当前记录的最小误差，则记录这次切分 if newS \u003c bestS: bestIndex = featIndex bestValue = splitVal bestS = newS # 如果误差减小不大，小于给定的阈值 tolS，则不再切分 if S - bestS \u003c tolS: return None, mean(dataSet[:, -1]) # 如果切分的两个子集的大小小于给定的阈值 tolN，则不再切分 mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue) if shape(mat0)[0] \u003c tolN or shape(mat1)[1] \u003c tolN: return None, mean(dataSet[:, -1]) # 返回最优切分特征和最优切分值 return bestIndex, bestValue 执行一次二元切分： def binSplitDataSet(dataSet, feature, value): mat0 = dataSet[nonzero(dataSet[:, feature] \u003e value)[0], :] # [0] mat1 = dataSet[nonzero(dataSet[:, feature] \u003c= value)[0], :] # [0] return mat0, mat1 2、分类树的生成 分类树用基尼指数选择最优特征，同时决定该特征的最优切分值。 基尼指数：分类问题中，假设有 $K$ 个类，样本点属于第 $k$ 类的概率为 $p_k$ ，则概率分布的基尼指数为： $$Gini(p)=\\sum_{k=1}^{K}p_k(1-p_k)=1-\\sum_{k=1}^{K}p_{k}^{2}$$ 对于给定的样本集合 $D$，其基尼指数为： $$Gini(D)=1-\\sum_{k=1}^{K} \\left( \\frac{|C_k|}{|D|} \\right)^2$$ 这里，$C_k$ 是 $D$ 中属于第 $k$ 类的样本子集，$K$ 是类的个数。 设集合 $D$ 被特征 $A_g$ 的某个取值 $v$ 二元切分，得到两个子集 $D_{left}$ 和 $D_{right}$，则在特征 $A_g$ 和切分值 $v$ 的条件下，集合 $D$ 的基尼指数定义为 $$ Gini(D,A_g,v)= \\frac{|D_{left}|}{|D|}Gini(D_{left})+ \\frac{|D_{right}|}{|D|}Gini(D_{right}) $$ 其中 $D_{left}={D \\mid D.A_g\u003ev}$，$D_{right}={D \\mid D.A_g \\leq v}$。 基尼指数 $Gini(D)$ 表示集合 $D$ 的不确定性，基尼指数 $Gini(D,A_g,v)$ 表示按特征 $A_g$ 的值 $v$ 二元切分后的不确定性。基尼指数越大，样本集合的不确定性越大，这一点和熵类似。 输入：训练数据集 $D$，停止计算的条件 输出：CART 决策树 根据训练数据集，从根节点开始，递归地对每个结点进行一下操作： (1)、设结点的数据集为 $D$，计算 $Gini(D)$。然后遍历每一个特征的每一个取值，尝试二元切分，并计算 $Gini(D,A_g,v)$，找出基尼指数最小的特征及其对应的切分值，作为最优切分特征和最优切分值。 (2)、把数据集 $D$ 按照最优切分特征和最优切分值进行二元切分，得到两个子集，并对应地从本结点生成两个子结点，将两个子集分配到对应的结点中去。 (3)、对左右子树递归地调用前面步骤，直至满足停止条件。算法的停止条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值，或者没有更多特征。这里需要注意，分类树的叶子结点不是像回归树那样取叶子结点所对应的数据集的目标变量的均值，而是像 ID3 算法那个，选择叶子结点对应的数据集中实例数最大的那个类别作为叶子结点的类别返回。 二、CART 剪枝 如果一棵决策树的结点过多，表明该模型可能对数据进行了“过拟合”。要分析是否确实发生了过拟合，可以使用交叉验证来判断。另一方面，可以","date":"2017-11-18","objectID":"/2017-11-18-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-cart%E7%AE%97%E6%B3%95/:0:0","tags":null,"title":"CART算法","uri":"/2017-11-18-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-cart%E7%AE%97%E6%B3%95/"},{"categories":["MachineLearning"],"content":"一、ID3算法 ID3算法的核心是在决策树各个结点上应用“信息增益”准则选择特征，递归地构建决策树。具体方法：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树，直到所有特征的信息增益均很小或没有特征可以选择为止。 输入：训练数据集 $D$，特征集 $A$，阈值 $\\varepsilon$ 输出：决策树 $T$ 若 $D$ 中所有实例属于同一类 $C_k$，则 $T$ 为单结点树，并将 $C_k$ 作为该结点的类标记，返回 $T$； 若 $A=\\emptyset$ ，则 $T$ 为单结点树，将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$； 遍历 $A$ 中每一个特征，计算信息增益 $g(D,A)=H(D)-\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}H(D_i)$，选择信息增益最大特征 $A_g$。如果 $A_g$ 的信息增益小于阈值 $\\varepsilon$，则置 $T$ 为单结点树，并将 $D$ 中实例数最大的类 $C_k$ 作为该结点的类标记，返回 $T$；否则，对 $A_g$ 的每一个可能值 $a_i$，按照 $A_g=a_i$ 将 $D$ 分割为若干非空子集 $D_i$，对每个子集 $D_i$，将其实例数中最大的类作为标记，构建子结点，由结点及其子结点构成树 $T$，返回 $T$； 对第 $i$ 个子结点，以 $D_i$ 为训练集，以 $A- { A_g }$ 为特征集，递归地调用前3步，得到子树 $T_i$，返回 $T_i$。 以下是部分关键代码，参考《机器学习实战》第三章。这里忽略了上述算法的“信息增益阈值 $\\varepsilon$ 这一条件。 def createTree(dataSet, labels): # 1、若所有实例属于同一类，返回该类 classList = [example[-1] for example in dataSet] if classList.count(classList[0]) == len(classList): return classList[0] # 2、若特征值集合为空集，则返回样本类别中最多的那个类别 if len(dataSet[0]) == 1: return majorityCnt(classList) # 3、遍历特征值集合中的每一个特征，找出信息增益最大的特征 bestFeat = chooseBestFeatureToSplit(dataSet) bestFeatLabel = labels[bestFeat] # 4、以该特征的label建立结点，并从特征集合中删去该特征 myTree = {bestFeatLabel: {}} del (labels[bestFeat]) # 5、收集该特征的所有值，去掉重复值 featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) # 6、对每一种取值，递归调用本函数进行决策树的构建 for value in uniqueVals: subLabels = labels[:] myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) return myTree 二、C4.5算法 C4.5算法与ID3算法基本相同，唯一不同的地方只是在生成决策树的过程中，使用“信息增益比”而不是直接使用“信息增益”来选择特征。 ","date":"2017-11-15","objectID":"/2017-11-15-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-id3%E7%AE%97%E6%B3%95%E4%B8%8Ec4.5%E7%AE%97%E6%B3%95/:0:0","tags":null,"title":"ID3算法与C4.5算法","uri":"/2017-11-15-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-id3%E7%AE%97%E6%B3%95%E4%B8%8Ec4.5%E7%AE%97%E6%B3%95/"},{"categories":["MachineLearning"],"content":"一、熵（entropy） 熵是表示随机变量不确定性的度量。熵越大，随机变量的不确定性越大。设 $X$ 是一个取值为有限个的离散型随机变量，其概率分布为： $$P(X=x_i)=p_i \\text{ , } i=1,2,\\cdots,n$$ 则随机变量 $X$ 的熵定义为 $$H(X)=-\\sum_{i=1}^{n}p_i\\log(p_i)$$ 当式中对数以 $2$ 为底时，熵的单位为比特（bit）；以自然对数 $e$ 为底时，熵的单位为纳特（nat）。 二、条件熵（conditional entropy） 条件熵 $H(Y \\mid X)$ 表示已知随机变量 $X$ 的条件下，随机变量 $Y$ 的不确定性。 设有随机变量 $(X,Y)$，联合分布概率为 $$P(X=x_i,Y=y_i)=p_{ij} \\text{ , } i=1,2,\\cdots,n \\text{ ; }j=1,2,\\cdots,m$$ 则随机变量 $X$ 给定条件下，随机变量 $Y$ 的条件熵为 $$H(Y \\mid X)=\\sum_{i=1}^{n} p_i H(Y \\mid X=x_i)$$ 这里 $p_i=P(X=x_i) \\text{ , } i=1,2,\\cdots,n$。 当熵和条件熵的概率有数据估计得到时，所对应的熵与条件熵称为经验熵（empirical entropy）和经验条件熵（empirical conditional entropy）。 三、信息增益（information gain） 信息增益表示得知特征 $X$ 的信息而使得类 $Y$ 的信息的不确定性减少的程度。 特征 $A$ 对训练数据集 $D$ 的信息增益 $g(D,A)$ 定义为集合 $D$ 的经验熵 $H(D)$ 与特征 $A$ 给定条件下 $D$ 的经验条件熵 $H(D \\mid A)$ 之差 $$g(D,A)=H(D)-H(D \\mid A)$$ 进一步，如果 $A$ 的取值把 $D$ 划分成 $D_1,D_2,\\cdots,D_n$ 这 $n$ 个子集，那么 $$H(D \\mid A)=\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}H(D_i)$$ 因此 $$g(D,A)=H(D)-\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}H(D_i)$$ 上述公式可以理解为，选择特征 $A$ 带来的信息增益为：原数据集的熵（混乱程度，也可以理解为不确定性），减去被特征 $A$ 划分成 $n$ 个子集后这 $n$ 个子集的熵的期望，而得到的差值。 我们在构建决策树时的目标是：使每次被划分数据集的不确定性尽可能小。由于原数据集的熵已经确定，那么，如果选择某个特征划分数据集后，整个数据集的不确定性减小最多（即信息增益最大），那么该特征就是当前最优的分类特征。 找到最优特征后，取该特征下的每一个值构建一个子结点，把数据集划分成 $n$ 个子集合。例如，假设当前最优特征为“年龄”，该特征下一共有“青年”、“中年”、“老年”三种取值，那么在决策树的当前结点下面新建对应的三个孩子结点，把数据集按这三种取值划分为三个子集，传递到对应的三个孩子结点。 四、信息增益比（information gain ratio） 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。用信息增益比可以对这一问题校正。 特征 $A$ 对训练数据集 $D$ 的信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 与训练数据集 $D$ 关于特征 $A$ 的值的熵 $H_A(D)$ 之比，即 $$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$$ 其中 $$H_A(D)=-\\sum_{i=1}^{n}\\frac{|D_i|}{|D|}\\log_2\\frac{|D_i|}{|D|}$$ $n$ 是特征 $A$ 取值的个数。 ","date":"2017-11-11","objectID":"/2017-11-11-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A/:0:0","tags":null,"title":"信息增益","uri":"/2017-11-11-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A/"},{"categories":["Algorithm"],"content":"一、二叉堆建堆过程 二叉堆的定义、父子结点的相对位置计算公式，还有建堆过程在 排序算法（一） 中已经说明。这里稍微温习一下建堆的过程： 从数组的中间位置开始，往左边扫描每个元素，并调用 $sink()$ 方法往下不断调整父子结点的相对位置，如果此堆是最大堆，那么 $sink()$ 方法会使小的结点下沉，大的结点上浮。当扫描完成，则可以保证每个子堆都是父结点大于两个孩子结点，此时堆有序，建堆完成。 从树的角度来理解，建堆的总时间成本，就是对于下标小于 $\\frac{n}{2}$ 的所有结点，也即所有的非叶子结点（二叉堆是完全二叉树），从原位置下沉到正确的位置的总成本。最好的情况是所有非叶子结点都比它的孩子结点大，即数组本来就是堆有序的。最坏的情况是所有非叶子结点都要从原位置一直向下被交换到叶子结点。现在我们来考虑最坏情况的时间复杂度。 在此之前，需要先得到推导过程中会用到的两个小结论。 二、含有 $n$ 个元素的二叉堆的高度为 $\\lfloor \\log_2{n} \\rfloor$ 《算法导论》中给出的关于堆的高度的定义： 堆中一个结点的高度为该结点到叶子结点最长简单路径上边的数目；一个堆的高度即为根结点的高度。 换句通俗的话说，就是从下往上数，以最深的叶子结点为第一层，高度为 $0$（注意这是《算法导论》给出的堆的高度定义，好像和树的高度的定义不太一样，网上有些博客对于树的高度好像是把最深叶子结点的高度算作 $1$ 而不是 $0$）；它的父结点所在的层为第二层，高度为 $1$，依此类推。因此，一个堆的高度（即根结点的高度）在数值上就等于该堆的总层数减一。这样就可转化成求含 $n$ 个元素的二叉树的总层数的简单问题： 对于一棵完全二叉树，结点总数为 $n$，设一共有 $x$ 层。根据完全二叉树的定义和性质，它的前 $x-1$ 层是一棵满二叉树。这棵满二叉树的总结点个数可以轻松求出：从根节点往下数，第一层共 $2^0$ 个结点，第二层共 $2^1$ 个结点，第 $k$ 层共 $2^{k-1}$ 个结点，因此满二叉树结点总数为： $$2^0+2^1+\\cdots+2^{(x-1)-1}=2^{x-1}-1$$ 考虑最后一层，可能是满的也可能不是满的，我们无法确定，但是可以确定这一层必定多于或等于 $1$ 个结点，少于或等于 $2^{x-1}$ 个结点。因此 $$2^{x-1}-1+1 \\leq n \\leq 2^{x-1}-1+2^{x-1}$$ $$2^{x-1} \\leq n \\leq 2^x-1$$ $$x-1 \\leq \\log_{2}n \\leq \\log_2(2^x-1) \u003c \\log_{2}2^x = x$$ $$x-1 \\leq \\log_{2}n \u003c x$$ $$\\log_2{n} \u003c x \\leq \\log_{2}n+1$$ 因为 $x$ 是堆的层数，必定为整数，而 $\\log_2n$ 可能是小数，例如 $2.3 \u003c x \\leq 3.3$，那么此时 $x$ 应该为 $2.3-0.3+1$，即向下取整加 $1$，因此解出 $x=\\lfloor \\log_2n \\rfloor +1$。回到高度的定义，则该堆的高度为 $x-1=\\lfloor \\log_2n \\rfloor$ 三、含有 $n$ 个元素的二叉堆，高度为 $h$ 的结点最多有 $\\lceil \\frac{n}{2^{h+1}} \\rceil$ 个 对高度为 $h$ 的结点，由定义知，它们位于从最深叶子结点往上数的第 $h+1$ 层。只要我们可以确定它们位于从根节点往下数是第几层，那么该层结点数就可以由上面的公式：“第 $k$ 层共 $2^{k-1}$ 个结点”直接求出。 那么如何确定从上往下数，高度为 $h$ 的结点位于第几层？非常简单，用总层数减去从下往上数的层数再加一即可得出。由上面的推导我们知道含 $n$ 个元素的二叉堆的高度（总层数）为 $\\lfloor \\log_2n \\rfloor +1$，因此高度为 $h$ 的结点的位于从上往下数第 $\\lfloor \\log_2n \\rfloor -(h+1)+1=\\lfloor \\log_2n \\rfloor -h$ 层，所以该层最多共有 $2^{(\\lfloor \\log_2n \\rfloor -h)-1}=2^{\\lfloor \\log_2n \\rfloor} \\cdot 2^{-(h+1)}=\\lceil \\frac{n}{2^{h+1}} \\rceil$ 个结点。最后一步推导如下： $$x \u003c \\lfloor x \\rfloor +1 \\leq x+1$$ $$\\log_2n \u003c \\lfloor \\log_2n \\rfloor +1 \\leq \\log_2n+1$$ $$\\log_2n-1 \u003c \\lfloor \\log_2n \\rfloor \\leq \\log_2n$$ $$\\frac{2^{\\log_2n-1}}{2^{h+1}} \u003c \\frac{2^{\\lfloor \\log_2n \\rfloor}}{2^{h+1}} \\leq \\frac{2^{\\log_2n}}{2^{h+1}}$$ $$\\frac{0.5n}{2^{h+1}} \u003c \\frac{2^{\\lfloor \\log_2n \\rfloor}}{2^{h+1}} \\leq \\frac{n}{2^{h+1}}$$ 因为 $\\frac{2^{\\lfloor \\log_2n \\rfloor}}{2^{h+1}}$ 是结点个数，必为整数，因此对区间右端向上取整即可，即 $\\lceil \\frac{n}{2^{h+1}} \\rceil$。 四、二叉堆建堆的时间复杂度为 $O(n)$ 在一个高度为 $h$ 的结点上调用 $sink()$ 方法一直往下调整到叶子结点的最大代价是 $O(h)$。结合上面两个结论，建堆的总代价为: $$O(h) \\cdot \\sum_{h=0}^{\\lfloor \\log_2n \\rfloor} \\lceil \\frac{n}{2^{h+1}} \\rceil= O(n \\cdot \\sum_{h=0}^{\\lfloor \\log_2n \\rfloor} \\frac{h}{2^h})$$ 在这里引用级数公式： $$\\sum_{k=0}^{\\infty} k \\cdot x^k=\\frac{x}{(1-x)^2}$$ 把 $x=\\frac{1}{2}$ 代入公式，$k$ 换成 $h$ 得 $$\\sum_{h=0}^{\\infty} \\frac{h}{2^h}=\\frac{0.5}{(1-0.5)^2}=2$$ 因此建堆总代价为 $O(n \\cdot 2)=O(n)$。 ","date":"2017-11-08","objectID":"/2017-11-08-%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%8F%89%E5%A0%86%E5%BB%BA%E5%A0%86%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/:0:0","tags":null,"title":"二叉堆建堆时间复杂度","uri":"/2017-11-08-%E7%AE%97%E6%B3%95-%E4%BA%8C%E5%8F%89%E5%A0%86%E5%BB%BA%E5%A0%86%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/"},{"categories":["Algorithm"],"content":"一、选择排序 输入长度为 $n$ 的数组，依次选择数组第 $i$ 小的元素，交换到数组第 $i$ 个位置。使用内外两个循环，外循环负责定位到数组第 $i$ 个位置，内循环负责从数组第 $i+1$ 个位置 遍历到数组末端，记录最小的元素，交换到数组第 $i$ 个位置上。 无论输入数组元素的初始序列如何，内循环都需要迭代 $n-i$ 次，所以选择排序的总比较次数始终为 $(n-1)+(n-2)+ \\cdots +2+1=\\frac{n(n-1)}{2} \\approx \\frac{n^2}{2}$ ，时间复杂度为 $O(n^2)$。 public class SortSelection extends Sort { public static void sort(Comparable[] a) { for (int i = 0; i \u003c a.length; i++) { int minIndex = i; for (int j = i + 1; j \u003c a.length; j++) { if (less(a[j], a[minIndex])) { minIndex = j; } } exchange(a, i, minIndex); } } } 二、插入排序 模仿扑克卡牌等的整理过程，从最左边开始，将元素逐个插入到左边有序元素序列中适当的位置。同样使用两层循环，外循环负责定位到第 $i$ 个位置，内循环负责从第 $i-1$ 到第 $1$ 个位置找到能让第 $i$ 个元素插入的合适位置（找到第一个比它小的元素时停止内循环，该元素后一个位置即为适合插入的位置）并进行插入。外循环每层迭代结束时，都保证数组从左边第 $1$ 到第 $i$ 个位置是有序的。 与选择排序不同，插入排序的总比较次数与输入的初始序列有关，这取决于插入排序内循环的比较次数。最好的情况就是输入的数组直接就是有序的，那么每次内循环都只需要比较一次就结束，因为合适的位置就是本身。考虑到外循环迭代 $n$ 次，因此总比较次数为 $(n-1) \\cdot 1=n-1$ 次，即最好情况下时间复杂度为 $O(n)$。相反地，最坏的情况就是输入的数组是刚好逆序的，此时每次内循环都需要从第 $i-1$ 位一直比较到数组最左端的第 $1$ 位，需比较 $i-1$ 次。考虑外循环，此时总比较次数为 $0+1+\\cdots+(n-1)=\\frac{n(n-1)}{2} \\approx \\frac{n^2}{2}$，即最坏情况下时间复杂度为 $O(n^2)$。因此对局部或者全部有序的输入数组，插入排序要比选择排序快。 public class SortInsertion extends Sort { public static void sort(Comparable[] a) { for (int i = 0; i \u003c a.length; i++) { for (int j = i; j \u003e 0 \u0026\u0026 less(a[j], a[j - 1]); j--) { exchange(a, j, (j - 1)); } } } } 三、希尔排序 希尔排序是基于插入排序的一种改进版本。由上面分析知道，对于大规模乱序的数组，插入排序的效率是平方级别的，非常低，因为它只能一点一点地把元素从数组右边移动到左边。例如，如果最小的元素正好在数组尽头，那么将它插入正确位置就需要移动 $n-1$ 次，从数组最右边移到最左端。希尔排序为了加快速度，选择跨若干个元素交换的方式而不是相邻元素交换的方式。例如，跨 $h$ 个元素进行交互，那么一遍下来就可以保证任意间隔为 $h$ 的子数组是有序的。这样的数组被称为 $h$ 有序数组。当这些数组都有序后，开始减小 $h$，直至 $h=1$，这样整个数组便有序了。 至于复杂度分析，根据《算法（第四版）》一书所描述： 透彻理解希尔排序性能至今仍然是一项挑战，也是唯一无法准确描述其对于乱序数组的性能特征的排序方式。 但作者通过一些简单的实验，给出了一些结论： 希尔排序比插入排序和选择排序要快得多，并且数组越大，优势越大。目前最重要的结论是它的运行时间达不到平方级别。已知的最坏的情况下的比较次数和 $N^{3/2}$ 成正比。 关于选择递增序列 $h$： 如何选择递增序列？要回答这个问题并不简单。算法的性能不仅取决于 $h$，还取决于 $h$ 之间的数学性质，例如它们的公因子等。有很多论文研究了各种不同的递增序列，但都无法证明某个序列是“最优”的。 书中样例代码使用的 $h$ 序列为 $3h+1$，如：1，4，13，40，… public class SortShell extends Sort { public static void sort(Comparable[] a) { int len = a.length; int h = 1; while (h \u003c len / 3) h = 3 * h + 1; while (h \u003e= 1) { for (int i = h; i \u003c len; i++) { for (int j = i; j \u003e= h \u0026\u0026 less(a[j], a[j - h]); j -= h) { exchange(a, j, (j - h)); } } h = h / 3; } } } 四、归并排序 将两个已经有序的数组，合并成一个有序数组。 1、归并（原地） 我们假设输入是一个数组 $a$，该数组的第 $lo$ 位到第 $mid$ 位，第 $mid$ 位到第 $hi$ 位分别已经有序，现在需要将两部分合并，并使使其有序，返回原数组。思路非常简单，使用辅助数组先把这两部分复制出去，然后不断比较两个部分的队头元素，谁最小谁就出队回来原数组。如果忽略新建辅助数组并复制过去的操作，显然进行归并时每个元素都只需要扫描一次。因此时间复杂度是线性的，若需要归并的元素总个数为 $n$，则时间复杂度为 $O(n)$。（注意这里仅指归并两个子数组的复杂度，不是整个完整归并排序的复杂度） // merge a[lo..mid] and a[mid+1..hi] to a[lo...hi] public static void merge(Comparable[] a, int lo, int mid, int hi) { int i = lo, j = mid + 1; for (int k = lo; k \u003c= hi; k++)//copy a[lo..hi] to aux[lo..hi] aux[k] = a[k]; for (int k = lo; k \u003c= hi; k++)//move items back if (i \u003e mid) a[k] = aux[j++]; else if (j \u003e hi) a[k] = aux[i++]; else if (less(aux[j], aux[i])) a[k] = aux[j++]; else a[k] = aux[i++]; } 2、排序 有了归并操作的基础后，后面的排序就有两种选择方式：自顶向下，自底向上。 (1)、自顶向下 自顶向下的思路是，在每一层递归函数中，先把数组切分成左半边和右半边，然后把这两部分分别放给下一层递归来处理（直到切分到子数组只有一个元素为止，便返回上一层），当左右两部分都完成排序返回到本层递归时，归并这两部分，返回上一层递归。 至于时间复杂度，我们可以把归并排序处理过程想象成一棵树，根节点是最顶层的递归函数，对应原数组 $a[0…(n-1)]$ 的归并，这层需归并 $n$ 个元素；树的第二层是根节点的左右两个孩子，分别对应为原数组的左右两部分，左孩子处理 $a[0…\\frac{n-1}{2}]$ 的归并，右孩子处理 $a[\\frac{n-1}{2}+1…(n-1)]$ 的归并，因此该层也是一共需要归并 $n$ 个元素；如此一直到最底层叶节点，数组被切分成长度为2的 $\\frac{n}{2}$ 个子数组：$a[0…1]，a[2..3],\\cdots,a[(n-2)…(n-1)]$，该层同样共需处理 $n$ 个元素的归并。由于每层递归都是把对应子数组再分成两个子数组，因此该树是棵二叉树，树的高度为 $log_2(n)$。由上面分析知道每一层需归并处理的元素都是 $n$ 个，因此整棵树的处理代价，也即自顶向下归并排序的时间复杂度为 $O(n \\cdot log_2(n))$。 public class SortMerge extends Sort { private static Comparable[] aux; public static void sort(Comparable[] a) { aux = new Comparable[a.length]; recursiveSort(a, 0, a.length - 1); } public static void recursiveSort(Comparable[] a, int lo, int hi) { if (lo \u003e= hi) return; int mid = lo + (hi - lo) / 2; recursiveSort(a, lo, mid); recursiveSort(a, (mid + 1), hi); merge(a, lo, mid, hi); } } (2)、自底向上 自底向上的思路是，使用迭代而不是递归的方式，对数组进行两两归并，四四归并，八八归并，一直到整个数组被归并。使用两层循环，外循环迭代地改变子数组的大小：1，2，4，8，依次类","date":"2017-11-04","objectID":"/2017-11-04-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/:0:0","tags":null,"title":"排序算法","uri":"/2017-11-04-%E7%AE%97%E6%B3%95-%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"},{"categories":["Algorithm"],"content":"一、问题 leetcode第190题，逆序所输入的整数的32位bit，构成一个新的整数并输出：190. Reverse Bits 二、解决 思路不难，对输入的整数$n$，不断右移（带符号）并取其最低位（与上1），加到返回的整数上；返回的整数每次接收$n$的1位后，左移1位腾出最低位给下一次接收。如此循环32次即可。 public class Solution { public int reverseBits(int n) { int result=0; for(int i=0;i\u003c32;i++){ result=result\u003c\u003c1; result=result+(n\u00261); n=n\u003e\u003e\u003e1; } return result; } } 我一开始没有注意四则运算符和位运算符的优先级，默认了位运算是高于加减乘除的，所以上面第6行处我一开始写成： result = result + n\u00261; 然后一直AC失败。我反复检查了我的思路也找不到出错的原因。无奈之下查看了discuss，发现我的思路和高票答案是一模一样的。不过别人的写法是： result += n\u00261; 此时我才醒悟难道是运算符优先级的问题？我立马尝试加上括号，发现AC成功。后来查了资料才知道加减运算符的优先级是比按位与运算符的优先级高的： int a = 2; int b = 0; int c = a + b \u0026 1; int d = a + (b \u0026 1); System.out.print(\"c=\" + c + \" , \" + \"d=\" + d); 输出结果为：c=0，d=2 这件事充分说明了学习基础不扎实，需要深刻反省引以为戒，因此写到博客里记录一下。 ","date":"2017-10-26","objectID":"/2017-10-26-%E7%AE%97%E6%B3%95-java%E4%BD%8D%E8%BF%90%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E7%BA%A7%E9%97%AE%E9%A2%98/:0:0","tags":null,"title":"java位运算符优先级","uri":"/2017-10-26-%E7%AE%97%E6%B3%95-java%E4%BD%8D%E8%BF%90%E7%AE%97%E7%AC%A6%E4%BC%98%E5%85%88%E7%BA%A7%E9%97%AE%E9%A2%98/"},{"categories":["Algorithm"],"content":"一、问题 给定一个整型数组，求其最大的子数组和。例如，给定数组：[-2,1,-3,4,-1,2,1,-5,4]，和最大的子数组为：[4,-1,2,1]，和为6。 二、递推式 设给定的数组为 $A$，用 $dp[i]$ 表示以 $A[i]$ 结尾的最大子数组之和。那么 $dp[i-1]$ 和 $dp[i]$ 有怎样的关系？由于给定的数组为整型，有正数也有负数，因此我们可以从子数组和的正负来考虑。显然，要使 $dp[i]$ 是最大的和，那么 以 $A[i-1]$ 结尾的子数组最大和 $dp[i-1]$ 如果是负数，则以 $A[i]$ 结尾的子数组的最大和 $dp[i]$ 应该去掉前面部分的负数，只取 $A[i]$ 作为最大和，因为如果加上前面部分的和只会比 $A[i]$ 更小；如果是正数，那么应该取 $A[i]$ 加上前面部分的 $dp[i-1]$ 作为最大和。形式化描述为： $$ dp[i]=A[i]+(dp[i-1]\u003e0?dp[i-1]:0) $$ 最终的最大子数组和就是数组 $dp[n]$ 的最大值。为了避免重复扫描该数组我们可以在计算该数组的时候维护一个当前最大值 $max$。 三、代码实现 public static int maxSubArray(int[] A) { int n = A.length; int[] dp = new int[n];//dp[i] means the maximum subarray ending with A[i]; dp[0] = A[0]; int max = dp[0]; for(int i = 1; i \u003c n; i++){ dp[i] = A[i] + (dp[i - 1] \u003e 0 ? dp[i - 1] : 0); max = Math.max(max, dp[i]); } return max; } public static void main(String[] args) { int[] A = {-2,1,-3,4,-1,2,1,-5,4}; System.out.println(maxSubArray(A)); } 时间和空间复杂度都为 $O(n)$。 ","date":"2017-10-18","objectID":"/2017-10-18-%E7%AE%97%E6%B3%95-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%95%B0%E7%BB%84%E5%92%8C/:0:0","tags":null,"title":"最大子数组和","uri":"/2017-10-18-%E7%AE%97%E6%B3%95-%E6%9C%80%E5%A4%A7%E5%AD%90%E6%95%B0%E7%BB%84%E5%92%8C/"},{"categories":["Algorithm"],"content":"一、问题 给定一个长度为 $N$ 的数组，找出一个最长的单调自增子序列的长度。例如：给定一个长度为 6 的数组 $A=\u003c 5，6，7，1，2，8 \u003e$，则其最长的单调递增子序列为 $\u003c 5，6，7，8 \u003e$，长度为4。 二、寻找最优子结构，导出递推式 设前 $i$ 个元素的最长递增子序列长度（即第 $i$ 个子问题最优解）表示为 $dp(i)$。那么第 $i$ 个子问题的最优解 $dp(i)$ 与前面 $i-1$ 个子问题各自的最优解 $dp(k)$ $(1 \\leq k \\leq i-1)$ 有着怎么样的递推关系？ 考察前 $i-1$ 个子问题中第 $k$ $(1 \\leq k \\leq i-1)$ 个子问题对应的最长递增子序列。如果原序列第 $i$ 个元素 $A[i]$ 大于等于该序列的最后一个元素（设为 $last[k]$），则 $A[i]$ 可以追加到此递增子序列的末尾。此时有 $dp(i)=dp(k)+1$；反之如果 $A[i]$ 小于该序列的最后一个元素 $last[k]$，则 $A[i]$ 不能追加到此递增子序列的末尾，此时 $dp(i)=dp(k)$。由于 $1 \\leq k \\leq i-1$，因此我们只要遍历前 $i-1$ 个元素对应的 $i-1$ 个最长子序列，并考察第 $i$ 个元素追加（或者不追加）到这些子序列末尾后得到的 $i-1$ 个 $dp(i)$，其中最大一个的即为第 $i$ 个问题的最优解。形式化描述为： $$ dp(i)= \\begin{cases} 1, \u0026 \\text{if $i=1$} \\\\ max_{1 \\leq k \\leq i-1} \\{ dp(k) \\} , \u0026 \\text{if $i1,A[i] 1,A[i] \\geq last[k]$} \\\\ \\end{cases} $$ 三、代码实现 public static int longestIncreasingSubsequence(int[] A) { int n = A.length - 1; int[] dp = new int[n + 1]; int[] last = new int[n + 1]; for (int i = 1; i \u003c= n; i++) { if (i == 1) { dp[i] = 1; last[i] = A[1]; } else { dp[i] = Integer.MIN_VALUE; for (int k = 1; k \u003c= i - 1; k++) { if (A[i] \u003e= last[k] \u0026\u0026 dp[k] + 1 \u003e dp[i]) { dp[i] = dp[k] + 1; last[i] = A[i]; } else if (dp[k] \u003e dp[i]) { dp[i] = dp[k]; last[i] = last[k]; } } } } return dp[n]; } public static void main(String[] args) { // 第一个元素从下标1开始 int[] A = {0, 9, 6, 7, 1, 9, 10}; System.out.println(longestIncrementSubsequence(A)); } 使用 $last[k]$ 来记录前 $k$ 个元素的最长递增子序列的最后一个元素。算法使用了两层 $for$ 循环，外层取值范围 $1$ 到 $n$，内层 $1$ 到 $i-1$，因此时间复杂度为 $O(n^2)$。另外由于使用了长度为 $n$ 的一维数组，因此空间复杂度为 $O(n)$。 ","date":"2017-09-26","objectID":"/2017-09-26-%E7%AE%97%E6%B3%95-%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/:0:0","tags":null,"title":"最长递增子序列","uri":"/2017-09-26-%E7%AE%97%E6%B3%95-%E6%9C%80%E9%95%BF%E9%80%92%E5%A2%9E%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["Algorithm"],"content":"一、问题 假设有几种硬币，如1、3、5、10，并且数量无限。请找出能够组成某个数目的找零所使用最少的硬币数 形式化定义：设需要找零的总额为 $V$ ，一共有 $n$ 种硬币可以选择，其中第 $i$ 种硬币面值为 $c_i$，对应的使用数量为 $y_i$。则问题表述为： $$min \\quad \\sum_{i=1}^{n}y_i , \\quad y_i \\in N \\text{（自然数集）} $$ $$s.t. \\quad \\sum_{i=1}^{n} c_i \\cdot y_i = V$$ 二、划分子问题，导出递推式 今天查了一些资料，对动态规划有了新的理解。 有博主说： 动态规划的本质是对问题状态的定义，和对状态转移方程的定义。动态规划是通过拆分问题，定义问题状态和状态之间的关系，使得问题能够以递推（或者说分治）的方式去解决。如何拆分问题，才是动态规划的核心，而拆分问题，靠的就是状态的定义和状态转移方程的定义。 也有博主说： 计算机的本质是一个状态机，内存里存储的所有数据构成了当前的状态，CPU只能利用当前的状态计算出下一个状态（不要纠结硬盘之类的外部存储，就算考虑他们也只是扩大了状态的存储容量而已，并不能改变下一个状态只能从当前状态计算出来这一条铁律）当你企图使用计算机解决一个问题是，其实就是在思考如何将这个问题表达成状态（用哪些变量存储哪些数据）以及如何在状态中转移（怎样根据一些变量计算出另一些变量）。所以所谓的空间复杂度就是为了支持你的计算所必需存储的状态最多有多少，所谓时间复杂度就是从初始状态到达最终状态中间需要多少步！ 回到我们的问题，要定义这个问题的状态和状态转移方程，首先需要分析这个问题的解的构建过程。那么找零的过程是怎么样的？很简单，每次可以拿起 $n$ 种硬币中，面值小于找零总额的任意一个。然后把找零总额减去拿起的硬币的面值，进入下一次选择。不断如此重复，直至拿起的硬币的总值等于找零的总额。（到这里，子问题的定义就显而易见了，这个问题的子问题就是每一次进行的硬币的选择，本次的选择只与之前的选择有关，因为每一次选择后剩余的找零总额会相应减小） 但是这样并不能保证硬币数是最小的。为了找到最优解，我们需要假设某个子问题已经得到了最优解，然后推出与它的子问题的最优解的关系，也就是最优解的递推关系式。设 $dp(i)$ 表示组合总价值为 $i$ 的零钱所需要的最小硬币数。因此每次可以选择 $n$ 种硬币中的一种，设本次选择的硬币为第 $j$ $(1 \\leq j \\leq n)$ 种，对应的面值为 $c[j]$，则在拿起该硬币之前的子问题即为：组合面值为 $dp(i-c[j])$ 的总额需要的最小硬币数。同样可以用“剪贴粘贴”的反正法证明其最优解为 $dp(i-c[j])-1$。形式化描述为： $$ dp(i)= \\begin{cases} 0, \u0026 \\text{if $i=0 $} \\\\ min_{1 \\leq j \\leq n}(dp(i-c[j])+1), \u0026 \\text{if $i 0 \\, , \\, c[j] \\leq i$} \\\\ \\end{cases} $$ 三、算法实现 public static int coin(int[] c, int v) { int[] dp = new int[v + 1]; for (int i = 0; i \u003c= v; i++) { if (i == 0) dp[i] = 0; else { dp[i] = Integer.MAX_VALUE; for (int j = 1; c[j] \u003c= i \u0026\u0026 j \u003c c.length; j++) { int temp = dp[i - c[j]] + 1; if (temp \u003c dp[i]) dp[i] = temp; } } } return dp[v]; } public static void main(String[] args) { //从下标1开始表示第1种硬币 int[] c = {0, 1, 3, 5, 10}; System.out.println(coin(c, 3)); } 外层的 $for$ 循环取值范围为 $0$ 到 $V$，内层的 $for$ 循环的取值范围为 $0$ 到 $n$，因此时间复杂度为 $O(V \\cdot n)$ 。空间复杂度为 $O(V)$。 ","date":"2017-09-24","objectID":"/2017-09-24-%E7%AE%97%E6%B3%95-%E7%A1%AC%E5%B8%81%E6%89%BE%E9%9B%B6/:0:0","tags":null,"title":"硬币找零","uri":"/2017-09-24-%E7%AE%97%E6%B3%95-%E7%A1%AC%E5%B8%81%E6%89%BE%E9%9B%B6/"},{"categories":["Algorithm"],"content":"一、问题 有 $n$ 个物品，其中第 $i$ 个物品的重量为 $w[i]$，价值为 $p[i]$，我们规定所有物品的重量和价格均非负。现有一个可以承受最大重量为 $W$ 的背包，对每个物品，只能选择装 0 个或者 1 个入背包。问选择哪些物品装入背包可以使背包中的总价值最大。 形式化定义： $$max \\quad \\sum_{i=1}^{n} p[i] \\cdot x_i$$ $$s.t. \\quad \\sum_{i=1}^{n} w[i] \\cdot x_i \\leq W , \\quad x_i \\in { 0,1 }$$ 二、划分子问题，导出递推式 如何从 $n$ 个物品中选择出若干个装入背包并使总价值最大？我们需要寻找一个有规律的装包过程，这样才能划分出子问题。 假设我们已经知道了选择哪几个物品是最优的，那么一个很直接的想法就是从左往右扫描这 $n$ 个物品，依次对每个物品做出选择：装入这个物品，或者放弃这个物品。当扫描完成，那么装包过程结束。 现在我们来分析每一步并试图划分出子问题。假设我们扫描了前 $i$ 个物品，并得到了当前的最大总价值，设为 $P_i$。那么倒回去看，这一步的最优解是怎么得到的？因为对每个物品，只能选择装入0个或者1个，因此这一步的最优解就是：在扫描并处理了前 $i-1$ 个物品后，选择装入第 $i$ 个物品后得到的最大总价值，或者放弃第 $i$ 个物品后的最大总价值。如果是通过选择物品 $i$ 而得到这一步的最优解 $P_i$，那么前 $i-1$ 个物品的最大总价值为 $P_i-p[i]$；如果是通过放弃物品 $i$ 而得到这一步的最优解，那么前 $i-1$ 个物品的最大总价值仍为 $P_i$。 考虑最优解的对应的背包容量的递推关系。设前 $i$ 物品恰好能装入容量为 $W_i$ 的背包得到最大总价值。如果是通过选择物品 $i$ 而得到的最优解，那么前 $i-1$ 个物品恰好能装入的背包的容量为 $W_i-w[i]$；反之，则前 $i-1$ 个物品恰好能装入的背包的容量为 $W_i$。 设 $dp(i,j)$ 表示前 $i$ 个物品装入容量为 $j$ 的背包的最大总价值。那么上述推导可以形式化描述为： $$ dp(i,j)= \\begin{cases} 0, \u0026 \\text{if $i=0 \\, or \\, j=0$} \\\\ dp(i-1,j), \u0026 \\text{if $w[i]j$} \\\\ max \\{\\, dp(i-1,j-w[i])+p[i], \\, dp(i-1,j) \\,\\} , \u0026 \\text{if $w[i] \\leq j$} \\\\ \\end{cases} $$ 三、算法实现 public static int zeroOnePackage(int[] p, int[] w, int n, int W) { int[][] dp = new int[n + 1][W + 1]; for (int i = 0; i \u003c= n; i++) { for (int j = 0; j \u003c= W; j++) { if (i == 0 || j == 0) dp[i][j] = 0; else if (w[i] \u003e j) dp[i][j] = dp[i - 1][j]; else dp[i][j] = Math.max(dp[i - 1][j - w[i]] + p[i], dp[i - 1][j]); } } return dp[n][W]; } public static void main(String[] args) { int[] p = {0, 6, 3, 5, 4, 6}; //从下标1开始表示第1个物品 int[] w = {0, 2, 2, 6, 5, 4}; //同上 System.out.println(zeroOnePackage(p, w, 5, 10)); } 外层的 $for$ 循环取值范围为 $0$ 到 $n$，内层的 $for$ 循环的取值范围为 $0$ 到 $W$，因此时间复杂度为 $O(W \\cdot n)$；因为使用了二维数组 $dp[0…n][0…W]$， 因此空间复杂度为 $O(W \\cdot n)$。 其实空间复杂度还有可以优化的地方：如果仔细分析，可以发现 $dp[i][…]$ 只与 $dp[i-1][…]$ 有关。因此可以只存储 $dp[i-1][…]$，即只使用一个一维数组，具体后续再讨论。 ","date":"2017-09-22","objectID":"/2017-09-22-%E7%AE%97%E6%B3%95-01%E8%83%8C%E5%8C%85/:0:0","tags":null,"title":"0-1 背包问题","uri":"/2017-09-22-%E7%AE%97%E6%B3%95-01%E8%83%8C%E5%8C%85/"},{"categories":["Thesis"],"content":"一、学术会议 1、SIGKDD CCF评级：A类 全称：Special Interest Group on Knowledge Discovery and Data Mining 出版社：ACM 网址：http://www.kdd.org/ DBLP：http://dblp.uni-trier.de/db/conf/kdd/ 时间：每年8月 2、SIGIR CCF评级：A类 全称：Special Interest Group on Information Retrieval 出版社：ACM 网址：http://sigir.org/ DBLP：http://dblp.uni-trier.de/db/conf/sigir/ 时间：每年7月至8月 3、ICDE CCF评级：A类 全称：International Conference on Data Engineering 出版社：IEEE 网址：https://www.icde.org/ DBLP：http://dblp.uni-trier.de/db/conf/icde/ 时间：每年4月至5月 4、AAAI CCF评级：A类 全称：Association for the Advancement of Artificial Intelligence 出版社：AAAI 网址：https://www.aaai.org/ DBLP：http://dblp.uni-trier.de/db/conf/aaai/ 时间：2014年之前每年7月到8月，2015年开始改到了每年1月到2月 5、TKDE CCF评级：A类 全称：IEEE Transactions on Knowledge and Data Engineering 出版社：IEEE 网址：https://www.computer.org/web/tkde DBLP：http://dblp.uni-trier.de/db/journals/tkde/ 时间：每年1月 6、WWW CCF评级：A类 全称：International World Wide Web Conferences 出版社：ACM 网址：http://www.iw3c2.org/ DBLP：http://dblp.uni-trier.de/db/conf/www/ 时间：每年4月到5月 7、CIKM CCF评级：B类 全称：International Conference on Information and Knowledge Management 出版社：ACM 网址：http://www.cikmconference.org/ DBLP：http://dblp.uni-trier.de/db/conf/cikm/ 时间：每年10月至11月 8、ICDM CCF评级：B类 全称：IEEE International Conference on Data Mining 出版社：IEEE 网址：http://www.cs.uvm.edu/~icdm/ DBLP：http://dblp.uni-trier.de/db/conf/icdm/ 时间：每年11月至12月 9、WSDN CCF评级：B类 全称：ACM International Conference on Web Search and Data Mining 出版社：ACM 网址：http://www.wsdm-conference.org/ DBLP：http://dblp.uni-trier.de/db/conf/wsdm/ 时间：每年2月 10、RecSys CCF评级：无 全称：ACM Conference on Recommender Systems 出版社：ACM 网址：https://recsys.acm.org/ DBLP：http://dblp.uni-trier.de/db/conf/recsys/ 时间：每年8月到9月 二、相关链接 1、中国计算机学会推荐国际学术会议和期刊目录：http://www.ccf.org.cn/xspj/gyml/ 2、数据挖掘领域顶级会议期刊及其分析：http://blog.csdn.net/yaoyepeng/article/details/6533745 3、人工智能与数据挖掘顶级会议：http://blog.csdn.net/zongzhiyuan/article/details/49964813 4、学术会议搜索：http://www.searchconf.net/ 5、Grouplens小组：https://grouplens.org/ ","date":"2017-09-21","objectID":"/2017-09-21-%E8%AE%BA%E6%96%87-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9B%B8%E5%85%B3%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/:0:0","tags":null,"title":"推荐系统相关学术会议","uri":"/2017-09-21-%E8%AE%BA%E6%96%87-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9B%B8%E5%85%B3%E5%AD%A6%E6%9C%AF%E4%BC%9A%E8%AE%AE/"},{"categories":["Algorithm"],"content":"一、问题 给定一个包含 $n$ 个不同关键字的已经排序的序列 $K=\u003c k_1,k_2, \\cdots , k_n\u003e$ $(k_1 \u003c k_2 \u003c \\cdots \u003c k_n)$，我们希望用这些关键字构造一颗二叉搜索树。 对每个关键字 $k_i$，都有一个概率 $p_i$ 表示其搜索频率。有些要搜索的值可能不在 $K$ 中，因此我们还有 $n+1$ 个“伪关键字”：$\u003c d_0,d_1,d_2,\\cdots,d_n \u003e$ 表示不在 $K$ 中的值。其中，$d_0$ 表示所有小于 $k_1$ 的值，$d_n$ 表示所有大于 $k_n$ 的值，对 $i=1,2,\\cdots,n-1$，伪关键字 $d_i$ 表示所有在 $k_i$ 和 $k_{i+1}$ 之间的值。对每个伪关键字 $d_i$，也都有一个概率 $q_i$ 表示对应的搜索频率。 由这 $n$ 个关键字和 $n+1$ 个 伪关键字构成的二叉搜索树，每个关键字 $k_i$ $(1 \\leq i \\leq n)$ 是一个内部结点，而每个伪关键字 $d_i$ $(0 \\leq i \\leq n)$ 是一个叶结点。 由于每次搜索要么成功（找到某个关键字 $k_i$）要么失败（找到某个伪关键字 $d_i$），因此这棵树所有结点的概率之和为1： $$\\sum_{i=1}^{n}p_i + \\sum_{i=0}^{n}q_i = 1$$ 因为我们知道了每个关键字和伪关键字的搜索概率，所以我们可以求出在这给定的二叉搜索树 $T$ 中进行一次搜索的期望代价 $E(T)$ 。假定一次搜索的代价等于访问的结点数，即搜索到的结点在 $T$ 中的深度再加 1（深度不算根结点，要从根节点算起就要加 1） ，那么在 $T$ 中进行一次搜索的期望代价为如下公式所示，其中 $D_{T}(k_l)$ 表示结点 $k_l$ 在树 $T$ 中的深度： $$E(T)= \\sum_{l=1}^{n} (D_{T}(k_l)+1) \\cdot p_l + \\sum_{l=0}^{n} (D_{T}(d_l)+1) \\cdot q_l $$ 现假设有一个 $n=5$ 的关键字序列，及如下的搜索概率： i 0 1 2 3 4 5 pi 0.15 0.10 0.05 0.10 0.20 qi 0.05 0.10 0.05 0.05 0.05 0.10 问如何构造一棵搜索期望代价 $E(T)$ 最小的二叉搜索树（即最优二叉搜索树）？ 二、划分子问题，导出递推式 考虑一棵二叉搜索树的任意子树 $T$，它包含了关键字 $\u003c k_i,\\cdots,k_j \u003e$（$1 \\leq i \\leq j \\leq n$），其叶结点是伪关键字 $\u003c d_{i-1},\\cdots,d_n \u003e$。由上一节的公式可以直接计算它的搜索期望 $E(T)$。现在我们来考察其子问题，并尝试推导该二叉搜索树的搜索期望和其子问题的搜索期望的递推关系式。 如何根据关键字序列 $\u003c k_i,\\cdots,k_j \u003e$ 构造一棵搜索二叉树？考虑到该序列是已经排好序的，而二叉搜索树遵循：左子树 \u003c 根结点 \u003c 右子树 的规则，因此用这些关键字构造二叉搜索树时，它们在树中的左右相对顺序与原序列一致。那么现在我们只需要确定用哪个结点来作为该树的根结点，把序列划分成左右两部分，左边作为左子树，右边作为右子树。假设选择第 $r$ $(i \\leq r \\leq j)$ 个关键字 $k_r$ 作为根结点，则左子树为 $\u003c k_i,\\cdots,k_{r-1} \u003e$，右子树为 $\u003c k_{r+1},\\cdots,k_j \u003e$ 。递归地对左子树和右子树进行这样的操作直到不可再分为止，即可构造出二叉搜索树。 这里还有一个要注意的细节——“空子树”。假定对于 $\u003c k_i,\\cdots,k_j \u003e$ 的子问题，我们选定了序列第一个关键字 $k_i$ 作为根结点，即 $r=i$。那么根据上面的推导，它的左子树为 $\u003c k_i,\\cdots,k_{i-1} \u003e$，我们将这样的序列解释：为不包含任何关键字的序列。但实际上由定义可知，它的作子树仍包含伪关键字 $d_{i-1}$（$d_{i-1} \u003c k_i $），其对应的搜索概率为 $q_{i-1}$。对称地，如果选择序列最后一个关键字作为根结点，那么它的右子树不包含任何关键字，但包含伪关键字 $d_j$，对应搜索概率为 $q_j$。 这样，构造一棵子树的两个递归的子问题是：构造其左子树和右子树。设子树 $T$ 的左子树为 $T_{L}$，右子树为 $T_{R}$。现在我们来推导 $E(T)$ 和 $E(T_{L})$、$E(T_{R})$ 的关系。 注意到，当 $T_{L}$ 和 $T_{R}$ 合并成一棵树 $T$ 的时候，结点的深度加1，即： $$D_{T}(k_l)=D_{T_{L}}(k_l)+1$$ 对前面的公式，我们把作为根结点的关键字 $k_r$ $(D_{T}(k_r)=0)$ 提出来，并把分割后的左右两部分写成子树的形式： $$ \\begin{aligned} E(T) \u0026= \\sum_{l=i}^{j} (D_{T}(k_l)+1) \\cdot p_l + \\sum_{l=i-1}^{j} (D_{T}(d_l)+1) \\cdot q_l \\\\ \u0026 = \\sum_{l=i}^{r-1} (D_{T}(k_l)+1) \\cdot p_l + \\sum_{l=i-1}^{r-1} (D_{T}(d_l)+1) \\cdot q_l + (D_{T}(k_r)+1) \\cdot p_r + \\sum_{l=r+1}^{j} (D_{T}(k_l)+1) \\cdot p_l + \\sum_{l=r+1-1}^{j} (D_{T}(d_l)+1) \\cdot q_l \\\\ \u0026 = \\sum_{l=i}^{r-1} (D_{T_{L}}(k_l)+1+1) \\cdot p_l + \\sum_{l=i-1}^{r-1} (D_{T_{L}}(d_l)+1+1) \\cdot q_l + p_r + \\sum_{l=r+1}^{j} (D_{T_{R}}(k_l)+1)+1) \\cdot p_l + \\sum_{l=r+1-1}^{j} (D_{T_{R}}(d_l)+1)+1) \\cdot q_l \\\\ \u0026 = E(T_{L}) + (\\sum_{l=i}^{r-1} p_l + \\sum_{l=i-1}^{r-1} q_l) + p_r + E(T_{R}) + (\\sum_{l=r+1}^{j} p_l + \\sum_{l=r+1-1}^{j} q_l) \\end{aligned} $$ 为简便表达，对于包含关键字序列 $\u003c k_i,\\cdots,k_j \u003e$ 的子树，我们记所有结点的概率和为： $$w(i,j)=\\sum_{l=i}^{j} p_l + \\sum_{l=i-1}^{j} q_l$$ 则上面推导结果进一步表示为： $$ \\begin{aligned} E(T) \u0026 = E(T_{left}) + w(i,r-1) + p_r + E(T_{right}) + w(r+1,j) \\\\ \u0026 = E(T_{left}) + E(T_{right}) + w(i,r-1) + p_r + w(r+1,j) \\\\ \u0026 = E(T_{left}) + E(T_{right}) + w(i,j) \\end{aligned} $$ 其中 $T_{L}$ 对应关键字序列 $\u003c k_i,\\cdots,k_{r-1} \u003e$， $T_{R}$ 对应关键字序列 $\u003c k_{r+1},\\cdots,k_j \u003e$。现在我们分别用 $e[i,r-1]$ 和 $e[r+1,j]$ 表示它们对应的期望代价。 类似于钢条切割和矩阵连乘，序列 $\u003c k_i,\\cdots,k_j \u003e$ 有 $r=j-i+1$ 个关键字可以作为根结点 $k_r$ $(i \\leq r \\leq j)$ 把序列划分成左子树和右子树。我们遍历这段序列中每一个可能的关键字来尝试构造二叉搜索树，然后取期望代价最小的那个方案作为最优解（同钢条切割和矩阵连乘，可以用“剪切-粘贴”反证法证明：如果 $E(T)$ 如果是最优解，那么 $E(T_{L})$ 和 $E(T_{R})$ 一定分别是左子树和右子树的最优解）并存储起来。因此我们可以得到最终递推公式： $$ e[i,j] = \\begin{cases} q_{i-1}，\u0026 \\text{if $j=i-1$} \\\\ min_{i \\leq r \\leq j} \\{ e[i,r-1]+e[r+1,j]+w(i,j) \\} \u0026 \\text{if $i \\leq j$} \\end{cases} $$ 四、实现 为降低算法复杂度，我们使用自底向上带备忘的动态规划法而不是递归求解。 我们使用一个二维数组 $e[1…n+1][0…n]$ 来保存 $e[i,j]$ 的值。第一维下标上界为 $n+1$ 而不是 $n$，原因在于对于只包含伪关键字 $d_n$ 的子树，我们需要计算并保存 $e[n+1,n]$。第二维下标下界为 $0$，是因为对于只包含伪关键字 $d_0$ 的子树，我们需要计算并保存 $e[1,0]$。我们实际上只使用满足 $j \\geq i-1$ 的表项 $e[i][j]$。 对于 $w(i,j)$，为了避免在每次计算 $e[i,j]$ 时都要重新计算 $w(i,j)$，我们使用一个二维数组 $w[1…n+1][0…n]$ 来保存它。当子树为空即只包含伪关键字的情况","date":"2017-09-20","objectID":"/2017-09-20-%E7%AE%97%E6%B3%95-%E6%9C%80%E4%BC%98%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/:0:0","tags":null,"title":"最优二叉搜索树","uri":"/2017-09-20-%E7%AE%97%E6%B3%95-%E6%9C%80%E4%BC%98%E4%BA%8C%E5%8F%89%E6%90%9C%E7%B4%A2%E6%A0%91/"},{"categories":["Algorithm"],"content":"一、问题：给定两个序列 $X=\u003c x_1, x_2, \\cdots, x_m \u003e$ 和 $Y=\u003c y_1, y_2, \\cdots, y_n \u003e$，求它们的最长公共子序列（longest-common-subsequence，LCS）。 子序列定义：给定一个序列 $X=\u003c x_1, x_2, \\cdots , x_m \u003e$，另一个序列 $Z=\u003c z_1, z_2, \\cdots, z_k \u003e$ 满足如下条件时成为 $X$ 的子序列，即存在一个严格递增的 $X$ 的下标序列 $\u003c i_1, i_2, \\cdots, i_k \u003e$，对所有的 $j=1, 2, \\cdots, k$，满足 $x_{i_j}=z_j$。例如，$Z=\u003c B,C,D,B \u003e$ 是 $X=\u003c A,B,C,B,D,A,B \u003e$ 的子序列。 公共子序列定义：给定两个序列 $X$ 和 $Y$，如果 $Z$ 是 $X$ 的子序列，也是 $Y$ 的子序列，则称它是 $X$ 和 $Y$ 的公共子序列。例如，如果 $X=\u003c A,B,C,B,D,A,B \u003e$，$Y=\u003c B,D,C,A,B,A \u003e$，则 $\u003c B,C,A \u003e$ 是它们的一个公共子序列，但不是最长公共子序列，$\u003c B,C,B,A \u003e$ 和 $\u003c B,D,A,B \u003e$ 才是最长公共子序列。 二、划分子问题，导出递推式 LCS的最优子结构定理：给定两个序列 $X=\u003c x_1, x_2, \\cdots, x_m \u003e$ 和 $Y=\u003c y_1, y_2, \\cdots, y_n \u003e$，设 $Z=\u003c z_1, z_2, \\cdots, z_k \u003e$ 为 $X$ 和 $Y$ 的任意 LCS。 1、如果 $x_m=y_n$，则 $z_k=x_m=y_n$，且 $Z_{k-1}$ 是 $X_{m-1}$ 和 $Y_{n-1}$ 的一个 LCS 2、如果 $x_m \\neq y_n$，且 $z_k \\neq x_m$ 意味着 $Z$ 是 $X_{m-1}$ 和 $Y$ 的一个 LCS 3、如果 $x_m \\neq y_n$，且 $z_k \\neq y_n$ 意味着 $Z$ 是 $X$ 和 $Y_{n-1}$ 的一个 LCS 证明： 1、已知 $x_m=y_n$，假设 $z_k \\neq x_m$ 或 $z_k \\neq y_n$，那么可以将 $x_m$ 或 $y_n$ 追加到 $Z$ 中，得到一个长度为 $k+1$ 的 LCS，与 $Z$ 是 LCS 矛盾。因此必然有 $z_k=x_m=y_n$ 。这样，前缀 $Z_{k-1}$ 是 $X_{m-1}$ 和 $Y_{n-1}$ 的一个 LCS。同样用反证法证明：假设存在 $X_{m-1}$ 和 $Y_{n-1}$ 的一个长度大于 $k-1$ 的公共子序列 $W$，那么如果将 $x_m=y_n$ 追加到 $W$ 的末尾会得到 $X$ 和 $Y$ 的一个长度大于 $k$ 的公共子序列，矛盾。 2、已知 $z_k \\neq x_m$，假设存在 $X_{m-1}$ 和 $Y$ 的一个长度大于 $k$ 的公共子序列 $W$，那么 $W$ 也是 $X_m$ 和 $Y$ 的一个长度大于 $k$ 的公共子序列，与 $Z$ 是 LCS 矛盾。 3、与情况2对称。 由上述定理可以推出： 1、如果 $x_m=y_n$，求 $X_{m-1}$ 和 $Y_{n-1}$ 的 LCS 2、如果 $x_m \\neq y_n$，求以下两个子问题的 LCS，取较长的那个作为最优解： 若 $z_k \\neq x_m$，求 $X_{m-1}$ 和 $Y$ 的 LCS 若 $z_k \\neq y_n$，求 $X$ 和 $Y_{n-1}$ 的 LCS 定义 $c[i,j]$ 表示 $X_i$ 和 $Y_j$ 的LCS的长度，由上述得出递推式： $$ c[i,j]= \\begin{cases} 0，\u0026 \\text{if $i=0$，或 $j=0$} \\\\ c[i-1,j-1]+1，\u0026 \\text{if $i,j0$ 且 $x_{i}=y_{j}$} \\\\ max(c[i-1,j],c[i,j-1])，\u0026 \\text{if $i,j0$ 且 $x_{i} \\neq y_{j}$} \\end{cases} $$ 三、实现 public static int LCSBottonUp(char[] X_0, char[] Y_0) { int x = X_0.length; char[] X = new char[x + 1]; for (int i = 0; i \u003c x; i++) X[i + 1] = X_0[i]; int y = Y_0.length; char[] Y = new char[y + 1]; for (int i = 0; i \u003c y; i++) Y[i + 1] = Y_0[i]; int[][] c = new int[x + 1][y + 1]; for (int i = 0; i \u003c= x; i++) { for (int j = 0; j \u003c= y; j++) { if (i == 0 || j == 0) c[i][j] = 0; else { if (X[i] == Y[j]) c[i][j] = c[i - 1][j - 1] + 1; else c[i][j] = Math.max(c[i - 1][j], c[i][j - 1]); } } } return c[x][y]; } public static void main(String[] args) { char[] X = {'A', 'B', 'C', 'B', 'D', 'B', 'A'}; char[] Y = {'B', 'D', 'C', 'A', 'B', 'A'}; System.out.println(LCSBottonUp(X, Y)); } 为了方便理解，通过预处理，使输入的序列下标从 $1$ 开始使用，即 $X[i]$ 表示 $x_i$，$Y[j]$ 表示 $y_j$，$c[i][j]$ 表示 $X_i$ 和 $Y_j$ 的公共子序列长度。由于外层循环的执行次数为输入序列 $X$ 的长度 $m$，内存循环的执行次数为输入序列 $Y$ 的长度 $n$，因此算法的时间复杂度为 $O(mn)$。另外使用了大小为 $m \\cdot n$ 的二维数组，因此空间复杂度为 $O(mn)$。 ","date":"2017-09-18","objectID":"/2017-09-18-%E7%AE%97%E6%B3%95-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97/:0:0","tags":null,"title":"最长公共子序列","uri":"/2017-09-18-%E7%AE%97%E6%B3%95-%E6%9C%80%E9%95%BF%E5%85%AC%E5%85%B1%E5%AD%90%E5%BA%8F%E5%88%97/"},{"categories":["Algorithm"],"content":"一、问题：给定 $n$ 个矩阵的链 $\u003c A_1, A_2, \\cdots , A_n \u003e$，其中矩阵 $A_i$ $ (1 \\leq i \\leq n)$ 的规模为 $p_{i-1} \\cdot p_i$，求完全括号化方案，使得计算乘积 $A_1A_2 \\cdots\\ A_n$ 所需标量乘法次数最少。 二、举例 以 $\u003c A_1, A_2, A_3 \u003e$ 相乘为例，假设三个矩阵的规模分别为 $10 \\cdot 100，100 \\cdot 5，5 \\cdot 50$ 。 如果按 $ ((A_1 \\cdot A_2) \\cdot A_3) $ 的顺序计算，则需要做 $ 10 \\cdot 100 \\cdot 5+10 \\cdot 5 \\cdot 50=5000+2500=7500 $ 次标量乘法运算。 如果按 $ (A_1 \\cdot (A_2 \\cdot A_3)) $ 的顺序计算，则需要做 $100 \\cdot 5 \\cdot 50+10 \\cdot 100 \\cdot 50=25000+50000=75000 $ 次标量乘法运算。 可以看到，第二种括号化方案导致的运算量是第一种情况的10倍。采取适当的括号化方案，能够极大的提高矩阵相乘运算的速度。 三、构造最优子结构，导出子问题递推式 对于 $n$ 个矩阵相乘进行括号化，可以先把前 $i$ $(1 \\leq i \\leq n-1)$ 个矩阵和后 $n-i$ 个矩阵加括号，也就是把这条矩阵链分成长度为 $i$ 和 $n-i$ 的两条子矩阵链，再继续对各子链递归地进行括号化。这样，矩阵括号化问题就类似于“钢条切割”问题了，只是这里求的是矩阵相乘的最小总标量乘法次数，而不是“钢条段的最大总价格”。 现在我们对某段矩阵链 $\u003c A_i \\cdots A_j \u003e$ 进行一次括号化，我们选择在第 $k$ $(1 \\leq k \\leq j-1)$ 个矩阵处断开该链，使得该链分成两条子链 $\u003c A_i \\cdots A_k \u003e$ 和 $\u003c A_{k+1} \\cdots A_j \u003e$ 。 设 $r[i,j]$ 表示链 $\u003c A_i \\cdots A_j \u003e$ 的总标量乘法次数，则由矩阵相乘的规则得 $$r[i,j]=r[i,k]+r[k+1,j]+p(i-1) \\cdot p(k) \\cdot p(j)$$ 式中 $r[i,k]$ 和 $r[k+1,j]$ 分别表示两条子链的各自的总标量乘法次数 $ p(i-1) \\cdot p(k) \\cdot p(j) $ 表示两条子链合并时的标量乘法次数 对上面公式，同理于“钢条切割问题”，如果 $r[i,j]$ 是对链 $\u003c A_i \\cdots A_j \u003e$ 括号化的所有方案中，总标量乘法次数最小（最优解），那么 $r[i,k]$ 和 $r[k+1,j]$ 一定是各自子链的最小总标量乘法次数（最优解）。 （因为当选定某个 $k$ 后，合并子链的代价随之确定，即公式中 $p(i-1) \\cdot p(k) \\cdot p(j)$ 部分。假设 $r[i,k]$ 和 $r[k+1,j]$ 不是两条子链各自的最优解，那么如果用它们各自的最优解（更小的 $r[i,k]$ 和 $r[k+1,j]$）代换进上面公式，那么必然产生比 $r[i,j]$ 更小的值，与 $r[i,j]$ 是最优解的假设矛盾。） 设 $m[i,j]$ 表示链 $\u003c A_i \\cdots A_j \u003e$ 的最小总标量乘法次数（最优解），由上述思考过程可以得出最优解递推式： $$ m[i,j]= \\begin{cases} 0, \u0026 \\text{if $i=j$} \\\\ min_{i \\leq k \\leq j-1} (m[i,k]+m[k+1,j]+p(i-1) \\cdot p(k) \\cdot p(j)), \u0026 \\text{if $i 四、动态规划法求解 public static int matrixChainBottonUp(int n, int[] s) { if (n \u003c 2) return 0; int[] p = new int[s.length + 1]; for (int i = 0; i \u003c s.length; i++) { p[i + 1] = s[i]; } int[][] m = new int[n + 1][n + 1]; for (int length = 0; length \u003c= n - 1; length++) { for (int i = 1; i + length \u003c= n; i++) { int j = i + length; if (i == j) m[i][j] = 0; else { int min = Integer.MAX_VALUE; for (int k = i; k \u003c= j - 1; k++) { int temp = m[i][k] + m[k + 1][j] + p[2 * i - 1] * p[2 * k] * p[2 * j]; if (temp \u003c min) min = temp; } m[i][j] = min; } } } return m[1][n]; } public static void main(String[] args) { int n1 = 2; int[] p1 = {30, 35, 35, 15, 15, 5, 5, 10, 10, 20, 20, 25}; int n2 = 3; int[] p2 = {10, 100, 100, 5, 5, 50}; System.out.println(matrixChainBottonUp(n1, p1)); } 为了便于理解，通过预处理使所有数组的下标从 $1$ 开始使用，这样第 $i$ 个矩阵的行和列就分别是 $p[2 \\cdot i-1]$ 和 $p[2 \\cdot i]$，二维 $m$ 数组的元素 $m[i][j]$ 表示矩阵链 $\u003c A_i \\cdots A_j \u003e$ 的最优解。 根据自底向上的思路，代码的计算过程为： m[1][1]，m[2][2]，m[3][3]，m[4][4]，m[5][5]，m[6][6] m[1][2]，m[2][3]，m[3][4]，m[4][5]，m[5][6] m[1][3]，m[2][4]，m[3][5]，m[4][6] m[1][4]，m[2][5]，m[3][6] m[1][5]，m[2][6] m[1][6] 其中，对 m[2][5]，算法会尝试在每一处$（2 \\leq k \u003c 5）$把链断开，然后求最优解： $$ \\begin{aligned} m[2][5] \u0026 = min \\begin{cases} m[2][2]+m[3][5]+p[2 \\cdot 2-1] \\cdot p[2 \\cdot 2] \\cdot p[5 \\cdot 2]=13000 \\\\ m[2][3]+m[4][5]+p[2 \\cdot 2-1] \\cdot p[3 \\cdot 2] \\cdot p[5 \\cdot 2]=7125 \\\\ m[2][4]+m[4][5]+p[2 \\cdot 2-1] \\cdot p[4 \\cdot 2] \\cdot p[5 \\cdot 2]=11375 \\end{cases} \\\\ \u0026 = 7125 \\end{aligned} $$ 由于循环嵌套的深度为三层，每层的循环变量 $i,j,k$ 最多取 $n-1$ 个值，因此算法的运行时间为 $O(n^3)$。另外还需要大小为 $n*n$ 的二维数组来存放中间过程，因此算法的空间开销为 $O(n^2)$。 ","date":"2017-09-17","objectID":"/2017-09-17-%E7%AE%97%E6%B3%95-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98/:0:0","tags":null,"title":"矩阵连乘","uri":"/2017-09-17-%E7%AE%97%E6%B3%95-%E7%9F%A9%E9%98%B5%E8%BF%9E%E4%B9%98/"},{"categories":["Algorithm"],"content":"一、问题：给定一段长度为 $n$ 的钢条和一个价格表 $p_i(i=1,2,…,n)$ ，求切割钢条方案，使得切割后所有钢条的总价格最大。钢条长度 $i$ 均为正整数，对应的价格为 $p_i$ : 长度 i 1 2 3 4 5 6 7 8 9 10 价格 pi 1 5 8 9 10 17 17 20 24 30 二、举例 以下用 $n$ 表示钢条长度，用 $r_n$ 表示切割长度为 $n$ 的钢条的最大总价格。 现在有一段长度为 3 的钢条，切割方案有： $n=3$，切割后总价：$r=8$ $n=1+2$，切割后总价：$r=1+5=6$ $n=1+1+1$，切割后总价：$r=1+1+1=3$ $n=2+1$，切割后总价：$r=5+1=6$ （这种方法其实和上面第二种一样） 综上，最优的切割方案是 $n=3$，即不切割直接销售，此时 $r_3$=8 三、划分子问题，导出递推式 现在考虑每一步的切割：暂时先不考虑整条销售的情况，那么需要对钢条进行切割以取得最大销售总价格。 我们规定，每一步切割只选择一个切割点进行一次切割，这次切割只把一段钢条一分为二，分成两条更小的钢条。 对一段长度为 $n$ 的钢条，因为最小切割长度为 1（钢条长度被规定为正整数），所以对该段钢条一共有 $n-1$ 个可选择的切割点。 对该钢条，假设在第 $i$（$1 \\leq i \\leq n-1$） 个切割点进行切割是最优的方案。该次切割后，把该段钢条分成更小的两段：$i$ 和 $n-i$ 。因此这次切割的总价格为： $$ r_n = r_i + r_{n-i} $$ 由这条公式显然可以推出，要使 $r_n$ 最大，$r_i$ 和 $r_{n-i}$ 也应该最大，因为 $r_n$ 是它们的和。 换句话说，要使 $r_n$ 是本次切割的最大总价（最优解），那么 $r_i$ 和 $r_{n-i}$ 必须分别是：对长度为 $i$ 的钢条进行切割的最大价格，对长度为 $n-i$ 的钢条进行切割的最大价格。反证法证明：假设本次切割的最大价格为 $r_n$，且 $r_n = r^{'}_i + r^{'}_{n-i}$，其中 $r^{'}_i$ 和 $r^{'}_{n-i}$ 分别是切割长度为 $i$ 和 长度为 $n-i$ 的钢条的总价格，但它们并不是最大各自的最大价格。现在我们用它们的最优解 $r_i(\u003e r^{'}_i)$ 和 $r_{n-i}(\u003e r^{'}_{n-i})$ 来替换公式中的 $r^{'}_i$ 和 $r^{'}_{n-i}$，将得到：$r_i + r_{n-i} \u003e r^{'}_i + r^{'}_{n-i} = r_n$，即存在比 $r_n$ 更大的总价格，与“ $r_n$是本次切割的最大价格 ”这一假设矛盾。因此如果 $r_n$ 是本次切割的最大价格，那么 $r_i$ 和 $r_{n-i}$ 必定也是切割各自钢条的所有方案中的最大价格，否则就会存在比 $r_n$ 更优的切割方案。 考虑到长度为 $n$ 的钢条一共有 $n-1$ 个切割点，以及不切割的方案，我们可以得到每一次切割的最优方案为： $$ r_n= \\begin{cases} p_1, \u0026 \\text{if $n=1$} \\\\ max ( p_n，r_1+r_{n-1}，\\cdots，r_{n-1}+r_1 ), \u0026 \\text{if $n1$} \\end{cases} $$ 四、递归方法求解 按照上面的递推式，可以直接写出用递归求解的代码 public static int cutSteelRecursive(int[] p, int n) { if (n == 1) return p[1]; int max = -1; for (int i = 1; i \u003c= n - 1; i++) { int temp = cutSteelRecursive(p, i) + cutSteelRecursive(p, n - i); if (temp \u003e max) max = temp; } return Math.max(p[n], max); } public static void main(String[] args) { int[] p = {0, 1, 5, 8, 9, 10, 17, 17, 20, 24, 30}; int result = cutSteelRecursive(p, n); System.out.println(result); } 但是用这样递归的方法会造成大量的重复计算，因为很多子问题都是重复的。如计算 $r_4$ 时，需要计算 $r_3$，$r_2$，$r_1$。其中计算 $r_3$ 时要先计算 $r_2$，$r_1$。计算好 $r_3$ 后返回最顶层，又要重新计算 $r_2$，$r_1$。 另外，长度为 $n$ 的钢条一共有 $n-1$ 个切割点，在每个切割点上，我们都可以有选择“切割”和“不切割”两种方案，所以一共有 $2^{n-1}$ 种切割方案。显然，递归的方法搜索了每一段钢条的每一个切割点的切割方案，因此复杂度为 $O(2^{n})$。 五、自底向上的动态规划 为了避免如上面递归方法对子问题重复计算，我们使用“备忘录”方法，把计算过的子问题的最优解存放到一个数组中，并从最小的子问题开始求解，再求解基于这个子问题的上一层的子问题，因此称为自底向上求解。 public static int cutSteelBottonUp(int[] p, int n) { int[] r = new int[n + 1]; for (int i = 1; i \u003c= n; i++) { int max = -1; for (int j = 1; j \u003c i; j++) { int temp = r[j] + r[i - j]; if (temp \u003e max) max = temp; } r[i] = Math.max(p[i], max); } return r[n]; } public static void main(String[] args) { int[] p = {0, 1, 5, 8, 9, 10, 17, 17, 20, 24, 30}; int result = cutSteelBottonUp(p, n); System.out.println(result); } 使用迭代的方法，从 $r_1$ 开始逐步计算 $r_2$，$r_3$ 直到 $r_n$，在每次计算出第 $i$ 个子问题的最优解后，立即存储到数组元素 $r[i]$ 中，供后续的求解利用：int temp = r[j] + r[i - j]; 对第 $i$ 个子问题 $r[i]$，内部循环一共执行 $i-1$ 次。当算法输入规模为 $n$ 时，总执行次数为： $0+1+2+\\cdots+n-1=\\frac{(n-1)^{2}}{2}$。因此对长度为 $n$ 的钢条，自底向上带备忘的迭代求解方法的复杂度为 $O(n^2)$ 。 ","date":"2017-09-15","objectID":"/2017-09-15-%E7%AE%97%E6%B3%95-%E9%92%A2%E6%9D%A1%E5%88%87%E5%89%B2/:0:0","tags":null,"title":"钢条切割","uri":"/2017-09-15-%E7%AE%97%E6%B3%95-%E9%92%A2%E6%9D%A1%E5%88%87%E5%89%B2/"},{"categories":["Algorithm"],"content":"递归调用方法时，系统会把方法隐式地存到一个“系统栈”里面，即在方法即将进入下一层（即下一次调用自身）前，把本层内部的变量存储到系统的某些位置（我们不用关心），当递归调用从最底层开始回退，回退到本层的时候，方法会自动重新加载之前存储的数据。我们可以利用这一点来隐式地存储我们逆序目标栈过程中需要存储的数据。另外需要注意的是，在JAVA中按引用传参时，形参和实参指向同一个对象，此时方法内部对形参的操作实际上是对实参的操作。 import java.util.Stack; public void stackReverse(Stack\u003cInteger\u003e stack){ if(stack.isEmpty()) return else{ int botton=getAndRemoveBotton(stack); stackReverse(stack); stack.push(botton); } } public int getAndRemoveBotton(Stack\u003cInteger\u003e stack){ int top=stack.pop(); if(stack.isEmpty()) return top; else{ int t=getAndRemoveBotton(stack); stack.push(top); return t; } } public static void main(String[] args){ Stack\u003cInteger\u003e stack=new Stack\u003cInteger\u003e(); stack.push(1); stack.push(2); stack.push(3); stack.push(4); stack.push(5); stackReverse(stack); while(!stack.isEmpty()){ System.out.println(stack.pop()); } } stackReverse：依次取出栈底元素：1-\u003e2-\u003e3-\u003e4-\u003e5，存储在每一层的递归调用中（botton）。栈空后，方法开始回退，逐层执行push，（逆序地）把原栈底元素（botton）重新入栈：5-\u003e4-\u003e3-\u003e2-\u003e1。此时栈已逆序，算法完成。 getAndRemoveBotton：取出栈底元素，使栈的高度减一，其余元素顺序不变。该方法在每一层调用时先取出栈顶元素，一直递归地往下取到栈空（利用“系统栈”来存储目标栈的元素）为止。当栈被取空，说明刚被取出的元素为栈底元素。此时开始回退递归，在每一层把pop出来的栈顶元素（top）又push回去，保持了栈的顺序不变，并把栈底元素一层层return上来（t）。如此即可在不改变栈顺序的情况下取出栈底元素。 算法的关键是：巧妙利用方法递归调用、回退的顺序，和栈元素出栈、入栈的顺序的对应关系来实现操作过程中数据的存储。 ","date":"2017-09-13","objectID":"/2017-09-13-%E7%AE%97%E6%B3%95-%E4%BB%85%E7%94%A8%E9%80%92%E5%BD%92%E9%80%86%E5%BA%8F%E4%B8%80%E4%B8%AA%E6%A0%88/:0:0","tags":null,"title":"仅用递归逆序一个栈","uri":"/2017-09-13-%E7%AE%97%E6%B3%95-%E4%BB%85%E7%94%A8%E9%80%92%E5%BD%92%E9%80%86%E5%BA%8F%E4%B8%80%E4%B8%AA%E6%A0%88/"},{"categories":["Algorithm"],"content":"一、算法分析 1、RAM 模型 分析算法的结果意味着预测算法需要的资源。虽然有时候需要关心像内存、通信宽带或计算机硬件这类资源，但是对算法进行分析时，我们一般主要关注算法的计算时间。在能够分析一个算法之前，我们需要有一个实现技术的模型，在《算法导论》一书中，假定了一种通用的单处理器计算模型——随机访问机 (RAM) 来作为实现技术，算法可以用计算机程序来实现。在 RAM 模型中，指令一条一条地执行，没有并发操作。严格来说，我们应该精确定义 RAM 模型的指令以及代价。但是这样即乏味又对算法的设计与分析没有太多意义。因此，在 RAM 模型中，我们不对计算机的内层次如高速缓存、虚拟内存等进行建模，把精力集中于算法的数学上的复杂度分析。 2、运行时间 一个算法在特定输入上的运行时间是指执行的基本操作数或者步数。我们认为计算机执行每一行代码需要的时间为常量（这个观点与 RAM 模型是一致的）。因此，我们核心关注的问题是：在给定输入规模 n 条件下，算法代码执行完成并输出正确结果所需要的总步数（次数）。如插入排序算法在 n 个值的输入上的最坏运行时间为 $T(n)=an^2+bn+c$ 3、增长量级 现在我们做出一种更简化的抽象：我们更感兴趣的是运行时间的增长率或增长量级。所以我们只考虑公式中最重要的项（例如，$an^2$），因为当 n 很大时，低阶项相对来说不太重要。同时我们也忽略最重要的项的系数，因为对大的输入，在确定计算效率时，常量因子不如增长率重要。在忽略了低阶项和最重要的项的系数后，只剩下最重要的项的因子。我们记某算法最坏情况下运行时间为 $\\Theta(g(n))$，如插入排序的的最坏运行时间为 $\\Theta(n^2)$。如果一个算法的最坏情况运行时间比另一个算法具有更低的增量级，那么我们认为前者比后者更有效。例如，一个 $\\Theta(n^2)$ 的算法，在足够大的输入规模 n 下，比一个 $\\Theta(n^3)$ 的算法要快。 二、渐近记号 1、$\\Theta$ 记号 对一个给定的函数 $g(n)$，用 $\\Theta(g(n))$ 来表示以下函数的集合： $\\Theta(g(n))={ f(n):存在正常量c_1,c_2和n_0,对所有n \\geq n_0,有 0 \\leq c_1g(n) \\leq f(n) \\leq c_2g(n) } $ 字面上来说：若存在正常量 $c_1,c_2和n_0$，使得对足够大的 n ，函数 $f(n)$ 能被夹入 $c_1g(n)与c_2g(n)$ 之间，则 $f(n)$ 属于集合 $\\Theta(g(n))$ 。我们记 $f(n)=\\Theta(g(n))$ 以表示 $f(n)$ 是函数集合 $\\Theta(g(n))$ 的成员。换句话说，对所有 $n \\geq n_0$，函数 $f(n)$ 在一个常量因子内等于 $g(n)$ 。我们也称 $g(n)$ 是 $f(n)$ 的一个渐近确界。 直觉上来说，我们可以通过忽略低阶项和最高阶项系数来直接求出 $\\Theta(g(n))$ , 如 $\\frac{1}{2}n^2 - 3n = \\Theta(n^2)$。（只需证明存在正数 $c_1和c_2$ 使 $c_1n^2 \\leq \\frac{1}{2}n^2 - 3n \\leq c_2n^2$ 即可。） 2、$O$ 记号 $\\Theta$ 记号渐近地给出一个函数的上界和下界。对于给定函数 $f(n)$，当只有一个渐近上界时，使用 $O$ 记号，表示以下函数的集合： $O(g(n))={ f(n):存在正常量c和n_0，使得对所有n \\geq n_0，有0 \\leq f(n) \\leq cg(n) }$ 3、$\\Omega$ 记号 正如 $O$ 记号提供了一个函数的渐近上界，$\\Omega$ 记号提供了渐近下界。对于给定函数 $f(n)$，用 $\\Omega$ 表示以下函数的集合： $\\Omega(g(n))={ f(n):存在正常量c和n_0，使得对所有n \\geq n_0，有0 \\leq cg(n) \\leq f(n) }$ 4、$o$ 记号 由 $O$ 记号提供的渐近上界可能是也可能不是渐近紧确的。我们使用 $o$ 记号来表示一个非渐近紧确的上界： $o(g(n))={ f(n):存在正常量c和n_0，使得对所有n \\geq n_0，有0 \\leq f(n) \u003c cg(n) }$ 例如，$2n^2=O(n^2)$，$2n=o(n^2)$，但 $2n^2 \\neq o(n^2)$。 5、$\\omega$ 记号 类似地，我们使用 $\\omega$ 记号来表示一个非渐近紧确的下界： $\\omega(g(n))={ f(n):存在正常量c和n_0，使得对所有n \\geq n_0，有0 \\leq cg(n) \u003c f(n) }$ 例如，$\\frac{n^2}{2}=\\Omega(n^2)$，$\\frac{n^2}{2}=\\omega(n)$，但 $\\frac{n^2}{2} \\neq \\Omega(n^2)$。 三、类比总结 $f(n)=O(g(n))$ 类似于 $a \\leq b$ $f(n)=\\Omega(g(n))$ 类似于 $a \\geq b$ $f(n)=\\Theta(g(n))$ 类似于 $a = b$ $f(n)=o(g(n))$ 类似于 $a \u003c b$ $f(n)=\\omega(g(n))$ 类似于 $a \u003e b$ 另，实数的三分性不能携带到渐近记号。 ","date":"2017-09-06","objectID":"/2017-09-06-%E7%AE%97%E6%B3%95-%E6%B8%90%E8%BF%91%E8%AE%B0%E5%8F%B7/:0:0","tags":null,"title":"渐近记号","uri":"/2017-09-06-%E7%AE%97%E6%B3%95-%E6%B8%90%E8%BF%91%E8%AE%B0%E5%8F%B7/"},{"categories":["MachineLearning"],"content":"一、高斯混合模型 为简洁表达，以下把第 $k$ 个高斯模型 $\\phi(y\\mid\\theta_k)$ 简记为 $\\phi_k(y)$ 那么高斯混合模型为： $$ P(y\\mid\\theta)=\\alpha_1\\cdot\\phi_1(y) + \\alpha_2\\cdot\\phi_2(y) + \\cdots + \\alpha_K\\cdot\\phi_K(y) $$ 二、高斯混合模型参数估计的EM算法 输入：观测数据 ($y_1,y_2,\\cdots,y_N$)，高斯混合模型 输出：高斯混合参数模型 (1) 取参数的初始值开始迭代 (2) E步：依据当前模型参数，计算每个分模型对每个观测数据的响应度 以第一个高斯模型 $\\phi_1$ 为例： 对 $y_1$ 响应度： $$ \\hat{\\gamma}_{11}= \\frac {\\alpha_1\\cdot\\phi_1(y_1)} {\\alpha_1\\cdot\\phi_1(y_1) + \\alpha_2\\cdot\\phi_2(y_1) + \\cdots + \\alpha_K\\cdot\\phi_K(y_1)} $$ 对 $y_2$ 响应度： $$ \\hat{\\gamma}_{12}= \\frac {\\alpha_1\\cdot\\phi(y_2\\mid\\theta_1)} {\\alpha_1\\cdot\\phi_1(y_2) + \\alpha_2\\cdot\\phi_2(y_2) + \\cdots + \\alpha_K\\cdot\\phi_K(y_2)} $$ $$\\cdots$$ 对 $y_N$ 响应度： $$ \\hat{\\gamma}_{1N}= \\frac {\\alpha_1\\cdot\\phi(y_2\\mid\\theta_1)} {\\alpha_1\\cdot\\phi_1(y_N) + \\alpha_2\\cdot\\phi_2(y_N) + \\cdots + \\alpha_K\\cdot\\phi_K(y_N)} $$ (3) M步：计算新一轮迭代的模型参数 $$ \\hat{\\mu}_1= \\frac {\\hat{\\gamma}_{11} \\cdot y_1 + \\hat{\\gamma}_{12} \\cdot y_2 + \\cdots + \\hat{\\gamma}_{1N} \\cdot y_N} {\\hat{\\gamma}_{11} + \\hat{\\gamma}_{12} + \\cdots + \\hat{\\gamma}_{1N}} $$ $$ \\hat{\\sigma}_1^2= \\frac {\\hat{\\gamma}_{11} \\cdot (y_1-\\mu_1)^2 + \\hat{\\gamma}_{12} \\cdot (y_2-\\mu_1)^2 + \\cdots + \\hat{\\gamma}_{1N} \\cdot (y_N-\\mu_1)^2} {\\hat{\\gamma}_{11} + \\hat{\\gamma}_{12} + \\cdots + \\hat{\\gamma}_{1N}} $$ $$ \\hat{\\alpha}_1= \\frac {\\hat{\\gamma}_{11} + \\hat{\\gamma}_{12} + \\cdots + \\hat{\\gamma}_{1N}} {N} $$ 对每个高斯模型都如此求出每一轮迭代的新参数 $(\\hat{\\alpha}_k，\\hat{\\mu}_k，\\hat{\\sigma}_k^2)$ (4) 重复第(2)步和第(3)步，直到收敛。 如图： ","date":"2017-08-25","objectID":"/2017-08-25-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84em%E6%8E%A8%E5%AF%BC%E7%AE%80%E5%8D%95%E7%89%88/:0:0","tags":null,"title":"高斯混合模型的EM推导简单版","uri":"/2017-08-25-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84em%E6%8E%A8%E5%AF%BC%E7%AE%80%E5%8D%95%E7%89%88/"},{"categories":["MachineLearning"],"content":"一、书 《统计学习方法》 李航 《机器学习》 周志华 《机器学习实战》/《MachineLearning in Action》 Peter Harrington 《集体编程智慧》/《Programming Collective Intelligence》 Toby Segaran 《An Introduction to Optimization》 E. Chong, S. Zak 《Convex Optimization》 Stephen Boyd, Lieven Vandenberghe 《推荐系统实践》 项亮 《MachineLearning》 Tom Mitchell(2008年出版，比较古老) 《Neural Networks and Learning Machines》 Simon Haykin(偏数学理论) 《The Elements of Statistical Learning》 Trevor Hastie，etc(看完这本就不用看其他机器学习的书了) 二、视频(Coursera) 《机器学习基石》 台湾大学, 林轩田 《MachineLearning》 Standford, Andrew Ng(吴恩达) 讲义 三、博客 tornadomeet机器学习笔记 The Missing Roadmap to Self-Study Machine Learning A Tour of Machine Learning Algorithms Best Machine Learning Resources for Getting Started 机器学习的最佳入门学习资源 机器学习入门资源不完全汇总 机器学习，数据挖掘在研究生阶段大概要学些什么？-知乎 机器学习、数据挖掘 如何进阶成为大神？-知乎 机器学习的数学基础-知乎 四、实战 kaggle的练习和比赛 自己实现算法的公式推导和core code python各种库如pandas等的熟练运用 五、总结 在学习理论的时候，多注意实践 重点是你要学ML干什么：最快最方便的使用一个通用方法做一个分类器或者回归器(例如验证码识别,etc)；从理论上想改进某一种机器学习的学习算法或数据结构(模型)；更好的了解各种机器学习算法的特点，应对不同的问题，选择不同的方法；利用现在的硬件产品(显卡，集群)，更好的实现一套机器学习算法。 如果想找这方面的工作，尤其是大企业的工作，主要看你发了哪些论文，开发了哪些系统，而不是看你用XX开源软件多熟练。你懂的，毕竟熟练使用一个软件并不是这个领域的关键问题。 一个有用的用于建立知识点图谱的工具metacademy，帮助你快速掌握你所需要的知识点，而不用盲目搜索浪费时间。 入门分两种级别，第一种是应用级，读几本书跟几门课程，多做实现和编程练习，就可以快速掌握用机器学习解决问题的能力。第二种是科研级，非常难，要做出成绩，需要找一个靠谱的导师。 机器学习比较偏向数学问题的推导，所以在顶会上的很多paper更看重idea，不是很看重实验是否来源于真实数据。 ","date":"2017-08-20","objectID":"/2017-08-20-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB/:0:0","tags":null,"title":"机器学习入门资料汇总","uri":"/2017-08-20-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E8%B5%84%E6%96%99%E6%B1%87%E6%80%BB/"},{"categories":["MachineLearning"],"content":"一、高斯混合模型： $$P(y \\mid \\theta) = \\sum_{k=1}^{K} \\alpha_k \\cdot \\phi(y \\mid \\theta_k)$$ 其中，$\\alpha_k \\text{是系数，} \\alpha_k \\geq0 \\text{，且 } \\sum_{k=1}^{K} \\alpha_k=1 \\text{；}$ $\\phi(y \\mid \\theta_k) = \\frac{1}{\\sqrt{2\\pi}} exp (-\\frac{(y-\\mu_k)^2}{2\\sigma_k^2}) \\text{，}\\theta_k = (\\mu_k ,\\sigma_k^2)\\text{ 是第k个高斯模型。}$ 二、高斯混合模型参数估计的EM算法 输入：观测数据 ($y_1,y_2,\\cdots,y_N$)，高斯混合模型 输出：高斯混合参数模型 (1) 取参数的初始值开始迭代 (2) E步：依据当前模型参数，计算分模型 k 对观测数据 $y_j$ 的响应度 $$\\hat{\\gamma}_{jk} = \\frac{\\alpha_k \\cdot \\phi(y_j \\mid \\theta_k)}{\\sum_{k=1}^{K} \\alpha_k \\cdot \\phi(y_j \\mid\\theta_k)}\\text{，}j=1,2,\\cdots,N;k=1,2,\\cdots,K$$ (3) M步：计算新一轮迭代的模型参数 $$\\hat{\\mu}_k =\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{jk} \\cdot y_j}{\\sum_{j=1}^{N} \\hat{\\gamma}_{jk}}\\text{，}k=1,2,\\cdots,K$$ $$\\hat{\\sigma}_k^2 = \\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{jk}\\cdot (y_j - \\mu_k)^2}{\\sum_{j=1}^{N} \\hat{\\gamma}_{jk}}\\text{，}k=1,2,\\cdots,K$$ $$\\hat{\\alpha}_k =\\frac{\\sum_{j=1}^{N} \\hat{\\gamma}_{jk}}{N}\\text{，}k=1,2,\\cdots,K$$ (4) 重复第(2)步和第(3)步，直到收敛。 三、上述算法基于似然估计进行推导： (1). 似然函数： $$ \\begin{aligned} P(y,\\gamma \\mid \\theta) \u0026= \\prod_{j=1}^{N} P(y_j, (\\gamma_{j1},\\cdots,\\gamma_{jK}) \\mid \\theta) \\\\ \u0026=\\prod_{k=1}^{K} \\prod_{j=1}^{N} [\\alpha_k \\cdot \\phi(y_j \\mid \\theta_k)]^{\\gamma_{jk}} \\\\ \u0026=\\prod_{k=1}^{K} \\alpha_k \\prod_{j=1}^{N} [\\phi(y_j \\mid \\theta_k)]^{\\gamma_{jk}} \\\\ \u0026=\\prod_{k=1}^{K} \\alpha_k \\prod_{j=1}^{N} [\\frac{1}{\\sqrt{2\\pi}} exp(- \\frac{(y-\\mu_k)^2}{2\\sigma_k^2})]^{\\gamma_{jk}} \\\\ \\end{aligned} $$ 式中，$n_k=\\sum_{j=1}^{N} \\gamma_{jk}$，$\\sum_{k=1}^{K} n_k=N$。 (2) 在 $E$ 步确定 $Q$ 函数： $$ \\begin{aligned} Q(\\theta,\\theta^{(i)}) \u0026= E[logP(y,\\gamma \\mid \\theta) \\mid y, \\theta^{(i)}] \\\\ \u0026= \\cdots \\\\ \u0026= \\sum_{k=1}^{K} \\{ n_k \\cdot log\\alpha_k + \\sum_{j=1}^{N} [log(\\frac{1}{\\sqrt{2\\pi}}) - log(\\sigma_k)- \\frac{1}{2\\sigma_k^2}(y_j-\\mu_k)^2] \\} \\end{aligned} $$ (3) 在 $M$ 步中，求 $Q(\\theta,\\theta^{(i)})$ 对 $\\theta$ 的极大值，即求新一轮迭代的模型参数： $$ \\theta^{(i)} = \\underset{\\theta}{\\operatorname{argmax}} Q(\\theta,\\theta^{(i)}) $$ 求 $\\hat{\\mu}_k，\\hat{\\sigma}_k^2$ 只需将上式分别对 $\\hat{\\mu}_k，\\hat{\\sigma}_k^2$ 求导并令导数等于0即可，这是常规的求极值的方法。但是对于 $\\hat{\\alpha}k$，因为存在着 $\\sum{k=1}^{K} \\alpha_k = 1$ 这个条件，所以这里要注意对有多个有关系的变量的求偏导问题。 (4) 对 $\\alpha_k$ 求偏导： $$ \\frac{\\partial Q}{\\partial \\alpha_k} = \\frac{\\partial\\{\\sum_{k=1}^{K} [ n_k \\cdot log(\\alpha_k) ]\\} } {\\partial \\alpha_k} \\\\ s.t. \\quad \\sum_{k=1}^{K} \\alpha_k = 1 $$ 由于存在和为1的这个条件，各个变量之间存在联系，所以不能简单的分别对各个 $\\alpha_k$ 求偏导。如果忽略这个约束，那么每个概率都可以无限大，目标函数也无限大了。 当对其中一个变量如 $\\alpha_1$ 求偏导的时候，考虑到总和为1的约束条件，此时应该把剩下的 $K-1$ 个变量看作“另一个”变量(可取其中一个变量如 $\\alpha_K$ 作为代表)： $$ \\begin{aligned} \\frac{\\partial Q}{\\partial \\alpha_1} \u0026= \\frac{\\partial\\{ n_1 \\cdot log(\\alpha_1) + \\sum_{k=2}^{K} n_k \\cdot log(\\alpha_k)\\}} {\\partial \\alpha_1} \\\\ \u0026= \\frac{n_1}{\\alpha_1} + \\frac{n_K \\cdot \\partial log(\\alpha_K)}{\\alpha_1} \\\\ \u0026= \\frac{n_1}{\\alpha_1} + \\frac{n_K}{\\alpha_K} \\cdot \\frac{\\partial \\alpha_K}{\\alpha_1} \\\\ \u0026= \\frac{n_1}{\\alpha_1} + \\frac{n_K}{\\alpha_K} \\cdot \\frac{\\partial (1-\\sum_{k=1}^{K-1} \\alpha_k)}{\\alpha_1} \\\\ \u0026= \\frac{n_1}{\\alpha_1} + \\frac{n_K}{\\alpha_K} \\cdot \\frac{\\partial (-\\alpha_1)}{\\partial \\alpha_1} \\\\ \u0026= \\frac{n_1}{\\alpha_1} - \\frac{n_K}{\\alpha_K} \\end{aligned} $$ 令 $$\\frac{\\partial Q}{\\partial \\alpha_1}=\\frac{n_1}{\\alpha_1} - \\frac{n_K}{\\alpha_K}=0$$ 得 $$\\frac{n_1}{\\alpha_1} = \\frac{n_K}{\\alpha_K}$$ 同理 $$\\frac{n_1}{\\alpha_1} =\\frac{n_2}{\\alpha_2} =\\cdots =\\frac{n_{K-1}}{\\alpha_{K-1}} =\\frac{n_K}{\\alpha_K}$$ 因此 $$\\frac{n_1}{\\alpha_1}=\\frac{n_1+\\cdots+n_K}{\\alpha_1+\\cdots+\\alpha_K}=\\frac{N}{1}=N$$ 因此 $$\\alpha_1=\\frac{n_1}{N},\\cdots,\\alpha_K=\\frac{n_K}{N}$$ 即 $$\\alpha_k=\\frac{n_k}{N},k=1,2,\\cdots,K$$ 求解完毕。 ","date":"2017-08-20","objectID":"/2017-08-20-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84em%E6%8E%A8%E5%AF%BC/:0:0","tags":null,"title":"高斯混合模型的EM推导","uri":"/2017-08-20-%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E6%A8%A1%E5%9E%8B%E7%9A%84em%E6%8E%A8%E5%AF%BC/"},{"categories":["Spark"],"content":"十二、运行Spark分布式程序 按照前面博客设置好Library后，选择Modules，可以看到导入的包列表如图（注意，这里包的顺序是会对项目的编译造成影响的，好像要把spark的包放在scala的包之前，否则编译的时候会报一些找不到某些函数的错误，具体问题忘了记录了，这个可以自己慢慢试试，百度上也有解决方案） 到这里，就可以运行单机/本地模式的spark程序了。要跑本地程序，命令如下（scala）： val conf=new SparkConf().setAppName(“XXXX”).setMaster(“local”) 但是如果要运行集群模式的spark程序，必须还要将程序项目打包，并将这个程序分发到集群各个节点运行。要使程序分布式运行，命令如下： val conf=new SparkConf().setAppName(“XXXX”).setMaster(“spark://master:8070”).setJars(List(“XXX”)) 其中的setMaster()里面的主机名和端口号需要根据你自己的实际配置进行设定，见第十节。而setJars()是本项目经过编译打包后的jar包的路径，要运行分布式程序必须添加这个包，否则会报错说workers找不到各种类 项目打包步骤如下： (1) 进入 Artifacts-\u003eJAR-\u003eFrom modules with … (2) 填入所建立的scala class的名字 (3) 选择Build on make，并记下项目打包输出的位置（重要！！） 设置完后，编译本项目。第一次编译的时候先不要在源码使用.setJars()这个函数。让编译通过。编译通过后，打开控制台，通过刚才的目录找到生成的Jar包。修改源码，加入这个包。在这里可以删除 “output root” 下的scala和spark的包，因为这两个包的导出没有什么意义。 (4) 根据路径找到与项目同名的JAR包： (5) 运行Spark的分布式程序 (6) 正确运行结果 注意，如果程序运行一半，没有打印结果直接结束，有可能是内存不足导致程序直接死亡。这种情况下，可能不会报任何错误，在 WEB UI 上也只是显示 Finished 。我当时给master分配的内存从2G扩展到4G，才解决了这个问题，正确运行出结果。 参考： 使用IDEA开发SPARK提交remote cluster执行 Windows下IntelliJ IDEA中调试Spark Standalone Spark入门实战系列–3.Spark编程模型（下）–IDEA搭建及实战 十三、使用Spark的机器学习库MLlib 我运行了其中的k-means算法，比较简单这里不再赘述，可参考： 使用 Spark MLlib 做 K-means 聚类分析 Scala Standard Library Spark学习1： 基础函数功能解读 ","date":"2016-07-05","objectID":"/2016-07-05-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-6/:0:0","tags":null,"title":"Spark完全分布式搭建与使用(6)","uri":"/2016-07-05-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-6/"},{"categories":["Spark"],"content":"十一、搭建Spark程序开发环境IDEA 1、下载地址：IDEA Download 2、解压并移动到路径：/usr/local/idea-IC-145.597.3 3、安装，进入 /usr/local/idea-IC-145.597.3/bin，运行 [root@master ~]# idea.sh 4、在安装导向过程中，选择下载并安装SCALA插件。参考：Spark入门实战系列–3.Spark编程模型（下）–IDEA搭建及实战 5、安装完成后，新建SCALA工程：File-\u003eNew Project-\u003eScala-\u003eScala-\u003eNext，然后填入Project的名字，选择项目目录、SDK、Scala SDK，一般默认即可。然选择Finish。 6、在src文件夹下直接新建一个scala类对象：右键 src-\u003enew-\u003escala class，命名，并填入代码。（这里要注意，在src文件下不要再建立嵌套的source类型的文件（像src这样的蓝色的文件夹），否则无法运行。关于这一点，上面的参考帖子里面是错的，也可能他那个是旧版本才支持的，这个问题具体我没有深入研究，我取消了中间嵌套的所有source文件夹就可以正常运行了） 7、设置library，导入jar包。添加两个即可，一个是Scala SDK Library，这里选择scala-2.10.4版本。另一个是$SPARK_HOME/lib/spark-assembly-1.1.0-hadoop2.2.0.jar (1) 进入设置界面 (2) 添加JAVA包 (3) 找到本地的Spark的安装目录下的lib，选择assembly包 (4) 添加Scala SDK (5) 选择本地安装的scala-2.10.4 ","date":"2016-07-05","objectID":"/2016-07-05-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-5/:0:0","tags":null,"title":"Spark完全分布式搭建与使用(5)","uri":"/2016-07-05-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-5/"},{"categories":["Spark"],"content":"十、SPARK WEB UI问题 因为spark有本地模式Local、伪分布式Local-cluster、完全分布式Standalone（集群模式） 三种运行模式。如果不用集群模式去运行spark，在 master:8090 是看不到执行任务的记录的。只能在 master:4040 里面看得到。但是4040只能在任务执行过程中看到，任务执行完后该端口会自动关闭，任务记录被撤销。 spark默认是运行本地模式，例如：$ spark-shell 要运行集群模式，使用： [root@master ~]# spark-shell --master spark://master:8070 这个端口号要按照8090页面顶头的提示来设置： 参考： Spark on YARN两种运行模式介绍 Spark运行模式 spark 介绍及本地模式、集群模式安装 ","date":"2016-06-29","objectID":"/2016-06-29-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-4/:0:0","tags":null,"title":"Spark完全分布式搭建与使用(4)","uri":"/2016-06-29-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-4/"},{"categories":["Spark"],"content":"九、安装Spark-1.6.1-bin-hadoop2.6 1、下载：Download Apache Spark 2、将安装包移动到到路径：/usr/local/，并解压 [root@master ~]# cd /usr/local/ [root@master ~]# tar -xzvf spark-1.6.1-bin-hadoop2.6.tar.gz 3、配置环境变量 [root@master ~]# vim /etc/profile export SPARK_HOME=/usr/local/spark-1.6.1-bin-hadoop2.6 export PATH=${SPARK_HOME}/bin: ${SPARK_HOME}/sbin 现在可以将所有的PATH整合到一起： export JAVA_HOME=\"/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.101.x86_64\" export JRE_HOME=${JAVA_HOME}/jre export CLASS_PATH=.:${JAVA_HOME}/lib: ${JRE_HOME}/lib export SCALA_HOME=/usr/lib/scala-2.10.4 export HADOOP_HOME=/usr/local/hadoop-2.7.2 export SPARK_HOME=/usr/local/spark-1.6.1-bin-hadoop2.6 export PATH=$PATH:${JAVA_HOME}/bin:${SCALA_HOME}/bin:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin:${SPARK_HOME}/bin: ${SPARK_HOME}/sbin 配置完成后 [root@master ~]# source /etc/profile 同步到worker1和worker2 [root@master ~]# scp /etc/profile root@worker1:/etc/profile [root@master ~]# ssh root@worker1 [root@worker1 ~]# source /etc/profile [root@worker1 ~]# exit [root@master ~]# scp /etc/profile root@worker2:/etc/profile [root@master ~]# ssh root@worker1 [root@worker2 ~]# source /etc/profile [root@worker2 ~]# exit 4、配置Spark文件 [root@master ~]# cd /usr/local/spark-1.6.1-bin-hadoop2.6/conf step 1：spark-env.sh [root@master ~]# cp spark-env.sh.template spark-env.sh [root@master ~]# vim spark-env.sh export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.101.x86_64 export SCALA_HOME=/usr/lib/scala-2.10.4 export SPARK_HOME=/usr/local/spark-1.6.1-bin-hadoop2.6 export SPARK_MASTER_IP=master export SPARK_MASTER_PORT=8070 export SPARK_MASTER_WEBUI_PORT=8091 export SPARK_WORKER_INSTANCES=2 export SPARK_WORKER_MEMORY=1g export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.2/etc/hadoop step2：slaves.template [root@master ~]# cp slaves.template slaves [root@master ~]# vim slaves // 加入 worker1 worker2 step 3：复制spark到worker1和worker2 [root@master ~]# scp –r /usr/local/spark-1.6.1-bin-hadoop2.6 root@worker1:/usr/local/spark-1.6.1-bin-hadoop2.6 [root@master ~]# scp –r /usr/local/spark-1.6.1-bin-hadoop2.6 root@worker2:/usr/local/spark-1.6.1-bin-hadoop2.6 5、启动Spark集群 [root@master ~]# /usr/local/spark-1.6.1-bin-hadoop2.6/sbin/start-all.sh 用命令 jps 查看，如果有master和worker进程说明启动成功。可以通过 http://master:8091 查看集群情况和运行的applications。也可以在 http://master:4040 查看application运行的job的具体信息 6、用例测试，略。 7、本文参考： Spark安装启动 and 在程序中调用spark服务 Spark 伪分布式 \u0026 全分布式 安装指南 Spark1.6.0 on Hadoop-2.6.3 安装配置 Spark1.4.1完全分布集群hadoop2.6.0上部署 ","date":"2016-06-22","objectID":"/2016-06-22-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-3/:0:0","tags":null,"title":"Spark完全分布式搭建与使用(3)","uri":"/2016-06-22-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-3/"},{"categories":["Spark"],"content":"八、安装Hadoop-2.7.2 说明：如果是想用 “Spark on Standalon” 模式就不用安装Hadoop，如果想用 “Spark on Yarn” 或者需要去hdfs取数据则应先装Hadoop 参见： CentOS7 上安装Hadoop 2.7.2 的安装 和 初步使用） Hadoop2.6, Red hat Linux 6.6 x64集群完全分布式安装 CentOS7安装Hadoop2.7完整流程 1、下载地址：Apache Hadoop Releases Download 这里我选择binary文件进行下载，这种是已经编译好的源码，下载的文件名是hadoop-2.6.4.tar.gz；如果喜欢自己编译，可以选择source文件进行下载，下载的文件名是hadoop-2.7.2-src.tar.gz。（参考：hadoop两个安装包分别是干啥的？） 2、将安装包移动到到路径：/usr/local/，并解压 [root@master ~]# cd /usr/local [root@master ~]# tar -xzvf hadoop-2.7.2.tar.gz 3、判断Hadoop版本 [root@master ~]# /usr/local/hadoop-2.7.2/bin/hadoop version 4、配置环境变量 [root@master ~]# vim /etc/profile export HADOOP_HOME=/usr/local/hadoop-2.7.2 export PATH=$PATH:{HADOOP_HOME}/bin:{HADOOP_HOME}/sbin [root@master ~]# source /etc/profile 同步到worker1和worker2 [root@master ~]# scp /etc/profile root@worker1:/etc/profile [root@master ~]# ssh root@worker1 [root@worker1 ~]# source /etc/profile [root@worker1 ~]# exit [root@master ~]# scp /etc/profile root@worker2:/etc/profile [root@master ~]# ssh root@worker2 [root@worker2 ~]# source /etc/profile [root@worker2 ~]# exit 5、配置Hadoop文件 step 1：在master本地创建以下文件夹 /home/Hadoop/name /home/hadoop/data /home/hadoop/temp step 2：进入目录 /usr/local/hadoop-2.7.2/etc/hadoop/ ，修改配置文件，共7个 (1) hadoop-env.sh：修改 JAVA_HOME 值 export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.101.x86_64 (2) yarn-env.sh：修改 JAVA_HOME 值 export JAVA_HOME=/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.101.x86_64 (3) slaves：写入 worker1 worker2 (4) core-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003efs.defaultFS\u003c/name\u003e \u003cvalue\u003ehdfs://master:9000\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003ehadoop.tmp.dir\u003c/name\u003e \u003cvalue\u003efile:/home/hadoop/temp\u003c/value\u003e \u003cdescription\u003eAbase for other temporary directories.\u003c/description\u003e \u003c/property\u003e \u003c/configuration\u003e (5) hdfs-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003edfs.namenode.secondary.http-address\u003c/name\u003e \u003cvalue\u003emaster:9001\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003edfs.namenode.name.dir\u003c/name\u003e \u003cvalue\u003efile:/home/hadoop/name\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003edfs.datanode.data.dir\u003c/name\u003e \u003cvalue\u003efile:/home/hadoop/data\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003edfs.replication\u003c/name\u003e \u003cvalue\u003e2\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003edfs.webhdfs.enabled\u003c/name\u003e \u003cvalue\u003etrue\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e (6) mapred-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003emapreduce.framework.name\u003c/name\u003e \u003cvalue\u003eyarn\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003emapreduce.jobhistory.address\u003c/name\u003e \u003cvalue\u003emaster:10020\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003emapreduce.jobhistory.webapp.address\u003c/name\u003e \u003cvalue\u003e master:19888\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e (7) yarn-site.xml \u003cconfiguration\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.aux-services\u003c/name\u003e \u003cvalue\u003emapreduce_shuffle\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.nodemanager.aux-services.mapreduce.shuffle.class\u003c/name\u003e \u003cvalue\u003eorg.apache.hadoop.mapred.ShuffleHandler\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.resourcemanager.address\u003c/name\u003e \u003cvalue\u003e master:8032\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.resourcemanager.scheduler.address\u003c/name\u003e \u003cvalue\u003emaster:8030\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.resourcemanager.resource-tracker.address\u003c/name\u003e \u003cvalue\u003emaster:8031\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.resourcemanager.admin.address\u003c/name\u003e \u003cvalue\u003e master:8033\u003c/value\u003e \u003c/property\u003e \u003cproperty\u003e \u003cname\u003eyarn.resourcemanager.webapp.address\u003c/name\u003e \u003cvalue\u003e master:8088\u003c/value\u003e \u003c/property\u003e \u003c/configuration\u003e step 3：复制整个hadoop目录到worker1和worker2 [root@master ~]# scp –r /usr/local/hadoop-2.7.2 root@worker1:/usr/local/hadoop-2.7.2 [root@master ~]# scp –r /usr/local/hadoop-2.7.2 root@worker2:/usr/local/hadoop-2.7.2 6、启动 hadoop step 1：进入安装目录 [root@master ~]# cd /usr/local/hadoop-2.7.2 step 2：格式化 namenode，成功的话会看到“successfully formatted”和“exitting with status 0” [root@master ~]# ./bin/hdfs namenode -format 注意，以后重新格式化可能导致datanode无法启动，需要手动更新集群ID，参考： 重新格式化HDFS的方法 Hadoop中重新格式化namenode step 3：启动 hdfs [root@master ~]# ./sbin/start-dfs.sh step 4：启动 yarn [root@master ~]# ./sbin/start-yarn.sh step 5：用 jps 命令来查看是否启动成功，若成功会看到master上面有 namenode secondarynamenode ","date":"2016-06-10","objectID":"/2016-06-10-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-2/:0:0","tags":null,"title":"Spark完全分布式搭建与使用(2)","uri":"/2016-06-10-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-2/"},{"categories":["Spark"],"content":"一、实验环境 宿主机：winserver 2012 服务器一台 目标：在三台虚拟机上搭建Spark完全分布式集群，一台master，两台slaves 二、安装虚拟机oracle virtualbox 下载地址：Download VirtualBox 三、安装 CentOS-6.7-x86_64.iso 镜像文件 1、下载地址：CentOS Mirror 2、导入到virtualbox中进行安装。（因为三台虚拟机将要进行差不多的软件安装，所以先在一台虚拟机上部署好了master，再将这个虚拟机完全复制两份作为slaves，然后再稍微修改配置文件，这样比较省事） 3、配置：名称为CentOS6_Master，CPU为2个，内存为2G，硬盘等其他参数默认安装。在配置CPU的时候可能无法使用，因为win8 之后的微软操作系统默认安装了Hyper-V虚拟机独占了虚拟资源，可以在：控制面板-\u003e程序和功能-\u003e启用或关闭Windows功能-\u003e启动“删除角色和功能”向导中进行禁用，问题解决。 4、安装好后从“存储”中删除CentOS镜像文件盘片，否则每次启动虚拟机都是进行镜像安装而不是进入虚拟机。另外，安装好后如果宿主机可以上网，那么虚拟机也是可以立即上网的。 四、安装 java-1.7.0 1、下载安装并验证，参考：在CentOS上安装Java环境 2、设置JAVA_HOME环境变量（使用 ls –lrt /etc/alternatives/java 来定位jdk的安装路径，参考：Linux如何查看JDK的安装路径） // 使用vim编辑器进入配置文件 [root@master ~]# vim /etc/profile // 修改配置文件 export JAVA_HOME=\"/usr/lib/jvm/java-1.7.0-openjdk-1.7.0.101.x86_64\" （加入） export JRE_HOME=${JAVA_HOME}/jre export CLASS_PATH=.:${JAVA_HOME}/lib: ${JRE_HOME}/lib // 保存并退出编辑器 // 在terminal中输入命令使其生效 [root@master ~]# source /etc/profile （注意：我这里安装的是openjdk，与常见的sun jdk存放的位置目录不同，参见：CentOS中JAVA_HOME的环境变量设置） 3、测试是否安装成功 [root@master ~]# java -version 五、安装 scala-2.10.4 1、因为spark是1.6.1版本，对应scala要用2.10.x版本，否则无法兼容 下载地址：SCALA 2.10.4 2、解压到系统目录 /usr/lib/ 下 3、配置环境变量 [root@master ~]# vim /etc/profile export SCALA_HOME=/usr/lib/scala-2.10.4 export PATH=$PATH:${SCALA_HOME}/bin [root@master ~]# source /etc/profile 4、测试 [root@master ~]# scala –version 六、安装 SSH 1、查看SSH是否已安装 [root@master ~]# rpm –qa | grep ssh 2、安装 [root@master ~]# yum install openssh-server 3、启动/关闭/重启 [root@master ~]# service sshd start/stop/restart 4、验证：查看22端口 [root@master ~]# netstat –antp | grep sshd 5、设置开机自启/关闭 [root@master ~]# chkconfig sshd on/off 七、复制虚拟机并构建局域网 1、virtualbox虚拟机的网络设置形式有四种（详情参考：VirtualBox虚拟机网络设置（四种方式））： NAT网络地址转换模式：安装后的默认模式，虚拟机可以访问主机，主机不能访问虚拟机，虚拟机之间不能相互访问。即虚拟机不真实存在于网络中，网络服务由主机单向提供。 Bridged Adapter网桥模式：虚拟机能被分配到一个网络独立的IP，直接连接入网络中，真是存在于网络。因此虚拟机与主机、虚拟机与虚拟机之间都是可以相互访问的。 Internal内部网络模式：虚拟机与外部网络完全断开，只实现虚拟机之间的内部网络模式。虚拟无法访问主机，也不能通过主机连接外部网络。 Host-only Adapter完全主机模式：最复杂，虚拟出一张网卡供虚拟机使用，灵活性最高。 2、复制虚拟机，选择完全复制即可，修改名字为 CentOS6_Worker1 、 CentOS6_Worker2 3、配置局域网，使三台虚拟机能相互ping通，并且能SSH免密码登录（因为本来想设置最好的Bridged Adapter网桥模式，但是尝试了几次都失败了，最后采用比较简单的Internal内部网络模式。如果你也选择该模式，请提前下载好Hadoop和Spark的安装包，因为部署好内部网络后将与外部网络断开无法上网下载）： (1)、先在宿主机（winServer）的cmd中启动vituralbox的dhcp服务，设置一个vituralbox内部网络（参考：Virtualbox下使用internal networking做一个小局域网）： \u003eC:\\Program Files\\Oracle\\VirtualBox\u003eVBoxManage.exe dhcpserver add --netname intnet --ip 192.168.1.1 --netmask 255.255.255.0 --lowerip 192.168.1.1 --upperip 192.168.1.100 --enable (2)、修改虚拟机-\u003e设置-\u003e网络 (3)、启动三台虚拟机，分别添加新的网络连接配置“eth0”。其中，掩码都是255.255.255.0，网关都是192.168.1.1，各自IP如下： Master：192.168.1.2 Worker1：192.168.1.3 Worker2：192.168.1.4 设置完后，重启eth0端口 [root@master ~]# ifdown eth0 [root@master ~]# ifup eth0 查看网络设置 [root@master ~]# ifconfig 如果下图所示就说明设置好了。（安装的无桌面版本CentOS的高手请忽略） (4)、关闭三台虚拟机的防火墙，然后用ping分别进行测试（参考：centOS 6.5关闭防火墙步骤） (5)、设置每台虚拟机的网络主机名，这步只是为了方便配置，不用每次都写一堆那么长的IP地址。要先获得root权限才能进行如下操作（下面以master为例）： step 1： [root@master ~]# vim /etc/hosts // 进入该配置文件后，清掉原来的默认设置，然后加入： 127.0.0.1 localhost 192.168.1.2 Master 192.168.1.3 Worker1 192.168.1.4 Worker2 注意：这个配置文件很关键，一定要把原来默认的地址全部去掉，否则之后安装hadoop的网页UI会无法正常工作。参见：Hadoop在master查看live nodes为0解决方案 step 2： [root@master ~]# vim /etc/sysconfig/network //将里面的HOSTNAME改为master step 3：再使用hostname命令指定一次 [root@master ~]# hostname master step 4： 重新登录/重启一次，这时如果配置成功，就会看见命令提示字符串变成\"[root@master ~]#\" (6)、以上，三台机器都设置好各自网络主机名后，再用ping测试 (7)、设置master到两个worker虚拟机SSH免密码登录 step 1：在master上面进入root的.ssh目录 [root@master ~]# cd ~/.ssh //如果没有该目录，就用mkdir ~/.ssh建立并进入 step 2：使用 ssh-keygen –t rsa 来生成公钥和私钥，连续回车，不设置密码 step 3：把公钥文件复制到要其他机器的root用户目录下的.ssh目录（如果不是root用户，需要在前面加/home/） [root@master ~]# scp ~/.ssh/id_rsa.pub root@master:/root/.ssh/authorized_keys [root@master ~]# scp ~/.ssh/id_rsa.pub root@worker1:/root/.ssh/authorized_keys [root@master ~]# scp ~/.ssh/id_rsa.pub root@worker2:/root/.ssh/authorized_keys step 4：检测master是否可以不需要密码登录到worker1和worker2 [root@master ~]# ssh master [root@master ~]# ssh root@worker1 [root@master ~]# ssh root@worker2 至此，所有准备步骤基本完成，下一步可以","date":"2016-05-29","objectID":"/2016-05-29-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-1/:0:0","tags":null,"title":"Spark完全分布式搭建与使用(1)","uri":"/2016-05-29-spark-spark%E5%AE%8C%E5%85%A8%E5%88%86%E5%B8%83%E5%BC%8F%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8-1/"},{"categories":["Technology"],"content":"0、准备：centos7镜像文件刻录成U盘系统盘 Download CentOS 1、服务器是PowerEdge R720，安装系统前要做radis磁盘阵列 Dell R720上的系统安装问题的解决办法（关于RAID建立磁盘阵列的技术） 2、服务器插上U盘，重启，进入BIOS设置U盘启动，安装centos7(尽量安装英文版否则路径是中文会出现乱码，可以选择安装GNOME桌面版，适用于新手) centos7.0安装教程 3、配置IP,DNS,GATEWAY,NETMASK,成功之后可以上网 centos下网络配置方法(网关、dns、ip地址配置) 4、下载并安装LAMP(apache2/httpd，mysql/mariadb，PHP以及PHP相关组件一堆) Install Apache, PHP And MySQL On CentOS 7 (LAMP) 5、部署web网站，记得要从防火墙firewall或者iptables开放网站端口，还要留意SELinux这个东西，有可能在阻止你的网站写缓存文件，导致网站故障 CentOS 7 开放端口 redhat7 centos7 关闭SELinux和防火墙的办法 6、部署ftp网站，如果想用浏览器访问的话，需要设置允许被动模式，并且开发3000-4000端口，否则只能用ftp命令访问 CentOS配置VSFTP服务器 为何客户端软件可以而浏览器则不能连接FTP服务器 ","date":"2015-10-14","objectID":"/2015-10-14-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B010/:0:0","tags":null,"title":"技术笔记(10)-CentOS7部署","uri":"/2015-10-14-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B010/"},{"categories":["Projects"],"content":"一、简介 参赛作品是《面向物联网的无线无源风速仪》，设计一种面向物联网的无线无源小型三杯式风速仪，其工作原理是： (1) 无源：在有风时通过一个内置的小型发电装置，捕获风能，转化为电能供应自身工作，在无风时使用后备电源。为节省电源，大部分时间处于休眠状态，除非被上位机唤醒进行数据交互； (2) 无线：使用zigbee无线通讯模块，与上位机进行通信。 其中我实现的部分是上位机软件，主要功能：定时向串口发送数据采集命令，然后从串口读取风速仪返回的风速值，显示到屏幕上。 二、上位机软件的实现 我用 WinForm + MS.Chart 的方式来实现，用 WinForm 来制作窗体，用 MS.Chart 来实现风速值的实时显示。上位机的采集数据的过程包括：定时、读写串口、CRC校验、字符串和十六进制等的数值转换、更新图形界面、监控报警最大风速值等。关键代码： 初始化 //用于更新图形化界面的事件委托 delegate void UpdateTextEventHandler_experimental(List\u003cdouble\u003e experimentalWSList); UpdateTextEventHandler_experimental updateTextEventHandler_experimental; delegate void UpdateChartEventHandler_experimental(string experimentalWSstr); UpdateChartEventHandler_experimental updateChartEventHandler_experimental; //初始化串口 public void InitCOM_experimental(string PortName) { experimentalPort = new SerialPort(PortName); experimentalPort.BaudRate = 9600;//波特率 experimentalPort.Parity = Parity.None;//无奇偶校验位 experimentalPort.StopBits = StopBits.One;//一个停止位 experimentalPort.DataBits = 8;//8位数据位 experimentalPort.DataReceived += experimentalPort_DataReceived;//接收返回数据时触发的事件 } 接收数据，更新图形化界面 //串口接收到风速仪返回数据时触发的事件 private void experimentalPort_DataReceived(object sender, SerialDataReceivedEventArgs e) { //读取缓冲区的数据，这里应该先处理读取的数据 int[] buffer = new int[7]; for (int i = 0; i \u003c buffer.Length; i++) { buffer[i] = experimentalPort.ReadByte(); } double newWindSpeed = Convert.ToDouble(DataHandler_ExperimentalMachine(buffer)); speedWarnTest(newWindSpeed); experimentalWSList.Add(newWindSpeed); //更新Textbox updateTextEventHandler_experimental = new UpdateTextEventHandler_experimental(UpdateTextBox_experimental); this.Invoke(updateTextEventHandler_experimental, new List\u003cdouble\u003e[] { experimentalWSList }); //更新Chart updateChartEventHandler_experimental = new UpdateChartEventHandler_experimental(UpdateChart_experimental); this.Invoke(updateChartEventHandler_experimental, new string[] { newWindSpeed.ToString() }); } 发送数据采集命令 //用于发送采集命令的定时器 private void SendCommand_Timer() { myTimer = new System.Timers.Timer(); myTimer.Elapsed += new ElapsedEventHandler(SendCommand_TimedEvent); myTimer.AutoReset = true; myTimer.Enabled = true; myTimer.Interval = 1000; } //发送采集命令事件 private void SendCommand_TimedEvent(object sender, System.Timers.ElapsedEventArgs e) { SendCommand_ExperimentalMachine(); } //发送采集命令具体操作：写数据到串口 private void SendCommand_ExperimentalMachine() { //ID 03 00 00 00 01 CR0 CR1；十六进制，ID为设备地址，CR0，CR1为CRC校验位，其他固定 byte[] commandByte = { 0x01, 0x03, 0x00, 0x00, 0x00, 0x01 }; //01 03 00 00 00 01 0A 84 byte[] commandByteCRC = CRC(commandByte, commandByte.Length); experimentalPort.Write(commandByteCRC, 0, commandByteCRC.Length); } //实验机型CRC校验算法 private byte[] CRC(byte[] byteArray, int datalen) { byte CRC16Lo, CRC16Hi, CL, CH, SaveHi, SaveLo; int i, Flag; CRC16Lo = 0xFF; CRC16Hi = 0xFF; CL = 0x01; CH = 0xA0; for (i = 0; i \u003c datalen; i++) { CRC16Lo ^= byteArray[i]; for (Flag = 0; Flag \u003c 8; Flag++) { SaveHi = CRC16Hi; SaveLo = CRC16Lo; CRC16Hi \u003e\u003e= 1; CRC16Lo \u003e\u003e= 1; if ((SaveHi \u0026 0x01) == 0x01) { CRC16Lo |= 0x80; } if ((SaveLo \u0026 0x01) == 0x01) { CRC16Hi ^= CH; CRC16Lo ^= CL; } } } byte[] newByteArray = new byte[datalen + 2]; for (int j = 0; j \u003c datalen; j++) { newByteArray[j] = byteArray[j]; } newByteArray[datalen] = CRC16Lo; newByteArray[datalen + 1] = CRC16Hi; //return (CRC16Hi \u003c\u003c 8) | CRC16Lo; return newByteArray; } 三、调试 1 先使用虚拟串口，让COM1和COM2连接 2 打开串口助手，连接COM1，模拟风速仪 3 启动上位机软件，连接COM2，测试数据采集命令的发送、风速值的采集和显示 ","date":"2015-08-27","objectID":"/2015-08-27-%E9%A1%B9%E7%9B%AE-%E5%85%A8%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%8A%82%E8%83%BD%E5%87%8F%E6%8E%92%E7%A4%BE%E4%BC%9A%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%A7%91%E6%8A%80%E7%AB%9E%E8%B5%9B/:0:0","tags":null,"title":"全国大学生节能减排社会实践与科技竞赛","uri":"/2015-08-27-%E9%A1%B9%E7%9B%AE-%E5%85%A8%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%8A%82%E8%83%BD%E5%87%8F%E6%8E%92%E7%A4%BE%E4%BC%9A%E5%AE%9E%E8%B7%B5%E4%B8%8E%E7%A7%91%E6%8A%80%E7%AB%9E%E8%B5%9B/"},{"categories":["Projects"],"content":"一、简介 参赛作品是《基于四旋翼无人机的输电线路智能巡检系统》，用无人机代替人工对高压电线塔进行智能巡检。其智能体现于： 根据高压电线塔的地理坐标自动计算出高压线塔检测点的位置 根据两塔之间的距离和气温、风速、覆冰等气象条件计算两塔之间高压电线的应力弧垂 自动制定飞行计划，实现无人控制，自动巡检 无人机硬件： 其中我负责的部分是无人机飞控系统的开发： 二、飞控系统的开发 该系统基于一个开源的飞控地面站系统——MissionPlanner进行改造。我所做的工作有： 清理多余的面板和插件，只留下我们需要的功能，使软件更加简洁好用。 加入一些新的小功能，如从文件读入高压输电塔的经纬度、在地图上选取高压输电塔的坐标点并且将其转换成经纬度显示在面板上等等。 加入自动生成巡检点的算法。该算法根据用户在地图上选取的点，或者从文件中读入的高压输电塔的经纬度，自动计算出关键巡检点，并生成飞行计划。 该软件是基于WinForm实现的，因此前面两点都不难实现，只是引用的插件和使用的控件比较多，理清各个层次的引用关系略复杂。此处不再赘述。 现在来分析第3点的算法。在用户添加一个航点(高压输电塔)的时候，我们能获取的数据只有塔的经度、纬度、海拔，那怎么计算出它周围对应的12个航点呢？这个问题的关键是要求出两个塔之间的电线的走向。我的思路是，地球的经纬度相当于地面的坐标轴，考虑到我们的监测范围是一个面积很小的范围，而且电线塔与电线塔之间的距离也不大，我们可以把地图的经纬度近似作为一个直角坐标系，经度作为X轴，纬度作为Y轴，于是地图上的每个航点就抽象成了一个包含坐标的点。 有了这个坐标，我们就可以把本航点和上一个航点连起来，而这条线段就是两个电线塔之间的电线。因为有了坐标轴，于是现在我们可以把电线塔的“走向”进行量化——斜率。这样，每两个塔之间的电线走向就可以轻易得到： $$k=\\frac{y_0-y_1}{x_0-x_1}$$ 有了斜率之后，我们就可以给每个塔的周围的对应的12个监测航点进行定位了，如图 (说明：图中的 $(x',y')$ 是前一个航点，$(x_0,y_0)$ 是要计算的本行航点。图中下方的1、2、3、4分别就是四个位置的监测航点的坐标位置了。而每个位置均有3个监测航点，只是高度不同而已，可以通过代码进行设置) 核心代码: //添加一个塔，自动计算出12个航点 private void AddTower_Click(object sender, EventArgs e) { double towerlat = Convert.ToDouble(TXT_towerlat.Text.ToString().Trim()); double towerlong = Convert.ToDouble(TXT_towerlng.Text.ToString().Trim()); //先往地图中加入塔点 AddWPToMap(towerlat, towerlong, 0); //根据塔点自动生成的航点 //上一个塔点 PointLatLngAlt wp_last = pointlist[pointlist.Count - 2]; //该塔点 PointLatLngAlt wp_this = pointlist[pointlist.Count - 1]; //通过两点的斜率k求夹角a(PI) double k = (wp_last.Lat - wp_this.Lat) / (wp_last.Lng - wp_this.Lng); double a = Math.Atan(k); //监测半径r double r = 0.0001; //添加4*4=16个监视航点 //第一角度4个航点 double Lat1 = (float)(wp_this.Lat + r * Math.Cos(0.25 * Math.PI - a)); double Lng1 = (float)(wp_this.Lng - r * Math.Sin(0.25 * Math.PI - a)); int Alt1 = 100; AddWPToMap(Lat1, Lng1, Alt1); AddWPToMap(Lat1, Lng1, Alt1 - 5); AddWPToMap(Lat1, Lng1, Alt1 - 8); AddWPToMap(Lat1, Lng1, Alt1); //第二角度4个航点 double Lat2 = (float)(wp_this.Lat + r * Math.Sin(0.25 * Math.PI - a)); double Lng2 = (float)(wp_this.Lng + r * Math.Cos(0.25 * Math.PI - a)); int Alt2 = 100; AddWPToMap(Lat2, Lng2, Alt2); AddWPToMap(Lat2, Lng2, Alt2 - 5); AddWPToMap(Lat2, Lng2, Alt2 - 8); AddWPToMap(Lat2, Lng2, Alt2); //第三角度4个航点 double Lat3 = (float)(wp_this.Lat - r * Math.Cos(0.25 * Math.PI - a)); double Lng3 = (float)(wp_this.Lng + r * Math.Sin(0.25 * Math.PI - a)); int Alt3 = 100; AddWPToMap(Lat3, Lng3, Alt3); AddWPToMap(Lat3, Lng3, Alt3 - 5); AddWPToMap(Lat3, Lng3, Alt3 - 8); AddWPToMap(Lat3, Lng3, Alt3); //第四角度3个航点 double Lat4 = (float)(wp_this.Lat - r * Math.Sin(0.25 * Math.PI - a)); double Lng4 = (float)(wp_this.Lng - r * Math.Cos(0.25 * Math.PI - a)); int Alt4 = 100; AddWPToMap(Lat4, Lng4, Alt4); AddWPToMap(Lat4, Lng4, Alt4 - 5); AddWPToMap(Lat4, Lng4, Alt4 - 8); AddWPToMap(Lat4, Lng4, Alt4); //再次加入该塔点，让飞机飞回塔顶 AddWPToMap(towerlat, towerlong, 0); } ","date":"2015-05-28","objectID":"/2015-05-28-%E9%A1%B9%E7%9B%AE-%E6%8C%91%E6%88%98%E6%9D%AF%E6%B2%B3%E5%8C%97%E7%9C%81%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AF%BE%E5%A4%96%E5%AD%A6%E6%9C%AF%E7%A7%91%E6%8A%80%E4%BD%9C%E5%93%81%E7%AB%9E%E8%B5%9B/:0:0","tags":null,"title":"挑战杯河北省大学生课外学术科技作品竞赛","uri":"/2015-05-28-%E9%A1%B9%E7%9B%AE-%E6%8C%91%E6%88%98%E6%9D%AF%E6%B2%B3%E5%8C%97%E7%9C%81%E5%A4%A7%E5%AD%A6%E7%94%9F%E8%AF%BE%E5%A4%96%E5%AD%A6%E6%9C%AF%E7%A7%91%E6%8A%80%E4%BD%9C%E5%93%81%E7%AB%9E%E8%B5%9B/"},{"categories":["Projects"],"content":"一、简介 我们参加的是2015年2月6日到10日的美赛，组队的时候已经接近学期末。而且我们三人中，只有一人参加过国赛，没有拿奖。其余两人包括我是完全没有比赛经验的。我们三人正式开始准备的时候，已经是2015年1月中下旬了吧，具体记得不太清楚了，貌似是大三上学期结束，已经放寒假了我们才有时间开始准备比赛。相比他们那些参加过各种培训的校队选手，有的甚至整个大二暑假都在学校做模拟训练，我们这样的业余队伍几乎是没有优势的。 在放假之后比赛之前，我们做了一些小训练，拿了往年的题目做，有：连环杀人案侦破、葡萄酒评价、树叶分类、碎纸片复原等等。因为时间太紧，只有通过直接上手实战来快速学习。我觉得这样的学习方式有好的地方也有不好的地方，好的地方在于效率高速度快能够体验成就感，形成良好的反馈机制，而不好的地方在于这样学习不系统不全面，知识体系松散，难以打好扎实的基础，或者说“功底”。 6日早上八点拿到赛题，我们讨论一小会儿，就确定了选择 MCM 的 A 题，对抗埃博拉病毒： PROBLEM A: Eradicating Ebola The world medical association has announced that their new medication could stop Ebola and cure patients whose disease is not advanced. Build a realistic, sensible, and useful model that considers not only the spread of the disease, the quantity of the medicine needed, possible feasible delivery systems, locations of delivery, speed of manufacturing of the vaccine or drug, but also any other critical factors your team considers necessary as part of the model to optimize the eradication of Ebola, or at least its current strain. In addition to your modeling approach for the contest, prepare a 1-2 page non-technical letter for the world medical association to use in their announcement. 二、思路 确定题目后，经过短暂的讨论，主题思路很快就清晰了：如何“对抗”埃博拉病毒？自然就是通过隔离病人、控制疫情、注射药物。具体怎么用数学模型去做？显然分三部： 建立埃博拉病毒的传播模型。有了这个模型，我们就可以预测病毒传播的发展情况，根据它的发展规律定制相应的疫情控制措施，这是整篇论文根基。 有了传播模型后，用该模型来预测至少需要多少的药物才能把疫情控制在可控的范围内。 确定好疫苗需求量后，最后一个问题就是，如何建立生产、运输、分配疫苗的整个体系？（很自然会联想到线性规划问题） 三、Modeling 1、建立埃博拉传播模型。我们先用传统的 S.I.R 模型对实际数据进行拟合，发现效果很差，因为这个模型是一个不考虑外界条件干扰的模型，例如政府的介入、世界卫生组织的先进医疗器械援助等，而且每种病毒由于感染期、潜伏期、患病期、致死期的时间都不一样。因此我们放弃了这个理想化的传播模型，而采用元胞自动机模型。我们通过调整参数来拟合实际数据。模型按实际情况分为两部分，一部分是传染病爆发后紧接着的一段时间内，政府和医疗机构还未采取有效的遏制措施，患病人数接近于自然增长；另一部分则是较长的一段时间后，政府和医疗机构采取了较为有效的防范和隔离措施，患病人数得到控制，在波动中减少。模型测试的效果比较好，因为我们在元胞自动机模型里面考虑了很多有实际意义的参数。 2、预测药物需求量。分别计算三个疫情国家 Guinea、Liberia 和 Sierra Leone 的药物需求量。药量的计算为当前药物的需求量与半年内增加患者的药物需求量和药物的储存量三部分之和。其中储存量的多少根据每周增加患者人数最多的一段时间的病人总量来确定，也就是考虑最严重的情况。对于疫苗量的计算我们首先估计一个合适免疫覆盖率，用模型一的改进模型中每人每天传染人数的改变（蒙特卡罗算法加以实现）来表示免疫覆盖率，然后根据患病者人数的变化来确定合适的免疫覆盖率。最后用相关公式计算出疫苗的需求量。 3、确定具体的药品和疫苗的配送方案。模型分为两部分，一部分为国际之间药物的配送，一部分为国内药物的配送。我们首先根据相关设施的发展水平给定四个出产国的产量。通过药物和疫苗总量的限制，不同国家的总成本应该相近等条件对其进行规划，从而得到具体的配送方案。国内的配送首先将发病地点用模糊-C聚类分析的方法分类，然后在每一类中产生最小生成树，也就是总代价最少。不同的类用不同的车运送药品，同一类中的地点用同一辆车按照最小生成树的路线运送。 (1). 国际运送：用线性规划确定从药物生产国到每个灾情国的配给量。 国内运送（以 Sierra Leone）为例：先对灾点进行聚类，再使用最小生成树找出投递药物的最小总路径。 四、总结（关于编程方面） 1、工具：熟练使用matlab，稍微熟悉使用spss等等其他工具。没必要花太多时间在研究工具上面，熟练使用matlab基本就足够。 2、算法：最好有一定的编程功底，了解面向对象和面向过程编程，这样写起程序来会快很多。一些基本的数学方法，例如线性规划、插值、拟合等等要在比赛之前能够自己写出来。进一步，有时间的话，最好能拓展一下一些智能算法，例如模拟退火、遗传算法、蒙特卡洛法、神经网络、自动元胞机等等。因为基本的数学模型大家都会，所以想要让自己的论文出彩，就要用一些一般人想不到的或者实现不了的方法，因为美赛要求的不是结果的准确性，而是解法的新颖性，思路的发散性。 3、技巧：编程说难也不难，说不难也难。如果你不是那种天才型的编程玩家，那就唯有多练，不断地动手实践，调试，改错，总结，一步步提高自己，这确实是一个很费时间和精力的学习任务，以我个人来看，看一个算法10遍，不如自己动手写一遍的印象来得深刻。 4、最后谈一点感受，虽然参加美赛时间仓促，但是我们三人都拿出了最大的努力，比赛这四天，基本每天都只睡不到3个小时。真的很累，比赛结束交了论文的那天，在坐火车回家的路上，站着都能睡着，整个人都感觉有点飘忽或者说精神有点恍惚。但是，那种和朋友们一起拼命为同一个目标奋斗的感觉真的很棒，很开心，也觉得很充实。每每回想起那些日子，我都认为，那就是我本科阶段，最珍贵最开心的时光了。至于论文，第一部分使用了元胞自动机模型，知道这个东西的人应该挺少吧，可以算是论文的一大亮点，对美赛的风格来说。第二部分的预测药物量不太好，模型有点过于简单。第三部分的药物运送分为国际和国内两部分处理，考虑了成本，时间，地理各种因素，我觉得也是一个可以加分的部分。最后拿了一等奖，遗憾也不是很大了，毕竟我们准备的时间真的太少，又没什么经验，我们只是临时组成的队伍。现在两位队友都走上了各自想走的路，但我仍然希望，日后还有一起努力奋斗的机会。 ","date":"2015-05-20","objectID":"/2015-04-20-%E9%A1%B9%E7%9B%AE-%E7%BE%8E%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B/:0:0","tags":null,"title":"美国大学生数学建模竞赛","uri":"/2015-04-20-%E9%A1%B9%E7%9B%AE-%E7%BE%8E%E5%9B%BD%E5%A4%A7%E5%AD%A6%E7%94%9F%E6%95%B0%E5%AD%A6%E5%BB%BA%E6%A8%A1%E7%AB%9E%E8%B5%9B/"},{"categories":["Projects"],"content":"一、简介 参赛作品是“基于物联网的车辆事故智能报警系统”，它对车辆运行情况进行实时监控。一旦发生车辆事故导致安全气囊打开时，系统便会检测到这一状态，并立即获取事故的定位信息、人员信息并进行自动报警。当交警服务中心接收到报警电话和相关信息后，就会立即通知离事故发生地最近的医院前往救援。 系统工作流程： 1、 当车载客户端监测到安全气囊打开后，它先从 OBD 系统中读取出车辆故障码等车辆信息，记录到单片机内存中，然后启动蓝牙模块，把车辆信息发送到手机客户端； 2、当手机客户端收到车辆故障码后， (1)、启动第一重自动报警：从手机内存卡读出驾车人员信息，启动 GPS 对车辆进行定位读出车辆经纬度，然后把车辆信息、人员信息、定位信息一并通过 GPRS 发送到交警信息中心服务器； (2)、启动第二重自动报警：调用手机的拨号功能，自动拨打 110 报警电话； (3)、当手机报警功能被触发后， 手机开始倒计时 15 秒。用户可以在 15 秒内点击取消按钮中断报警， 否则在计时结束后执行报警任务。 3、当交警信息中心服务器收到手机发送过来的报警信息后，会马上把报警信息显示在屏幕上，并响起警铃，通知交警人员，前往事故现在进行处理。 二、实现 我负责开发的部分是 Server 端的“交警信息中心”网站，和 Client 端的“EmergencyCall”安卓 App。 1、Server 端 基于 .NET 的 WebService 框架，采用 Model -\u003e DAL -\u003e BLL 三层架构。Server 端的主要功能是：提供 WebService 方法给安卓 客户端调用，接收其发送过来的数据，经过处理后插入数据库，最后显示到网页上。 Server 端主要的两个文件是：网站主页 TICSystemDefault.aspx 和 WebService 接口 TICSystemWebService.asmx。网站主页每3秒自动刷新一次，把所有数据即时地显示到网页上。WebService 接口负责和安卓端数据交互。其他文件分别实现解析Json字符串并转化成Model、操作数据库、绑定数据到网页等等。 其中提供给安卓端调用的 WebService 方法： // webservice方法 [WebMethod] public int AddAccidentInfo(string JasonAccidentInfo) { AccidentInfoModel AccidentInfo = ModelJsonChange\u003cAccidentInfoModel\u003e.JsonToModel(JasonAccidentInfo); return accidentDal.AddAccidentInfo(AccidentInfo); } //Helper类，实现Json和Model的互转 public class ModelJsonChange\u003cT\u003e { //Model转换为Json public static string ModelToJson(T model) { JavaScriptSerializer js = new JavaScriptSerializer(); return js.Serialize(model); } //Json转换为Model public static T JsonToModel(string json) { JavaScriptSerializer js = new JavaScriptSerializer(); return js.Deserialize\u003cT\u003e(json); } } 2、Client 端 Client 端的安卓 App 名为“EmergencyCall”。 其中分为五大功能模块： (1). SaveActivity：保存、修改用户的个人信息。我使用了SharedPreferences来保存这些数据。SharedPreferences是Android平台上一个轻量级的存储类，用来保存应用的一些常用配置。 关键代码（为节省篇幅，以下均省略 OnCreate、setContentView、addActivity、findViewById 等基本的常用代码，只显示实现某功能的关键代码）： // 声明 private EditText EditText_userName;// 姓名 private EditText EditText_userPhone;// 电话 private EditText EditText_userCar;// 车牌 private SharedPreferences sharedPrefrences; private Editor editor; private static final String FILENAME = \"filename\";// 要存储的文件名 // 进入页面时，将已经保存在文件中的内容取出，显示到三个文本框中 sharedPrefrences = this.getSharedPreferences(FILENAME, MODE_PRIVATE); EditText_userName.setText(sharedPrefrences.getString(\"userName\", \"\")); EditText_userPhone.setText(sharedPrefrences.getString(\"userPhone\", \"\")); EditText_userCar.setText(sharedPrefrences.getString(\"userCar\", \"\")); // 保存按钮 savebutton.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View v) { editor = getSharedPreferences(FILENAME, MODE_PRIVATE).edit(); editor.putString(\"userName\", EditText_userName.getText().toString()); editor.putString(\"userPhone\", EditText_userPhone.getText().toString()); editor.putString(\"userCar\", EditText_userCar.getText().toString()); editor.commit(); Intent intent = new Intent(); intent.setClass(SaveActivity.this, MainActivity.class); startActivity(intent); } }); (2). BlueToothActivity：蓝牙连接，使手机通过蓝牙与车上的硬件模块通讯。 关键代码： // 通过蓝牙连接硬件 BluetoothAdapter btAdapt = BluetoothAdapter.getDefaultAdapter(); BluetoothDevice btDev = btAdapt.getRemoteDevice(address);// \"00:11:00:18:05:45\" Method m = btDev.getClass().getMethod(\"createRfcommSocket\", new Class[] { int.class }); BluetoothSocket btSocket = (BluetoothSocket) m.invoke(btDev, Integer.valueOf(1)); try { btSocket.connect(); Log.e(TAG, \" BT connection established, data transfer link open.\"); Toast.makeText(testBlueTooth.this, \"连接成功\", Toast.LENGTH_SHORT).show(); // 进入护航界面 Intent intent = new Intent(); intent.setClass(testBlueTooth.this, DisplayActivity.class); startActivity(intent); } catch (IOException e) { Log.e(TAG, \" Connection failed.\", e); setTitle(\"连接失败..\"); } (3). DisplayActivity：连接成功后，手机将一直监控车载硬件模块的状态，接收硬件模块发送过来的状态码。 关键代码： // 监听车载硬件模块发送过来的消息 ConnectedThread manageThread = new ConnectedThread(); Handler mHandler = new MyHandler(); manageThread.Start(); public ConnectedThread() { isRecording = false; this.wait = 50; thread = new Thread(new ReadRunnable()); } private class ReadRunnable implements Runnable { public void run() { while (isRe","date":"2015-01-20","objectID":"/2015-01-20-%E9%A1%B9%E7%9B%AE-%E5%85%A8%E5%9B%BD%E9%AB%98%E6%A0%A1%E7%89%A9%E8%81%94%E7%BD%91%E5%BA%94%E7%94%A8%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B/:0:0","tags":null,"title":"全国高校物联网应用创新大赛","uri":"/2015-01-20-%E9%A1%B9%E7%9B%AE-%E5%85%A8%E5%9B%BD%E9%AB%98%E6%A0%A1%E7%89%A9%E8%81%94%E7%BD%91%E5%BA%94%E7%94%A8%E5%88%9B%E6%96%B0%E5%A4%A7%E8%B5%9B/"},{"categories":["Technology"],"content":"一、数据库 1.数据库的选定 use testDB; 2.数据库的创建（后面两个是设置字符集和排序方式）： create database(if not exist) testDB (character set charset)(collate collation); 3.数据库的删除： drop database testDB; 4.数据库的更变： alter database testDB(character set charset)(collate collation); 二、数据表 1.存储引擎（查看存储引擎：show engines）： ARCHIVE——插入 BLACKHOLE——删除 EXAMPLE——示例 Falcon——事务 FEDERATED——访问远程数据 InnoDB——外键 MEMORY——内存数据表 MERGE——联合 MyISAM——默认 NDB——MySQL Cluster专用 CSV——使用逗号分隔数据项 2.数据表的创建： (1).数据表选项（选择存储引擎，否则默认为MyISAM）： create table testTB( ... ) engine=InnoDB; (2).创建全新的数据表： create table if not exist testTB( ... ) engine=InnoDB; (3).创建临时数据表： create temporary table testTB( ... ) (4).使用查询结果或者其它数据表创建数据表： i.如果要确保新表和原表的数据列的属性、索引都完全一致： create table testTB like exampleTB; insert into testTB select * from exampleTB; ii.如果只要保证列名和数据一致，不需复制数据列的属性和索引： create table testTB select * from exampleTB; iii.为了弥补上一点，可以使用cast()函数强制使用特定的属性： create table ... select cast(... as int) as ... , cast(... as char) as ... ; (5).使用MERGE数据表，设置插入方式（要联合的表必须是结构完全一致的MyISAM数据表）： create table test_4( ... ) engine=MERGE union=(test_1,test_2,test_3) insert_method=last; (6).使用分区数据表： create table testTB ( id int not null, name varchar(20) null ) partition by range(id) ( partition p1 values less than 10, partition p2 values less than 20, partition p3 values less than 30, partition p4 values less than 40, partition p5 values less than MAXVALUE, ); (7).使用federated数据表： create table federated_testDB( ... ) engine=FEDERATED connection='mysql://wys:79135QW@hostname/testDB/testTB'; 为了避免暴露连接字符串中的信息，可以： create server testDB_server foreign data wrapper mysql options ( user 'wys' password '79135qw' host '...' database 'testDB' ); create table federated_testTB( ... ) engine=FEDERATED connection='testDB_server/testTB' 3.数据表的删除： drop table if exist testTB; 4.数据表的索引： (1).创建索引： // 在create内部 create table testTB( id int auto_increment primary key not null, ... index index_name(id),...auto ); // 或者在create外部 create table testTB(...); alter table testTB add index_name(id); (2).删除索引： drop index index_name on testTB; //或者 alter table testTB drop index index_name; 5.改变数据表的结构： (1).改变数据列的类型： alter table testTB modify id varchar(100); // 或者 alter table testTB change id id_newname varchar(100); (2).改变存储引擎： alter table testTB engine=InnoDB; (3).重新命名数据表： alter table testTB RENAME TO new_testTB; (4).把一个数据表从一个数据库移动到另一个数据库： rename table testDB.testTB to testDB_2.testDB 三、数据检索 1.获取数据库的元数据（各种show）： show database; show tables from ...; show colums from ... like ...; 2.利用联结操作对多表进行检索： select ... from ... inner/left/right/cross join ... on ...=... where ... group by ... order by ... having ... limit ... 3.使用子查询对多表进行检索： (1).in和not in：是否存在或者不存在子查询中 (2).all和any和some：all所有；any任何一个；some和any等价 (3).exist和not exist：判断子查询是否有结果集 4.使用union对多表进行检索： (select ... from ...) union (select ... from ...) union (select ... from ...) ... 5.使用视图（可用视图完成一些必要的数学运算）： ctreate view testView as select ... from ... 6.涉及多个数据表的删除和更新操作 //删除表t1中id与表t2匹配的行 delete t1 from t1 inner join t2 on t1.id=t2.id; // 删除表t1中id不与表t2匹配的行 delete t1 from t1 left join t2 on t1.id=t2.id where t2.id is null; //同时删除表t1和表t2中id匹配的行 delete t1,t2 from t1 inner join t2 on t1.id=t2.id; // 或者使用using delete from t1 using t1 inner join t2 on t1.id=t2.id; delete from t1 using t1 left join t2 on t1.id=t2.id where t2.id is null; delete from t1,t2 using t1 inner join t2 on t1.id=t2.id; 四、事务处理（需要使用的存储引擎是InnoDB或者Falcon） start transaction; // 或者set autocommit=0 insert into ... delete from ... commit; // set autocommit=1; 对应于上面的括号内的语句 1.事务的特点： a.原子性，事务内容要么全部成功，要么全部失败 b.隔离性，事务之间是隔离的，比如在某事务向数据库插入一些数据的时候，如果事务还没结束，即使某些数据已经插入去了，但是这些数据对其他所有用户是不可见的，直到事务完成提交 c.稳定性，数据在事务开始和结束后都是稳定的状态 d.可靠性，如果事务执行成功，它的影响将被永久保存到数据库 2.备注：在默认的情况下，MySQL从自动提交（autocommit）模式运行，这种模式会在每条语句执行完毕后把它当做一个事务进行提交，如果使用了 set autocommit=0 语句后，这个自动提交将被禁止，这样的结果是，除非遇到一些显式提交语句（如：commit，rollback…），或者隐式提交前一个事务，然后在本语句重新开始事务的DLL语句（如：alter table，create index，drop database，drop index…），系统会认为接下来的所有的语句都属于一个事务 3.使用事务保存点：使事务部分回滚 create table testTable(id int) engine=InnoDB; start transaction; in","date":"2014-12-28","objectID":"/2014-12-28-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B09/:0:0","tags":null,"title":"技术笔记(9)-MySql语法小结","uri":"/2014-12-28-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B09/"},{"categories":["Projects"],"content":"一、数据库 1、数据库文档：建立数据库前，要根据需求把需求在脑海里实现一遍，这样才能避免缺少某些表或字段导致功能无法实现，切记要先建立好完整的数据库，写好完善的数据库文档（要注意的细节有：命名的规范，注意单词拼写是否正确，还有各表的关系（见3），要写好各表、各字段的功能解析和备注提醒等等，规范的文档包括——是否主键，字段名称，字段描述即中文注释，数据类型，长度，是否可空，约束即是否外键，默认值，备注），再开始搭建其他各层 2、数据库备份功能的实现：每个表都添加字段：IsDelete（bit）——是否删除，在删除的时候就把该字段赋值为“1”或者“True”，表明该行数据已经被删除，而不是真的删除 3、注意建表时各表的关系：一对多的关系应该建立子表，并使用外键；多对多的关系应该在两表之间建立一个关系表（RelationTable），把两者之间的关系保存到关系表中，并使用两个表的主键作为外键。若删除两者其中之一，必须切记把关系表中对应的那个关系也删除掉（也是给IsDelete字段赋值为1，而不是真的删除） 4、外键的使用：这个地方我自己也还是不大懂，什么时候用外键什么时候不用外键。目前，我默认为能用外键就应该添加外键约束，虽然可以通过后台在逻辑上去进行约束，但是我还是觉得在数据库使用外键约束更加可靠 5、多条件（不定项条件）搜索：采用字符串拼接技术，给每个搜索条件参数声明一个对应的字符串拼接变量，先在备注中写好当输入所有搜索条件参数时的SQL语句，然后将传入的每个条件参数判断是否为空字符串（条件如果不用，传空字符串到数据库存储过程，不要使用NULL，因为NULL代表未知量，容易造成条件判断时的逻辑混乱），若为空字符串，则对应的拼接变量赋值为空字符串；若条件参数不为空，就按照刚才写的SQL语句格式给对应的拼接变量赋值（把传入的条件参数拼进字符串时要注意使用Convert()函数进行强制转换，把各种数据类型转换成字符串格式，否则会报错）；最后，当每个条件都判断过了并且给对应的拼接变量都赋值了后，就可以把各个拼接变量按照在备注中写好的SQL语句拼接起来，然后使用EXEC(@SQL)执行最终拼接的SQL语句 6、临时表的使用：在存储过程中无法一次性选择出想要的表时，就需要建立一个临时表，把每次选择的表插入到临时表中（可能需要使用游标），最后再一次性把临时表选择出来。临时表的表名要以#开头，表明该表是临时表，它每次使用完毕后都会自动销毁，不会永久性存在数据库中 7、分页功能：这里说的是（AspNetPager+存储过程）的真分页。其中AspNetPager负责取出每次分页的开始和结束的索引并传给数据库，存储过程需要根据两个索引取出索引之间的那几行数据。所以，存储过程需要在取出数据前把表按要求排好顺序，并添加一个索引字段（为了按索引取出数据），这样有两个方案，一个是上面说的建立一个和原表一样的临时表，但是给它一个新的自增主键，让它自增长作为序号，这样把原表按要求的顺序插进临时表的时候，分页索引就自动建立好了，然后取数据的时候就把那两个传入的分页索引作为条件取出临时表的数据即可。另一个方案是使用数据库系统函数Row_Number()建立索引字段，即在子查询中使用该函数，并把本表按要求的顺序选择出来，给该表重命名，然后再按索引取数据： select * from (select Row_Number() over(order by ...) as Row_Num , * from ...）as T where T.Row_Num between ... 8、关于存储过程：每个表至少四个存储过程：Select（按自己的主键ID取出数据），Insert（添加数据，有子表的话可能需要返回一个值，就是新插入的数据的ID，可使用output关键字实现），Update（更新，切记添加更新的where条件），Delete（删除，如果该数据库需要设计备份功能的话，就不执行删除，而是建立一个IsDelete（bit）字段，删除的时候update为1，也切记添加where条件）。有的时候还可能需要取出全部表的所有数据，Select_All（用于控件的数据绑定等等） 9、游标的使用：游标配合While（@@FETCH_STATUS=0）循环语句可以遍历表并逐行对数据进行各种操作。一般用于对含有子表（含外键）的表进行删除，修改等操作。首先本表的某行数据的ID放进游标，然后在子表中根据该ID找出对应的数据并进行删改等操作，然后再把本表的该行数据进行删改等操作，完了后再把本表下一行数据的ID放进游标，如此继续下去即可完成含子表的表的相关操作 10、数据库导出表到EXCEL：使用外部插件，比麻烦 二、Model层 1、该层用来将各个表的数据封装成对象进行数据传递，所以，表中的每个字段都是该表对应的类的属性。但是有时候需要经常用到一些额外的参数，例如分页时用的两个索引值，如果每次都作为额外参数进行传递会十分麻烦，所以可以把这些参数也作为该类的一个属性，使用时直接像其他参数一样封装到该类的对象中进行传递或调用，这样可以简便很多 2、为了封装参数时的灵活性，应该给每个类写一个空的构造函数，这样可以每次使用该类时可以调用不定个属性，而不是固定死每次封装对象都必须把所有属性赋值 3、一般来说，Model层中，每个类对应数据库中的每个表，每个类的属性的要和表中对应的字段数据类型要保持一致。但是由于从前台取参，到赋值给Model层对象，到再调用BLL层函数，到传参到数据库，这几个环节都需要对参数进行非空判断，而DateTime等数据类型的非空判断比较困难，所以在本项目中，我例外的把DateTime类型的属性改为String字符串类型（因为字符串在前台取值时不会为NULL，例如，如果用户在TextBox中什么都不填，那么在后台取值时，将取得一个空字符串''，而不是NULL） 三、Common层 1、该层主要是存放SqlHelper，FileHelper等通用的帮助类 2、其实后来发现，有很多东西都是公用的，比如一些方法，例如：输入的数据的非空检测，输入数据的最大长度检测和截取，防止SQL注入的字符串检测和处理，添加用户操作日志等等，这些可用抽出来的公用方法，最好是在这个common层写好，然后在其他层引用或者调用，这样维护起来十分的方面，不用在修改的时候逐个cs文件或者逐个页面的改 四、DAL层 1、该层主要负责存放针对于每个表进行的操作方法（和MODEL层一样，每个类对应一个表），如增删改取 2、这部分还需要下工夫，这部分的方法主要是把传过来的参数进行一定的格式化，然后调用SqlHelper把这些参数传给存储过程，并执行存储过程，最后接收存储过程返回的结果。如果要对返回的结果进行一些修改，比如数据的格式转换，应该使用List\u003c\u003e比较容易实行，如果只需要原封不动的接收一张表，应该使用Table比较方便 3、方法的大概步骤：存储过程名字sql，参数数组paras，调用SqlHelper方法ExecuteQuery()，接收返回结果集dt或者list 五、BLL层 1、对应调用DAL层的方法，供给UI层使用，这层封装的是业务逻辑 2、目前该项目的这层并没有体现出应有的业务逻辑，都是直接同名调用DAL层的方法就直接去UI绑数据了，我觉得真正的项目不应该是这样的 3、我认为，DAL层应该只存放针对某个Model（也就是某张表）的最基本的增删查改，不考虑UI层需要的数据格式。然后BLL层调用DAL层的方法，对数据进行整合，处理成可以直接绑定到UI的格式（如过滤，筛选，多个表的联合、匹配、业务逻辑上的计算，业务上的数据验证等等） 六、UI层 1、页面的大致分类：登录页，母版页，首页，项目添加页面，管理页面（显示项目查询结果列表，提供查改删等操作的入口），详细信息页面（单个项目的查看，修改），出错页 2、页面的浏览权限：项目一般把用户的权限进行划分，不同的用户的权限级别应该不同，比如普通用户只能查看项目，高级用户能够添加，修改，删除项目，而管理员能拥有所有权限，包括对用户的管理。因此，每个用户都应该有一个权限属性，在用户登录成功的时候，马上取出该用户的权限并保存到Session中。这样在该用户浏览页面的时候，每次跳转页面或者点击按钮，都可以根据Session里面的值来判断用户是否拥有该操作的权限。 3、页面分类：能提供不同权限的用户使用的页面，尽量分开成与权限对应的页面，比如管理页面，普通用户只能查看不能修改和删除，高级用户能查看修改和删除，最好的做法是供普通用户使用的做成一个页面，供高级用户使用的做成另一个页面，而不是做成一个页面然后根据用户的权限判断是否隐藏修改和删除按钮。虽然两个页面大部分是相同的，但是代码的逻辑十分清晰，也不会导致安全性问题的出现（普通用户进行越权入侵），并且维护起来而十分容易 4、登陆页：主要负责用户名和密码的验证，高级一点的会加个验证码。当然用户名和密码是保存在数据库中的，所以要调用一个存储过程，看该用户验证是否正确。正确的话，取出该用户的权限，保存到Session中，并根据权限的不同跳转到不同的首页 5、项目添加页面：主要负责添加项目，用户等等。该过程的实质是，在页面的控件获取用户输入的数据，进行一定的格式转换（根据需要而定），然后将这堆数据封装成Model层的对象，把该对象传给BLL层的方法，调用该方法把该对象包含的数据传到DAL层的存储过程，最终把该对象添加到数据库的表中。在用户输入添加信息时，一定要考虑该数据的最大值或者最大长度，否则是添加不进去数据库的。所以每获取一个控件的值，在封装到Mod","date":"2014-06-20","objectID":"/2014-06-20-%E9%A1%B9%E7%9B%AE-%E5%8D%8E%E5%8C%97%E7%94%B5%E5%8A%9B%E5%A4%A7%E5%AD%A6%E5%9F%BA%E9%87%91%E4%BC%9A%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93/:0:0","tags":null,"title":"华北电力大学基金会管理系统开发总结","uri":"/2014-06-20-%E9%A1%B9%E7%9B%AE-%E5%8D%8E%E5%8C%97%E7%94%B5%E5%8A%9B%E5%A4%A7%E5%AD%A6%E5%9F%BA%E9%87%91%E4%BC%9A%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%E5%BC%80%E5%8F%91%E6%80%BB%E7%BB%93/"},{"categories":["Technology"],"content":"一、SQL多条件查询 在实现SQL的多条件查询时，一般采用的实现方法是根据传入的参数是否有效来判读是否拼接该项查询条件，如 // 传入参数：@ProjectName='rrrr'，@ProjectNumber='123' // 先写好基本查询句子 declare @sqlString varchar(100) set @sqlString = 'select * from Project_Info where 1=1' // 拼接查询条件1 if (@ProjectName IS NOT NULL) AND (@ProjectName \u003c\u003e '') begin set @sqlString = @sqlString + ' and ProjectName like ''%' + @ProjectName + '%''' end // 拼接查询条件2 if (@ProjectNumber IS NOT NULL) AND (@ProjectNumber \u003c\u003e '') begin set @sqlString = @sqlString + ' and ProjectNumber =''' + @ProjectNumber + '''' end // 执行最终结果 exec @sqlString 二、问题 拼接SQL时，如果不小心注意参数的引号，容易出现如下错误： set @sqlString = @sqlString + ' and ProjectNumber =' + @ProjectNumber 执行报错，原因是在sql存储过程输入参数的时候，输入的rrrr，会直接赋给@ProjectName，系统不会自动给rrrr加上'‘两个单引号，导致最终SQL语句变成： select * from Project_Info where 1=1 and ProjectNumber = 123 而正确SQL的应该是： select * from Project_Info where 1=1 and ProjectNumber = '123' 于是系统报错“列名’rrrr’无效”。解决办法：使用多个连续的单引号： set @sqlString = @sqlString + ' and ProjectNumber =''' + @ProjectNumber + '''' 三、SQL Server中的单引号使用规则 首先尽量不要在SQL中使用双引号。单引号如果要作为一个“字符串单引号”，那就在它的前面加一个单引号进行转义。即： 两个连用的单引号 == 一个“字符串”意义上的单引号（即 '’ == \" ' “） SQL 语句 执行结果 select '''''' '' select ' '' '' ' ' ' select ' ''ab ' ‘ab select ' ''a''b ' ‘a’b select ' ''a''b'' ' ‘a’b’ 技巧：拿到一个包含很多单引号的字符串（如，' and ProjectName like ''%'），分析时，首先可以确定第一个和最后一个单引号都是传统的最外围的用来定义这条字符串的单引号，把它们除开后，对于内部其他多个连用的单引号，把每两个连用的单引号替换成一个“字符串单引号”即可（如，and ProjectName like \" ' \"）。 ","date":"2014-03-06","objectID":"/2014-03-06-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B08/:0:0","tags":null,"title":"技术笔记(8)-SQL多条件查询字符串拼接问题","uri":"/2014-03-06-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B08/"},{"categories":["Technology"],"content":"编程语言、标记语言、脚本语言分别有哪些？区别是什么？ 一、标记语言 是一种将文本（Text）以及文本相关的其他信息结合起来，展现出关于文档结构和数据处理细节的电脑文字编码。与文本相关的其他信息（包括例如文本的结构和表示信息等）与原来的文本结合在一起，但是使用标记（markup）进行标识。 如：HTML、XML 二、脚本语言 为了缩短传统的编写-编译-链接-运行（edit-compile-link-run）过程而创建的计算机编程语言。它的特点是：程序代码即是最终的执行文件，只是这个过程需要解释器的参与。脚本语言通常是被解释执行的，而且程序是文本文件。 如：JavaScript，Python 三、解析型语言 指用专门解释器对源程序逐行解释成特定平台的机器码并立即执行的语言；相当于把编译型语言的编译链接过程混到一起同时完成的。 解释型语言执行效率较低，且不能脱离解释器运行，但它的跨平台型比较容易，只需提供特定解释器即可。 如：Python（同时是脚本语言），Java，C# 三、编译型语言 指用专用的编译器，针对特定的操作平台（操作系统）将某种高级语言源代码一次性翻译成可被硬件平台直接运行的二进制机器码（具有操作数，指令、及相应的格式），这个过程叫做编译；编译好的可执行性文件（.exe），可在相对应的平台上运行（移植性差，但运行效率高）。有些程序编译后，还需要把其他编译好的，可能需要组装两个以上的目标代码生成最终的可执行性文件，称为链接（可实现对低层次代码的复用）。 如：C、C++、Java 另外，Java语言是一门很特殊的语言，Java程序需要进行编译步骤，但并不会生成特定平台的二进制机器码，它编译后生成的是一种与平台无关的字节码文件（*.class）（移植性好的原因），这种字节码自然不能被平台直接执行，运行时需要由解释器解释成相应平台的二进制机器码文件；大多数人认为Java是一种编译型语言，但我们说Java即是编译型语言，也是解释型语言也并没有错。 四、区别 标记语言没有逻辑和行为能力，不用于向计算机发出指令，常用于格式化和链接 脚本语言介于标记语言和编程语言之间，脚本语言脚本语言不需要编译，可以直接用，由解释器来负责解释 编译型语言写的程序执行之前，需要一个专门的编译过程，把程序编译成为机器语言的文件，比如exe文件，以后要运行的话就不用重新翻译了，直接使用编译的结果就行了（exe文件），因为翻译只做了一次，运行时不需要翻译，所以编译型语言的程序执行效率高 五、脚本语言与系统语言的区别 抽象的级别：这是最重要也是最明显示的不同。脚本语言对程序员提供了更高级的抽象。这一点明显表现在：在这种语言自身中，存在有高级的数据结构，如列表和字典结构，和对这种结构简单方便的嵌套和操作。这样可以创建非常成功的程序 类型定义：系统语言通常是强类型和静态类型定义。这就意味着所有变量的类型要在程序中指定，在编译时检查。相反地，脚本语言是最松散的类型定义，完全没有类型声明，并且在运行时进行动态类型检查 执行：系统语言的特点是编译的。程序被编译成可执行的二进制。另一方面，脚本语言的特点是解释，也就是，指令被立即执行，不存在一个编译的中间状态。这就意味着脚本语言是交互式的(你可以在提示符下敲入命令，并且看到结果)，这是另一个巨大的胜利。这样完全将编译过程从编辑-编译-运行循环中去掉了。 速度：以上三点是脚本语言一方面在速度与效率，另一方面在易用性与表示式的强大性之间进行折衷的典型例子。这就使得脚本语言的执行速度比系统语言慢一个数量级。这就是对脚本语言诽谤最多的方面。性能的降低不是真正的问题，因为思想是用脚本语言来组合组件，这些组件是用象C 这样的快速系统语言来编写的。所以所有需要运行快速的东西将因为是用快速的语言实现的而运行得快速。脚本语言只用于将东西绑在一起，并且这些通常不是性能的瓶颈(或如果是，你需要重新检查你的设计) 六、本文参考： 编程语言、标记语言、脚本语言分别有哪些？区别是什么？ 编译型语言、解释型语言与脚本语言三大类型详解 ","date":"2014-01-04","objectID":"/2014-01-04-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B07/:0:0","tags":null,"title":"技术笔记(7)-标记语言、脚本语言、编程语言","uri":"/2014-01-04-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B07/"},{"categories":["Technology"],"content":"刚学ASP.NET时用的比较多的一个简单版的SqlHelper： using System; using System.Configuration; using System.Data; using System.Data.SqlClient; using System.Collections; namespace TICSystemCommon { public abstract class SqlHelper { //Database connection strings public static readonly string ConnectionStringShop = ConfigurationManager.ConnectionStrings[\"ConnectionString\"].ConnectionString; /// \u003cparam name=\"connectionString\"\u003ea valid connection string for a SqlConnection\u003c/param\u003e /// \u003cparam name=\"commandType\"\u003ethe CommandType (stored procedure, text, etc.)\u003c/param\u003e /// \u003cparam name=\"commandText\"\u003ethe stored procedure name or T-SQL command\u003c/param\u003e /// \u003cparam name=\"commandParameters\"\u003ean array of SqlParamters used to execute the command\u003c/param\u003e /// \u003creturns\u003ean int representing the number of rows affected by the command\u003c/returns\u003e public static int ExecuteNonQuery(string connectionString, CommandType cmdType, string cmdText, params SqlParameter[] commandParameters) { SqlCommand cmd = new SqlCommand(); using (SqlConnection conn = new SqlConnection(connectionString)) { PrepareCommand(cmd, conn, null, cmdType, cmdText, commandParameters); int val = cmd.ExecuteNonQuery(); //cmd.Parameters.Clear(); return val; } } /// \u003cparam name=\"connectionString\"\u003ea valid connection string for a SqlConnection\u003c/param\u003e /// \u003cparam name=\"commandType\"\u003ethe CommandType (stored procedure, text, etc.)\u003c/param\u003e /// \u003cparam name=\"commandText\"\u003ethe stored procedure name or T-SQL command\u003c/param\u003e /// \u003cparam name=\"commandParameters\"\u003ean array of SqlParamters used to execute the command\u003c/param\u003e /// \u003creturns\u003eA SqlDataReader containing the results\u003c/returns\u003e public static SqlDataReader ExecuteReader(string connectionString, CommandType cmdType, string cmdText, params SqlParameter[] commandParameters) { SqlCommand cmd = new SqlCommand(); SqlConnection conn = new SqlConnection(connectionString); // we use a try/catch here because if the method throws an exception we want to // close the connection throw code, because no datareader will exist, hence the // commandBehaviour.CloseConnection will not work try { PrepareCommand(cmd, conn, null, cmdType, cmdText, commandParameters); SqlDataReader rdr = cmd.ExecuteReader(CommandBehavior.CloseConnection); //cmd.Parameters.Clear(); return rdr; } catch(Exception e) { conn.Close(); throw e; } } /// \u003cparam name=\"connectionString\"\u003ea valid connection string for a SqlConnection\u003c/param\u003e /// \u003cparam name=\"commandType\"\u003ethe CommandType (stored procedure, text, etc.)\u003c/param\u003e /// \u003cparam name=\"commandText\"\u003ethe stored procedure name or T-SQL command\u003c/param\u003e /// \u003cparam name=\"commandParameters\"\u003ean array of SqlParamters used to execute the command\u003c/param\u003e /// \u003creturns\u003e the first column of the first record against an existing database connection\u003c/returns\u003e public static object ExecuteScalar(string connectionString, CommandType cmdType, string cmdText, params SqlParameter[] commandParameters) { SqlCommand cmd = new SqlCommand(); using (SqlConnection connection = new SqlConnection(connectionString)) { PrepareCommand(cmd, connection, null, cmdType, cmdText, commandParameters); object val = cmd.ExecuteScalar(); cmd.Parameters.Clear(); return val; } } /// \u003csummary\u003e /// Prepare a command for execution /// \u003c/summary\u003e /// \u003cparam name=\"cmd\"\u003eSqlCommand object\u003c/param\u003e /// \u003cparam name=\"conn\"\u003eSqlConnection object\u003c/param\u003e /// \u003cparam name=\"trans\"\u003eSqlTransaction object\u003c/param\u003e /// \u003cparam name=\"cmdType\"\u003eCmd type e.g. stored procedure or text\u003c/param\u003e /// \u003cparam name=\"cmdText\"\u003eCommand text, e.g. Select * from Products\u003c/param\u003e /// \u003cparam name=\"cmdParms\"\u003eSqlParameters to use in the command\u003c/param\u003e private static void PrepareCommand(SqlCommand cmd, SqlConnection conn, SqlTransaction trans, CommandType cmdType, string cmdText, SqlParameter[] cmdParms) { if (conn.State != ConnectionState.Open) conn.Open(); cmd.Connection = conn; cmd.CommandText = cmdText; if (trans != null) cmd.Transaction = trans; cmd.CommandType = cmdType; if (cmdParms != null) { foreach (SqlParameter parm in cmdParms) cmd.Parameters.Add(p","date":"2013-12-21","objectID":"/2013-12-21-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B06/:0:0","tags":null,"title":"技术笔记(6)-SqlHelper","uri":"/2013-12-21-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B06/"},{"categories":["Technology"],"content":"数据库的常用表联接主要有inner join、left join、right join三种。 例表A Aid Adate 1 a1 2 a2 3 a3 例表B Bid Bdate 1 b1 2 b2 4 b4 1、inner join：取出id相同的字段，仅返回两表匹配的数据 select * from A inner join B on A.Aid = B.Bid 此时的取出的是: Aid Adate Bid Bdate 1 a1 1 b1 2 a2 2 b2 2、left join：先取出“左表”A表中所有数据，然后再加上与A、B匹配的的数据。不管匹配与否，“左表”表A的全部数据将全部返回 select * from a left join b on a.aid = b.bid 此时的取出的是: Aid Adate Bid Bdate 1 a1 1 b1 2 a2 2 b2 3 a3 3、right join：先取出“右表”B表中所有数据，然后再加上与A、B匹配的的数据。不管匹配与否，“右表”B表的全部数据将全部被返回 select * from a right join b on a.aid = b.bid 此时的取出的是: Aid Adate Bid Bdate 1 a1 1 b1 2 a2 2 b2 4 b4 4、小结 若 A join B，那么 A 为“左表”，B为“右表”。此时， 用 left join，则取左表（A）全部数据，和右表（B）匹配数据 用 right join，则取右表（B）全部数据，和左表（A）匹配数据 用 inner join，则仅取两表同时匹配数据 由此，可以推断 select * from A left join B on A.Aid=B.Bid 等价于(表的内容一样，只是字段的顺序不同) select * from B right join A on A.Aid=B.Bid ","date":"2013-12-21","objectID":"/2013-12-21-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B05/:0:0","tags":null,"title":"技术笔记(5)-表联接Join","uri":"/2013-12-21-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B05/"},{"categories":["Technology"],"content":"一、ADO.NET主要用于访问数据，尤其是数据库中的数据，该类被封装在System.Data.dll中。它的两个核心组件：DataSet和.NET数据提供程序 1、DataSet：记录在内存中的数据，相当于内存中的一个完整的数据集。但它并不与外部的物理数据库直接相连，而是通过DataAdapter间接的完成与外部数据库的交互。该类的主要对象有： DataTable：使用行和列形式来组织的一个矩形数据集 DataColumn：列（规则的集合） DataRow：行（实际的对应于规则的数据存储） Constraint：决定能进入DataTable的数据 DataRelation：不同的DataTable之间的关联 2、.NET数据提供程序：主要有4种对象，分别是： Connection：提供与数据源的连接 Command：执行一系列的数据库命令 DataReader：从数据源中提供高性能的数据流 DataAdapter：提供连接DataSet对象和数据源的桥梁。DataAdapter通过Command对象在数据源中执行SQL命令，并将数据加载到DataSet中，使DataSet中数据的更改与数据源保持一致 二、ADO.NET Connection：Connection类表示一个数据源的单个连接，但并不是单个调用。ADO.NET支持断开式连接，即可在一个Connection对象的使用过程中多次打开或关闭操作，但这些操作并不意味着该对象被创建或者销毁。主要属性有ConnectionString，ConnectionTimeOut，Database，ChangeDatabase，State等。主要的方法有：Open()，Close()。主要的状态属性有：Broken，Closed，Connecting，Executing，Fetching，Open。 三、ADO.NET Command：从本质上讲，ADO.NET的Command类就是SQL命令或者是对存储过程的引用。 1、主要属性有： CommandText：执行命令的内容（字符串），或者存储过程的名称 CommandType：告诉Command对象怎样解释CommandText属性的内容，当该属性的值为StoredProcedure时，说明CommandText属性的内容为存储过程名称；当为TableDirect时，说明内容为表名；当为Text时，说明为SQL文本命令（即直接的sql语句） Connection：对Connection对象的引用 Parameters：包含了针对CommandText属性所指定的SQL命令或存储过程的参数集合（通过函数AddRange(paras)向Command对象添加参数集） CommandTimeout：决定了命令出错前等待服务器响应的时间 Transaction： UpdateRowSource: 2、主要的方法有： ExecuteNonQuery()：执行命令并返回受影响的行数（用于返回结果为空的sql命令或存储过程） ExecuteScalar()：执行查询并返回结果集中的第一行的第一列（用于返回单个值的sql命令或存储过程） ExecuteReader()：向Connection发送CommandText并生成DataReader（用于返回多个结果的sql命令或存储过程，该方法创建一个DataReader对象。该方法执行时可以不带参数，或者带一个参数CommandBehavior） ExecuteXmlReader()：向Connection发送CommandText并生成XmlReader 四、DataReader：当Command对象返回结果集时，需要用DataReader来检索数据。DataReader对象返回一个来自Command对象的只读的，只能向前的数据流。DataReader每次只在内存中保留一行，所以开销非常小。 1、主要属性： Depth：多层结果集中，当前行的嵌套深度 FieldCount：当前行的数目 IsClosed：DataReader是否需要关闭 Item：列值 RecordsAffected：被修改，插入或者删除的行的数目 2、主要方法： Close()：关闭 GetType()：将指定的列的值作为指定类型获取 GetDataTypeName()：获取数据源类型的名称 GetFieldType()：返回指定列的系统类型 GetName()：获得指定列的名称 GetValue()：获得指定列的值 NextResult()：前进到得下一个结果 Read()：前进到下一列 五、DataAdapter：DataAdapter是DataSet和物理数据源之间的桥梁，即DataSet通过DataAdapter与数据库间接交互。DataAdapter类的目的是执行数据库查询并创建包含查询结果的DataTable，该类也可以把对DataTable的更改写回数据库。 1、主要方法： Fill()：将查询结果填充DataSet Update()：将DataSet中的数据的变动写回数据库 六、应用实例 Sqlconnection conn=new SqlConnection(); conn.ConnectionString=\"Server=XXX;\"+\"Database=XXX;\"+\"Intergrated Security=true;\"; SqlCommand cmd=new SqlCommand(); cmd.CommendText=\"select * from studentTable\"; cmd.Connection=conn; conn.Open(); DataTable dt=new DataTable(); DataSet ds=new DataSet(); SqlDataAdapter adapter=new SqlDataAdapter(); adapter.SelectCommand=cmd; adapter.Fill(ds,\"dt\"); conn.Close(); dt=ds.Tables[\"dt\"]; this.GridView.DataSource=dt.DefaultView; this.GridView.DataBind(); ","date":"2013-12-21","objectID":"/2013-12-21-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B04/:0:0","tags":null,"title":"技术笔记(4)-ADO.NET","uri":"/2013-12-21-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B04/"},{"categories":["Technology"],"content":"一、创建连接： 引入所使用的ADO.NET类的命名空间（引入System.Data类；使用SQL Server数据库的话就要再引入System.Data.SqlClient类） 创建连接字符串变量，以保存生成连接需要的信息 实例化Connection 二、使用连接： 打开连接 使用连接，主要进行读取数据，修改和删除数据等操作 关闭连接 三、示例： // 引入ADO.NET类的命名空间 using System.Data; using System.Data.SqlClient; // 创建连接字符串 string connStr = \"server=XXX（服务器名）; database=XXX（数据库）; uid=XXX（用户名）; pwd=XXX（该用户登陆密码）;\" // 创建连接数据库的SqlConnection对象，传入连接字符串，打开连接 SqlConnection conn = new SqlConnection(connStr); conn.Open(); // 创建要对数据库操作的SQL语句 string sql = \"insert into subjectTable(ID,name,point) values(2,'物理',4)\"; // 创建SqlCommand对象并传入SQL语句操作语句和SqlConnection连接对象 SqlCommand cmd = new SqlCommand(sql, conn); // 调用ExecuteNonQuery()函数（执行查询语句并返回受影响的行数） int res=cmd.ExecuteNonQuery(); // 关闭连接，返回数据 conn.Close(); return res; 四、共享连接字符串 1.打开项目的web.config文件，在connectionStrings中修改 \u003cadd name=\"connStr\" connectionString=\"server=xxx; database=xxx; uid=xxx; pwd=xxx;\"/\u003e 2.在原来存放连接语句的项目中引入using System.Configuration，并将原来语句 string connStr = \"server=xxx; database=xxx; uid=xxx; pwd=xxx;\" 改为 string connStr = ConfigurationManager.ConnectionStrings[\"connStr\"].ConnectionString; ","date":"2013-12-18","objectID":"/2013-12-18-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B03/:0:0","tags":null,"title":"技术笔记(3)-C#连接数据库","uri":"/2013-12-18-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B03/"},{"categories":["Technology"],"content":"1、 Transact-SQL语言分类 数据定义语言（DDL）：创建，删除，修改数据表，建立索引和约束，创建其他数据库对象等 数据操纵语言（DML）：查询，添加，删除，修改数据等 数据控制语言（DCL）：控制安全性的命令 2、数据类型： 数值型：int,smallint,bigint,tinyint,bit,decimal,numeric,float,real 字符型：char,varchar,text 日期/时间型：datetime,smalldatetime 货币型：money,smallmoney 二进制型：binary,varbinary,image unicode型：nchar,nvarchar,ntext sql-variant型：混合型 table型：临时表 自定义型 3、标识符命名规则： 规则标识符（符合标识符规则，不用分隔符） 分隔标识符（不符合标识符规则，必须包含在\" “和 内） 4、对象命名规则：服务器名+数据库名+所有者名+对象名，如：sever.database.owner_name.object_name 5、常量：见上数据类型 6、变量： //声明 DECLARE @myint int，@mychar char(8) //赋值： SET @myint=10 SET @mychar='wang' SELECT @myint=10，@mychar='wang' //查询（即输出） SELECT @myint as myint 7、全局变量：系统预定好的变量，只读，如：@@ERROR，@@DBTS 8、算术运算：+，-，*，/，% 9、逻辑运算： and：仅当两个都为true时返回true or：仅当两个都为false时返回false not：对布尔表达式取反 all：如果全都为true，返回true any：如果有一个为true，返回true like：如果操作数与一种模式匹配，返回true in：如果操作数等于表达式列表中的一个，返回true 10、字符串连接符：+ 11、比较运算符：=，\u003c，\u003e，！=，\u003c=，\u003e=，\u003c\u003e（不等于） 12、null与空判断：null为空值，即未知值（不是0或空字符），用操作符is来运算，而不是用比较运算符 13、大对象处理：对ntext，text，image都是当作二进制大对象（BLOB）进行访问的，有专门的语句writetext，updatetext，readtext等来处理 14、流程控制： declare @cal int select @cal=1 while(@cal\u003c5) begin select @cal=@cal+1 end 15、select语句 select * from newstable select ID,title,content from newstable select content1 + ' , ' + content2 as content from newstable select top 10 ID from newstable 16、where语句： select * from newstable where ID = 10 17、添加排序顺序： select * from newstable where caID = 10 order by createTime select * from newstable where caID = 10 order by createTime desc 18、添加汇总信息：（聚合函数） select AVG(ID) from newstable 返回newstable表的ID列的平均值 select MAX(ID) from newstable 返回newstable表的ID列的最大值 select MIN(ID) from newstable 返回newstable表的ID列的最小值 select COUNT(ID) from newstable where ID\u003c10 select SUM(ID) from newstable where ID\u003c10 19、group by语句：必须对不出现在聚合函数中的列使用该语句，否则报错。SQL要求，在含聚合函数的查询中，任何列名要么加入聚合函数中，要么包含在group by短语中 select SUM(ID),caID from newstable // 必须改为 select SUM(ID),caID from newstable group by caID 20、添加HAVING子句 select caID from newstable group by caID having min(ID)\u003e7 21、distinct：去除重复的值 select distinct caID from newstable 22、内连接： select * from table1 inner join table2 on table1.ID=table2.ID 返回table1和table2中所有满足table1.ID=table2.ID的行 23、外连接： select * from table1 left join table2 on table1.ID=table2.ID ; select * from table1 right join table2 on table1.ID=table2.ID 24、组合字段，建立新列 select ID * caID as ID_caID from newstable 25、insert语句： insert into newstable(title,content) values('wang','yusheng') insert into test select title,content,createTime from newstable where createTime\u003cGETDATE() 26、select into语句，利用已有表的字段创建新表（可作为临时表）： select newstable.title,content,createTime into test2 from newstable 27、update语句： update test set createTime=GETDATE(),content='abcdefg' where ID=1 update test2 set createTime=GETDATE() where title not like 'wang' update test2 set content='update2' where title in ('wang','sheng') 28、delete语句：由于delete语句操作不可恢复，所以删除数据前最好用select into语句建立一个临时表来备份 delete from test where ID=1 delete from test where ID in (select ID from test group by ID having sum(ID) \u003c3) 29、创建表： create table categorytable ( ID int identity(1,1) primary key , [name] varchar(20) not null default 'abcd' ... ) 30、键：键是行标识符，能唯一的标识行和列。但键并不是独立的另一个列，它可以是表中任何一列或多个列组合来表达，但被定义为键的列不能有重复的项，必须保持其唯一性。 31、主键：本表的行标识符 create table newtable(newText char(10) null primary key) 32、外键：外键是在另一个表中出现的主键的实例 create table newstable(newsID int references category(newsID)) 33、数据规范化：设计一组表的过程叫规范化，目标是避免数据的冗余存储 第一范式：列出所有要保存的数据项，并为他们建立列名称，并把相关的列组合到表中，对每个表建立一个主键 第二范式：检查每一列，将完全依赖主键的列保留，将部分依赖主键的列移到另一个新表中重新组合 第三范式：存在可传递关系的列从单一的表中除去，移到单独的表中 34、添加索引：（数据库一般自动为表中的主键建立索引） 唯一索引：不允许两行有相同的索引，不允许重复值，没有可以引用到多行的索引条目 群集索引：在其内部排序的索引，并且按照索引排序结构规定了行的存储结构（存储顺序），每张表中只能有一个群集索引，索引要求唯一 非群集索引：对表中的行生成索引但不改变他们的存储顺序，索引不要求唯一 35、建立索引： create table newstable ( newsText char(6) null) primary key unique(唯一索引) [或clustered(群集索引)，nonclustered(非群集索引)] ) create table newstable(...) create unique clustered index newsID_ind on newstable(newsID) 36、添加约束：约束可以是主键，外键的引用，或者是数据验证的规则 37、更改表： alter table newstable add ID2 int null alter table newstable drop co","date":"2013-10-23","objectID":"/2013-10-23-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B02/:0:0","tags":null,"title":"技术笔记(2)-SQL语法小结","uri":"/2013-10-23-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B02/"},{"categories":["Technology"],"content":"1、 camelCase规则:第一个单词以小写开头，其他每个单词开头大写，其余字母小写，如: firstName(建议使用) 2、 PascalCase规则: 每个单词除了第一个字母大写，其余小写，如: FirstName 3、 C#的布尔类型只包含true和false，不能用1和0表示，即布尔值和整数值之间不能相互转换 bool c=a\u003eb; //若a大于b，则d为true，否则为false 4、 注意\u0026\u0026和\u0026的区别（同||和|）：\u0026\u0026如果第一个操作数可判断为false，则不会继续以后的操作数而直接得出语句的值为false；||如果第一个操作数可判断为true，则不会继续以后的操作数而直接得出语句的值为true 5、 取a和b中的最大者 double max=(a\u003eb)?a:b; 6、 整型和字符串转换 // 整型转字符串 int i=10; String s=i.ToString(); // 字符串转整型 String s=“100”; int i=Int32.Parse(s); 7、 字符串转换类：Convert。需注意的是，字符串是不能改变的，如果要改变字符串的值，系统会创建一个新的字符串，而不会改变原来的字符串 8、 注意switch语句的开关性，如果某个case语句符合条件，系统会进入switch语句并一直执行到最后，除非遇到break语句才会跳出 9、 数组声明和初始化 int[] a=new int[10]; int[] b=new int[3]{2,4,6}; 10、 foreach循环：对数组进行只读访问而不能做任何修改，而且不会造成数组下标越界 int[] b=new int[3]{2,4,6}; foreach(int m in b){ Console.WriteLine(m)；// 输出数组b的每个元素 } 11、 一个函数可以有多条返回语句return，但有返回值类型的函数必须每种情况都有对应的返回值（特别注意含if语句的函数） 12、 函数中实参向形参的数据传递是按值传递的话，形参的数值只是从实参复制过来的一个数值，因此在函数中对形参的任何操作对实参无任何影响；如果是按应用传递的话，形参就是实参的另一个名字，两者是同一个数据，因此在函数中对形参的任何操作都是对实参的间接操作。另外，可以用ref关键字指定参数为引用参数，使按值传递变为按引用传递 void fun(ref int a){...}; ...; int b=10; fun(b); // 这时，指定了a是b的引用，在fun内对a的操作会影响b 13、 params：可以用该关键字来化简数组参数的传递，如： void fun(params int[] a){...}; ...; fun(1,2,3,4,5); 14、 如果要修改全局变量的值，就要使用static，因为const禁止修改变量的值。如果局部变量和全局变量同名，全局变量就会被屏蔽 15、 C#不允许派生类的可访问性比基类（父类）更高，即内部类可以继承于一个公共类，但公共类不可以继承于一个内部类 16、 如果在定义类的时候没有指定基类，编译器就会默认该类派生于Object类，于是就可以使用Object类的方法，如：GetType()[返回从System.Type派生的类的一个实例]；ToString()[获取对象的字符串] 17、 构造函数执行顺序：为了实例化派生的类，必须先实例化它的基类。而要实例化这个基类，又必须实例化这个基类的基类，这样一直实例化到根类System.Object为止。结果是，无论使用什么构造函数实例化一个类，总是要先调用System.Object.Object()；如果对一个类使用非默认的构造函数，默认的情况是在其基类上使用匹配于这个构造函数签名的构造函数，如果找不到这样的构造函数，就使用基类的默认构造函数。（调用base关键字可以指定.NET实例化过程中使用基类中指定签名的构造函数；调用this关键字可以使当前类在调用this指定的构造函数前，先调用与this的参数匹配的非默认构造函数） 18、 定义变量时，如果使用了static关键字，则表明该变量是类的静态成员而不是对象实例的成员；定义方法时，如果使用了static关键字，该方法就只能通过类来访问，不能通过对象实例来访问（virtual方法可以重写；abstract方法必须重写；override方法重写了一个基类方法；extern方法定义字其他地方） 19、 定义属性：get创建只读属性，set创建只写属性 20、 执行接口的类必须实现接口中的所有的成员；可以使用virtual和abstract来执行接口的成员，但不能使用static和const 20、 高级转换： (1). 封箱和拆箱； (2). is运算符：检查两个类型是否兼容，是则为true； (3). as运算符：把一种类型转换为指定的引用类型 22、 深度复制：GetCopy()，Clone() 23、 定义委托：delegate-返回类型-委托标识符（参数表） public delegate void EvenNumberHandler(int Number){...}; 24、 定义事件：event-委托标识符-事件标识符 public event EvenNumberHandler OnEvenNumber; 25、 安装事件：用new创建一个委托实例并安装到激发事件的类中 public void MyEvenNumberHandler(string URL){...}; EvenNumberHandler HandlerInstance=new EvenNumberHandler(MyEvenNumberHandler); 26、 创建了委托实例后，用+=运算符（相当于注册监听器的“.”运算符）将其添加到事件变量中 OnEvenNumber += HandlerInstance; // 删除委托实例用 -= 27、 激发事件：将事件参数传递到委托方法中即可 EvenNumberHandler(10); ","date":"2013-09-20","objectID":"/2013-09-20-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B01/:0:0","tags":null,"title":"技术笔记(1)-C#语法小结","uri":"/2013-09-20-%E6%8A%80%E6%9C%AF%E7%AC%94%E8%AE%B01/"}]